
\documentclass[11pt,twoside,a4paper]{article}
\usepackage[hmargin=2cm, vmargin=2cm]{geometry}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage[english]{babel}
\usepackage{setspace}
\usepackage{xcolor}
\usepackage{multicol}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage[hidelinks=true]{hyperref}
\usepackage{overpic}
\usepackage[font=small,skip=2pt]{caption}
\usepackage[
backend=biber,
style=authoryear-comp,
]{biblatex}


\newcommand{\independent}{\perp\!\!\!\!\perp} 

\addbibresource{RMbibliography.bib}
\onehalfspacing
\parindent=0pt

\begin{document}
\begin{titlepage}

	\title{{\LARGE Basis Choice for Scalar-on-Function Regression \\ with Applications to Near-Infrared Spectroscopy}}
	\author{Jonghun Baek, Jakob R. Juergens, Jonathan Willnow}
	\date{11.02.2022}
	\maketitle
	\thispagestyle{empty}
	\vspace{1.5 cm}
	\begin{center}
		University of Bonn \\
		\vspace{0.2cm}
		Research Module in Econometrics and Statistics \\
		Supervised by: Prof. Dr. Dominik Liebl and Dr. Christopher Walsh \\
		\vspace{0.2cm}
		Winter Semester 2021/2022
	\end{center}
\end{titlepage}
	\newpage
	
	\thispagestyle{empty}
	\tableofcontents
	
	\newpage
	
	\setlength{\abovedisplayskip}{0.35cm}
	\setlength{\belowdisplayskip}{0.35cm}

	\setlength{\abovedisplayshortskip}{0.2cm}
	\setlength{\belowdisplayshortskip}{0.35cm}

	\pagenumbering{arabic}
	\section{Introduction}
		
	Functional Data Analysis (FDA) has its roots in the work of Ulf Grenander and Kari Karhunen and is gaining traction as researchers from different fields collect data that is functional in nature. Although classical statistical methods can often process this data, FDA has advantages allowing it to extract information given by properties such as the smoothness of the underlying process or its derivatives (cf. \cite{levitin_introduction_2007}).	As \cite{kokoszka_introduction_2017} describe, using methods from FDA should be considered when one variable of a given data set can be seen as smooth curves.	 
	Examples of such curves are the absorption curves of electromagnetic radiation in the Near-infrared (NIR) spectrum by chemical samples.\footnote{For more details on Near-Infrared-Spectroscopy, refer to Appendix \ref{NIR}.}
	
	This paper introduces Functional Linear Regression in a scalar-on-function setting. The distinct feature of this framework is that the regressor is a function, which makes a different approach to estimation necessary because the problem of estimating an unknown coefficient function is inherently infinite-dimensional. 
	We introduce two ways of translating this infinite-dimensional problem into a finite-dimensional problem that can be addressed using theory from multivariate regression: First, a so-called basis expansion of the coefficient function, and second, Functional Principal Component Regression (FPCR). Both methods depend on a parameter called a truncation parameter for a functional basis, and this paper focuses on exploring the selection of these parameters using cross-validation.\\
	
	In Section \ref{Theory}, we introduce the necessary theoretical concepts, describe the estimation procedures, and address the theoretical importance of the truncation parameter. Section \ref{Simulation} contains a description of our Monte-Carlo Simulation, which aims to provide information on how to choose an appropriate functional basis and truncation parameter for the aforementioned methods. The application in Section \ref{Application} uses the insights from theory and simulation to choose an appropriate basis for the estimation of octane numbers of gasoline samples based on Near-Infrared absorption curves. In Section \ref{Outlook}, we describe the limitations of our approach and give an outlook on possible extensions for this paper.

	\section{Theory}\label{Theory}
	To introduce scalar-on-function regression, it is necessary to extend some concepts from multivariate regression to the realm of infinite-dimensional objects, as the statistics derived from infinite-dimensional random functions cannot be defined on a finite-dimensional space. One integral concept that must be defined are random functions as a special case of random variables. Paraphrasing a definition by \cite{bauer_wahrscheinlichkeitstheorie_2020}, a random variable $X:\Omega \rightarrow \Omega'$ is an $\mathcal{A} \text{-} \mathcal{A'} \text{-measurable}$ function, where $(\Omega, \mathcal{A}, P)$ is a probability space and $(\Omega', \mathcal{A'})$ is a measure space. 
	The typical case for a random variable realizing in $\mathbb{R}$ is $(\Omega', \mathcal{A'}) = (\mathbb{R}, \mathcal{B})$, where $\mathcal{B}$ is the canonical $\sigma$-algebra on the real numbers. As a first intuition, it is possible to imagine a similar concept where a random variable does not realize as an element of the real numbers but as a function in a function space. A formalization of this idea makes some more in-depth considerations necessary. The following theoretical introduction closely follows chapters 2.3 and 2.4 from \cite{hsing_theoretical_2015} and chapters 4.4 and 4.6 from \cite{kokoszka_introduction_2017}. 
	
	\subsection{Inner Products and Hilbert Spaces}
	The first concept we will introduce is the concept of Hilbert spaces. We start from inner product spaces but restrict our analysis to vector spaces over $\mathbb{R}$ for clarity. Let $\mathbb{V}$ be a vector space over $\mathbb{R}$.  Then, a function $\langle \cdot, \cdot \rangle : \mathbb{V} \times  \mathbb{V} \rightarrow \mathbb{R}$ is called an inner product, if $\forall v, v_1, v_2 \in \mathbb{V}$ and $a_1, a_2 \in \mathbb{R}$ the following properties hold.
	
	\begin{multicols}{2}
		\begin{enumerate}
			\item $\langle v, v \rangle \geq 0$
			\item $\langle v, v \rangle = 0$ if $v = 0$
			\item $\langle a_1 v_1 + a_2 v_2, v \rangle = a_1 \langle v_1, v \rangle + a_2 \langle v_2, v \rangle$
			\item $\langle v_1, v_2 \rangle = \langle v_2, v_1 \rangle$
		\end{enumerate}
	\end{multicols}

	A vector space with an associated inner product is called an inner product space. The inner product defines a norm and an associated distance on the vector space.
	\begin{equation}
		\lvert \lvert v \rvert \rvert = {\langle v, v \rangle}^{\frac{1}{2}} \quad \text{and} \quad 
		d(v_1, v_2) = {\langle v_2 - v_1, v_2 - v_1 \rangle}^{\frac{1}{2}}
	\end{equation}
	
	If the inner product space is complete with respect to the induced distance, it is called a Hilbert space, denoted $\mathbb{H}$ in the following. To extend the known concept of a basis in a finite dimensional space to potentially infinite Hilbert spaces, it is necessary to define the closed span of a sequence of elements of $\mathbb{H}$. Recall that the span of a set of vectors $S \subseteq \mathbb{R}^P$ is given by
	\begin{equation}
		span(S) = \left\{\sum_{i = 1}^{k} \lambda_i v_i \: \bigg\vert \: k \in \mathbb{N}, \: v_i \in S, \: \lambda_i \in \mathbb{R} \right\}
	\end{equation}
			
	The closed span $\overline{span}(S)$ of a sequence $S$ in $\mathbb{H}$ is defined as the closure of the span with respect to the distance induced by the norm and $S$ is called a basis of $\mathbb{H}$ if $\overline{span}(S) = \mathbb{H}$.	It is called an orthonormal basis if, in addition, the following properties hold. 
	\begin{multicols}{2}
		\begin{enumerate}
			\item $\langle v_i, v_j \rangle = 0 \quad \forall v_i, v_j \in S $ for $i \neq j$
			\item $\lvert \lvert v \rvert \rvert = 1 \quad \forall v \in S$
		\end{enumerate}
	\end{multicols}

	Each element of a Hilbert space can be expressed in terms of a corresponding basis. Using a Fourier expansion of an element $x \in \mathbb{H}$ with respect to a basis $S = \{s_n\}$ leads to the following representation.
	\begin{equation}
		x = \sum_{j = 1}^{\infty}{\langle x, s_j \rangle}s_j
	\end{equation}
	
	Differing from the finite-dimensional case, these representations can be limits of series. As using an infinite number of basis functions is infeasible in applied contexts, an intuitive way to approximate elements in a Hilbert space is to use a truncated series.
	\begin{equation}
		x \approx \sum_{j = 1}^{K}{\langle x, s_j \rangle}s_j
	\end{equation}
	
	\subsection{Random Functions in the Hilbert Space of Square-Integrable Functions}\label{L_2}
	In functional data analysis, one Hilbert space of particular importance is the space of square-integrable functions on $[0,1]$, but analogous constructions can be made for different domains. Denoted by $\mathbb{L}^2[0,1]$, this space consists of all Lebesgue-measurable functions $f(t)$ on $[0,1]$ that fulfill the following condition.
	\begin{equation}
		\lvert \lvert f \rvert \rvert_2 = \int_{0}^{1} \lvert f \rvert^2 \mathrm{d}\mu < \infty
	\end{equation}
	
	This ensures that a random function has a finite second moment so that the variance and covariance functions can be defined. The inner product on $\mathbb{L}^2[0,1]$ is defined by Equation \ref{inner_prod}.
	\begin{equation}\label{inner_prod}
		\langle f_1, f_2 \rangle = \int_{0}^{1} f_1 f_2 \mathrm{d}\mu.
	\end{equation}
	
	A random function on $\mathbb{L}^2[0,1]$ can now be defined formally as a function $X : \Omega \rightarrow \mathbb{L}^2[0,1]$ defined on a probability space $(\Omega, \mathcal{A}, P)$ where $\Omega$ is a sample space with $\sigma$-algebra $\mathcal{A}$ and a probability space $P$.
	
	\subsection{Functional Data Sets}
	If we take a random function $X(\omega)$ as defined in the previous section, then its realizations $x(t)$ are called sample curves of the random function. This presence of functional observations $x_i(t)$ in a data set defines functional data sets. However,  realizations of random functions are not typically observed in their functional form. Instead, each curve is observed at a set of discrete measurement points. Consider the case of a data set containing observations $x_i(t)$ of a random function $X(\omega)$
	\begin{equation}
		x_{i}(t_{i,j}) \in \mathbb{R}, \quad i = 1,\: \dots\: ,N, \; j = 1, \: \dots \:, J_i, \; t_{i,j} \in [T_1, T_2]
	\end{equation}
	
	Each curve $x_i(t)$ exists $\forall t \in [T_1, T_2]$, but is only observed at measurement points $t_{i,j}$. These measurement points can be different for each sample curve. In this paper, we only consider the case where curves share their measuring points.
	To use the unique capabilities of functional data analysis with functional data obtained in this form, it is necessary to restore its functional structure. Therefore, we introduce methods such as basis representations in the following parts of the Theory Section.\\
	As in the finite-dimensional setting, the concept of independent and identically distributed (i.i.d.) data is important for many aspects of functional data analysis. One example of i.i.d. curves could be Near-Infrared absorption spectra of gasoline samples where each sample is produced by the same production process and can therefore be interpreted as a realization of and i.i.d. random process itself. More information about this example can be found in Appendix \ref{NIR}.
	
	\subsection{Representing a Function in terms of a Basis}\label{bases}
	As previously described, a basis of a Hilbert space can be used to express its elements using a Fourier expansion. Let $\{\phi_i(t) \: \vert \: i \in \mathcal{I}\}$ be a basis used to express a realization $x(t)$ of $X(\omega)$. The following equation shows how to use a basis to express a function as a weighted sum of its elements.
	\begin{equation}
		x(t) = \sum_{j \in \mathcal{I}} a_j \phi_j(t) 
	\end{equation}
	
	One important question in this context is how the coefficients $a_j$ for $j \in \mathcal{I}$ are derived for a given function $x(t)$. In this paper, this process will remain a black box, but detailed information on the derivation of these coefficients can be found in chapter 4 of \cite{ramsay_functional_2005}.
	Three examples of bases used to approximate elements of $\mathbb{L}^2[0,1]$ in practice and in the later parts of this paper are explained in the following. Diagrams showing these bases are found in Appendix \ref{Basis_Plots}.
	
	\newpage
	\paragraph{Monomial Basis}\label{Monomial_basis}
	A straightforward idea to approximate functions in $\mathbb{L}^2[0,1]$ is to take inspiration from the well-known Taylor expansion and to use the monomials as a basis. For entire functions $f(t)$, such as polynomials, the exponential function, or trigonometric functions, we can express the function as a potentially infinite but converging sum of weighted monomials.
	\begin{equation}\label{Taylor_expansion}
		f(t) = \sum_{i = 1}^{\infty}a_i t^i \quad \text{where} \quad a_i = \frac{f^{(i)}(0)}{i!}
	\end{equation}
	
	The monomials are only a basis for the space of entire functions and not for $\mathbb{L}^2[0,1]$. However, even for functions that do not fall into this category, using a truncated Taylor expansion around a chosen point can lead to reasonable approximations around this specific point or even on $\mathbb{R}$ as a whole.
	As in the case of the Taylor expansion, it is not necessary to approximate a function around zero, as shown above. Instead, one can introduce a shift parameter $\alpha$. This leads to the following formalization of the Monomial basis.
	\begin{equation}
		\phi_{i}^{M}(t) = (t-\alpha)^i \quad i \in \mathbb{N}
	\end{equation}

	Due to the implementation of our simulation, we limit our paper to the case of $\alpha = 0$. A different choice of $\alpha$ could lead to performance improvements.
	As the monomials are not pairwise orthogonal, this basis is prone to collinearity problems, which can result in numerically unstable estimates. This restricts the number of basis functions that can be used in the estimation procedures limiting its ability to capture pronounced local peculiarities. The effects of this problem will be addressed in more detail in later parts of this paper. The limitation to a low number of monomials can additionally lead to undesirable behavior away from the point of evaluation (cf. \cite{ramsay_functional_2005}).
	\vspace{-0.2cm}
	
	\paragraph{Fourier Basis}
	In the same way the Monomial basis is connected to the Taylor series, the Fourier basis corresponds to the Fourier series, which can be used to decompose a periodic function into trigonometric functions. Equation \ref{Fourier_Series} shows an example for a function $s(x)$ with a period of $T = 1$.
	\begin{equation}\label{Fourier_Series}
		s(x) = \frac{A_0}{2} + \sum_{i = 1}^{\infty} A_i \cos(2\pi i x - \phi_i) = \frac{a_0}{2} + \sum_{i = 1}^{\infty}\left[a_i \cos(2\pi i x) + b_i \sin(2\pi i x)\right]
	\end{equation}
	
	Typically, the series is represented in the so-called amplitude-phase form. This, however, is impractical for the estimation procedures shown in the later parts of this paper due to the phase shift parameter. Rewriting the series in its sine-cosine form, as shown above, is necessary.
	The Fourier basis for $\mathbb{L}^2[0,1]$ is thus given by the following sequence of functions defined on $[0,1]$ directly corresponding to the terms of the sine-cosine form of the Fourier series.
	
	\begin{equation}
		\phi_{i}^{F}(x) = 
		\begin{cases}
			1 & \text{if} \quad i = 1\\
			\sqrt{2} \cos(\pi i x) & \text{if} \quad i \quad \text{is even} \\
			\sqrt{2} \sin(\pi (i-1)x) & \text{otherwise}
		\end{cases}
	\end{equation}
	To stay true to the original amplitude-phase form, it is reasonable to restrict the number of Fourier basis functions to odd-numbered values. The Fourier basis' elements are cyclical which is useful to expand functions that represent a periodic or seasonal underlying process. Due to the nature of the trigonometric functions, it is especially suitable to expand functions with a similar curvature across their domain, resulting in uniformly smooth expansions. (cf. \cite{ramsay_functional_2005})
	\vspace{-0.2cm}

	\paragraph{B-spline Basis} Following chapter 3.5 from \cite{ramsay_functional_2005}, splines are defined by first dividing an interval of interest $[\tau_0, \tau_B]$ into $B$ subintervals of non-negative length bounded by a non-decreasing sequence of points $(\tau_b)_{b = 0, \: \dots \:, B}$ called knots. On each subinterval, a spline is a polynomial of order $m = n+1$ where $n$ is its degree. If there are no multiplicities in the set of knots, the polynomials on neighboring subintervals share derivatives up to order $m-2$ at the boundary knot $\tau_b$. A typical case that is often used in practice is an equidistant grid of knots. In some settings, however, it can be sensible to place multiple knots at the same value to replicate specific properties of the data structure, allowing for a reduced number of matching derivatives at the corresponding knots. For the purposes of this paper, we will focus on the case of equidistant knots without multiplicity at inner knots.\\
	
	B-splines are a specific system of spline functions developed by \cite{de_boor_practical_1978} and are defined by a recursive procedure. Let $\phi_{b,m}^{BS}(x)$ for $b \in \{1, \: \dots \:, B + m - 2\}$ be a B-spline of order $m$ for an interval $[\tau_0, \tau_B]$ and knots $\{\tau_b \: \vert \: b = 0, \:\dots \:, B\}$, then it is defined by the Cox-de Boor recursion formula as follows. 
	\begin{equation}
		\begin{split}
			\phi_{b,0}^{BS}(x) = &
			\begin{cases}
				1 & \text{if} \quad x \in \left[\tau_b, \tau_{b+1}\right)\\
				0 & \text{otherwise}
			\end{cases}\\ \\
			\phi_{b,m}^{BS}(x) = &\frac{x - \tau_b}{\tau_{b+m} - \tau_b} \phi_{b,m-1}^{BS}(x) + \frac{\tau_{b+m+1} - x}{\tau_{b+m+1} - \tau_{b+1}} \phi_{b+1,m-1}^{BS}(x)
		\end{split}
	\end{equation}
	
	As this equation references knots that are not defined by the original vector of knots, implementations typically repeat the knots at the boundaries of the interval, $\tau_0$ and $\tau_B$, an additional $m$ times. This padding of the knot vector allows calculating every object that is needed for the definition of the basis over the original set of knots.\\	
	This does not lead to a basis of $\mathbb{L}^2[0,1]$ as the closed span of this finite sequence of functions is not equal to $\mathbb{L}^2[0,1]$. However, to focus on specific approximation errors in the later parts of this paper, we will assume that a B-spline basis representation of a function in $\mathbb{L}^2[0,1]$ will serve as a sufficient approximation for an appropriately chosen number of B-spline basis functions. 
	As the B-spline basis does not have infinitely many elements, it is slightly misleading to speak of truncating the B-spline basis at a truncation parameter $L$. For the sake of keeping the notation concise, we will still keep this notation. By convention, truncating a B-spline basis at truncation parameter $L$ shall mean using a B-spline basis consisting of $L$ functions from this point on.
	
	\subsection{Approximation and Smoothing via Basis Truncation}
	Realized curves from a data set can be expressed in terms of a chosen functional basis. For this expansion, it is possible to use a complete basis of $\mathbb{L}^2[0,1]$. In many cases, this is not a desirable approach as this expansion can introduce high amounts of variance or even lead to the approximation of noise in the sample curves, the latter being a typical case of overfitting. To combat this problem, smoothing methods such as acceleration penalties are employed to enforce a degree of smoothness in the analyzed curves. On the other hand, important information on the curves could be missed by oversmoothing the data, giving too much weight to a chosen penalty term leading to oversmoothing and loss of valuable information. This is a typical occurrence of the Bias-Variance tradeoff.\\
	
	\newpage
	A usual setup is described by \cite{Goldsmith_2011} in which an explicit smoothing term is used to tune the smoothness of the estimator $\hat{\beta}(t)$ while setting setting the number of basis functions sufficiently high. To provide intuition for this approach, let 
	\begin{equation}
		PSSE_\lambda(\alpha, \beta) = \sum_{i = 1}^{N} \left[ Y_i -\alpha -\int_0^1 \beta(t)X_i(t)dt \right]^2 + \lambda \int \left[D^m\beta(t)\right]^2 dt
	\end{equation}
	
	denote the penalized residual sum of squares for the derivative of order $m$. A typical choice is the second derivative, as highly variable functions are expected to exhibit large second derivatives and therefore a larger penalty. The smoothing parameter $\lambda$ is set to minimize the $PSSE_\lambda(\alpha, \beta)$, which can be achieved by different criteria as shown in \cite{ThomasLee_2003}.
	
	A different approach is to enforce smoothing by limiting the number of basis functions in the approximation of the functional objects. Here, the parameter of choice is not a weighting term for the penalty but the number of basis functions. Exploring this alternative smoothing method in two different estimation procedures is the main focus of this paper. A truncated basis expansion as described above is given in Equation \ref{basis_exp}.
	\begin{equation}\label{basis_exp}
		X(\omega_0) = x(t) = \sum_{j \in \mathcal{I}} a_j(\omega_0) \phi_j(t) = \sum_{j = 1}^{L} a_j(\omega_0) \phi_j(t) + \delta(t) \approx \sum_{j = 1}^{L} a_j(\omega_0) \phi_j(t)
	\end{equation}

	Here, $\delta(t)$ is the truncation error and $L \leq \max\limits_{j \in \mathcal{I}}(j)$ for all $L \in \mathcal{I}$.  In later parts, this approximation error is explicitly denoted in the derivation and then omitted for the final approximations. $L$ can be chosen subjectively, but also through applying data-driven methods such as Cross-Validation (CV). Figure \ref{Different_Expansions} shows the effect of choosing different numbers of basis functions for one NIR absorption curve from the gasoline data set, which exemplifies the tradeoffs at the core of the truncation parameter choice.
			
	\begin{figure}[H]
		\includegraphics[width=\textwidth]{../Graphics/basis_expansions.pdf}
		\caption{B-spline Approximations of NIR Absorption Spectra with different Basis Truncation Parameters}
		\label{Different_Expansions}
	\end{figure}
	
	\newpage
	\subsection{Karhunen-Lo\'{e}ve Expansion and Empirical Eigenbases}\hypertarget{KL}{}
	Given a random function $X: \Omega \mapsto \mathbb{L}^2[0,1]$, it is possible to represent its realizations in terms of the stochastic process. To do so, we define the mean and covariance functions of $X(\omega)$.
	\begin{equation}\label{MeanFunction}
		\mu(t) = \mathbb{E}\left[ X(\omega)(t) \right]
	\end{equation}
	\begin{equation}\label{CovarianceFunction}
		c(t,s) = \mathbb{E}\big[ \left( X(\omega)(t) - \mu(t) \right) \left( X(\omega)(s) - \mu(s) \right) \big]
	\end{equation}

	Here, the $c: [0,1] \times [0,1] \rightarrow \mathbb{R}$ are Hilbert-Schmidt Kernels and $K$ is a corresponding Hilbert-Schmidt integral operator\footnote{Definitions of Hilbert-Schmidt Kernel and Integral Operator are provided in Appendix \ref{def_HS}.} $K : \nu \rightarrow K \nu$ for $\nu \in \mathbb{L}^{2}[0,1]$ defined by the following equation.
	\begin{equation}\label{HSKernal}
		[K \nu](t) = \int_{0}^{1}c(t,s) \nu(s)ds
	\end{equation}

	The operator $K$ has orthonormal basis functions $\nu^{m} \in \mathbb{L}^{2}[0,1]$, each corresponding to an Eigenvalue $\lambda^{m}$ (cf. \cite{alexanderian_KLexpansion_2015}).
	Theoretical considerations lead to the result that $X(\omega)$ can be represented in the following form, called its Karhunen-Lo\'{e}ve expansion.\footnote{Proofs for these theorems are provided in Appendix \ref{Proof1} and \ref{Proof2}.} 
	\begin{equation}\label{KarhunenLoeve}
		X(\omega)(t) = \mu(t) + \sum_{m = 1}^{\infty} \xi^m(\omega) \nu^m(t), \quad \xi^m(\omega) =  \int_{0}^{1} \left(X(\omega)(s) - \mu(s)\right) \nu^m(s) \mathrm{d}s
	\end{equation}

	Here, the $\nu^m$ are defined by the countable set of solutions $\{(\lambda^m, \nu^m) \: \vert \: m \in \mathbb{N}\}$ of $[K \nu](t) = \lambda \nu(t)$. The random variables $\xi^{m}(\omega)$, which are called scores, satisfy the following properties.
	\begin{multicols}{2}
		\begin{enumerate}
			\item $\mathbb{E}\left[\xi^m(\omega)\right] = 0$
			\item $Cov\left(\xi^m(\omega), \xi^n(\omega)\right) = 0$ if $m \neq n$
			\item $Var\left(\xi^m(\omega)\right) = \lambda^m$
		\end{enumerate}
	\end{multicols}

	As in the well-known setting of principal component analysis, the Eigenvalues correspond to the variance in the random function explained by the corresponding Eigenfunction. 
	Therefore, ordering the Eigenfunctions according to their corresponding Eigenvalues $\lambda^{1} \geq \lambda^{2} \geq \dots \geq 0$ is useful for approximation purposes. Due to this property, a functional observation can often be approximated well by using a limited number of the Eigenfunctions of its generating random process. Moreover, our data generating process for the simulation study is based on this property as explained in Section \ref{similar_curves}.\\
	The concept of principal components, which can be extended to the functional setting.\\
	Let $\{x_1(t),\: \dots, \: x_n(t)\}$ be a set of i.i.d. realizations generated by a random function $X(\omega)$.	Define the following sample analogs for the mean and covariance functions.
	\begin{equation}
		\hat{\mu}(t) = \frac{1}{N}\sum_{i = 1}^{N}x_i(t)
	\end{equation}
	\begin{equation}
		\hat{c}(t,s) = \frac{1}{N} \sum_{i = 1}^{N} \left(x_i(t) - \hat{\mu}(t)\right) \left(x_i(s) - \hat{\mu}(s)\right)
	\end{equation}

	\newpage
	With these it is possible to derive a set of sample analogs $\{(\hat{\lambda}^m, \hat{\nu}^m) \: \vert \: m = 1,\: \dots\:, \mathcal{O}\}$ for $\{(\lambda^m, \nu^m) \: \vert \: m = 1, 2, \:\dots\}$ as the solutions of the following equation. 
	\begin{equation}
		\int_{0}^{1}\hat{c}(t,s)\hat{\nu}(s) \mathrm{d}s = \hat{\lambda} \hat{\nu}(t)
	\end{equation}

	In the following, we will call the $\hat{\nu}^m(t)$ Functional Principal Components (FPC's) to distinguish the sample analogs from the theoretical Eigenfunctions $\nu^m(t)$. As in the case of ordinary principal components, the number of FPC's corresponding to non-zero Eigenvalues is limited (cf. \cite{ramsay_functional_2005}). As each curve is infinite-dimensional, there is no upper limit to this number due to the dimensionality. However, the number of curves still imposes an upper limit of $N-1$ non-zero Eigenvalues, where $N$ is the number of curves in the data set. A second upper limit is given by the number $L$ of basis functions available in the derivation of the FPC's. Define therefore $\mathcal{O} := \min(N-1, L)$.
	In case the number of FPC's is limited by $L$, an exact representation of the functional observations is impossible, introducing an approximation error. For the purposes of this paper, we will not address this error and assume that an exact representation is possible. Using this assumption, $\mathcal{O}$ is implicitly assumed to always equal $N-1$.
	
	These sample analogs naturally lead to the following representation of each sample curve $x_i(t)$.
	\begin{equation}
		x_i(t) = \hat{\mu}(t) + \sum_{j = 1}^{\mathcal{O}} \hat{\xi}_{i}^{m} \hat{\nu}^{m}(t)
	\end{equation}
	Here, the $\hat{\xi}_{i}^m$ are derived as 
	\begin{equation}
		\hat{\xi}_i^m(\omega) = \langle x_i - \hat{\mu}, \hat{\nu}^m\rangle = \int_{0}^{1} \left(x_i(s) - \hat{\mu}(s)\right) \hat{\nu}^m(s) \mathrm{d}s
	\end{equation}
	In reality, these calculations are often implemented using basis representations of both the functional principal components $\hat{\nu}^m$ and the observations $x_i(t)$ leading to the following representation. For clarity, we assume that the bases used for the expansion of both the observations and the coefficient function are proper bases of $\mathbb{L}^2[0,1]$ and can therefore be used to express the corresponding objects exactly. The $\delta_i^J(s)$ denotes the approximation error of the demeaned observation due to the basis truncation and $\delta_{\nu}^{m,K}(s)$ denotes the corresponding error for the FPC.
	\begin{equation}\label{FPCA_basis_expansion}
		\begin{split}
			\hat{\xi}_{i}^m & = \int_{0}^{1} {\color{red}\left( x_i(s) - \hat{\mu}(s)\right)} {\color{blue}\hat{\nu}^m(s)} \mathrm{d}s
			= \int_{0}^{1} {\color{red}\left(\sum_{j \in \mathcal{I}} a_{i,j} \phi_j(s)\right)} {\color{blue}\left(\sum_{k \in \mathcal{L}} d_{k}^m \psi_{k}(s)\right)} \mathrm{d}s \\
			& = \int_{0}^{1} {\color{red}\left(\sum_{j = 1}^{J} a_{i,j} \phi_j(s) + \delta_i^J(s)\right)} {\color{blue}\left(\sum_{k = 1}^{K} d_{k}^m \psi_{k}(s) + \delta_{\nu}^{m,K}(s)\right)} \mathrm{d}s \\
			& = \sum_{j = 1}^{J} \left[a_{i,j}\sum_{k = 1}^{K} d_{k}^m \int_{0}^{1} \phi_j(s) \psi_{k}(s)\mathrm{d}s \right] +  \sum_{k = 1}^{K} d_{k}^m \int_{0}^{1} \delta_i^J(s) \psi_{k}(s) \mathrm{d}s \\
			& \quad + \sum_{j = 1}^{J} a_{i,j} \int_{0}^{1}\phi_j(s) \delta_{\nu}^{m,K}(s) \mathrm{d}s + \int_{0}^{1} \delta_i^J(s) \delta_{\nu}^{m,K}(s) \mathrm{d}s
		\end{split}
	\end{equation}

	\newpage
	In practice, a typical choice is to use the same basis $\left(\phi_j(t)\right)_{j \in \mathcal{I}}$ and the same truncation parameter $L$ for the basis expansion of the demeaned observations $\left(x_i(t) - \hat{\mu}(t)\right)$ and the functional principal components $\hat{\nu}^m$. This leads to the following simplification of Equation \ref{FPCA_basis_expansion}.
	\begin{equation}\label{score_approx}
		\begin{split}
			\hat{\xi}_{i}^m &= \sum_{j = 1}^{L} \left[a_{i,j}\sum_{k = 1}^{L} d_{k}^m \int_{0}^{1} \phi_j(s) \phi_{k}(s)\mathrm{d}s \right] +  \sum_{k = 1}^{L} d_{k}^m \int_{0}^{1} \delta_i^L(s) \phi_{k}(s) \mathrm{d}s \\
			& \quad + \sum_{j = 1}^{L} a_{i,j} \int_{0}^{1}\phi_j(s) \delta_{\nu}^{m,L}(s) \mathrm{d}s + \int_{0}^{1} \delta_i^L(s) \delta_{\nu}^{m,L}(s) \mathrm{d}s
		\end{split}
	\end{equation}
	We define the following objects
		\begin{align}
			\tilde{\xi}^{m,L}_{i} & := \sum_{j = 1}^{L} \left[a_{i,j}\sum_{k = 1}^{L} d_{k}^m \int_{0}^{1} \phi_j(s) \phi_{k}(s)\mathrm{d}s \right] 
			& \delta_{\xi, i}^L & := \hat{\xi}_{i}^m - \tilde{\xi}^{m,L}_{i} \\
			\tilde{\nu}^{m,L}(t) & := \sum_{k = 1}^{L} d_{k}^m \phi_{k}(t) 
			& \delta_{\nu}^{m, L}(t) & := \hat{\nu}^m(t) - \tilde{\nu}^{m,L}(t)
		\end{align}	
	
	This method of deriving or approximating the Eigenfunctions and scores from a data set is introduced in chapter 8.4.2 of \cite{ramsay_functional_2005} and implemented in the R package \textit{fda}. The following considerations and results of the simulation study might provide information about the performance of this method in a scenario where a limited number of basis functions is provided to the method instead of using more conventional smoothing approaches.
	
	\subsection{Scalar-on-Function Regression}\label{Scalar_on_function_regression}
	In the simple scalar setting, one of the essential tools in econometrics is linear regression. To motivate the jump from multivariate regression to scalar-on-function regression, assume a data generating process as follows.
	\begin{equation}
		Y = X\beta + \epsilon
	\end{equation}
	
	Here, $Y$ is the vector of response variables, $X$ is the matrix containing the corresponding regressors in its columns, and $\beta = (\beta_0, \beta_1, \: \dots, \beta_p)'$ is the vector containing the unknown coefficients.
	In this finite-dimensional setting, one important question is how to estimate the unknown coefficients $\beta$. The most famous estimator, the Ordinary Least Squares estimator, fulfills this purpose.
	\begin{equation}
		\hat{\beta}_{OLS} = (X'X)^{-1}X'Y
	\end{equation}
	
	The concept of linear regression can be extended to the setting of functional data, where a scalar response variable is assumed to be dependent on a functional regressor. 
	Integrating over the product of an observation with the coefficient function is not the only functional that can be used to create a data generating process involving functional observations. However, it is the most typical as it naturally extends the intuition from multiple linear regression to the realm of infinite-dimensional objects. Therefore, we will always assume a data generating process as follows in this paper.
	\begin{equation}\label{DGP}
		Y(\omega) = \alpha + \int_{0}^{1} \beta(s)X(\omega)(s) \mathrm{d}s + \epsilon(\omega)
	\end{equation}
	
	\newpage
	Similar to the finite-dimensional setting, a challenge is to estimate the unknown coefficient function $\beta(t)$ given a data set containing realizations of a random function and associated scalar response variables. A simple extension of the OLS estimator to allow for infinite-dimensional objects is not possible. Therefore, other options have to be considered, two of which are explained in the following.
	
	\subsubsection{Estimation using Basis-Representation}\label{basis_exp_transf}
	The most common way to make this problem tractable is via a basis representation of $\beta(t)$. Let $\{\phi_j(t) \: \vert \: j \in \mathcal{I}\}$ be a basis of $\mathbb{L}^2[0,1]$ and represent $\beta(t)$ in terms of this basis.
	\begin{equation}
		\beta(t) = \sum_{j \in \mathcal{I}} b_j \phi_j(t)
	\end{equation}
	This enables us to write Equation \ref{DGP} using this representation to obtain a formulation as a sum of scalar random variables $Z_j(\omega)$.	
	\begin{equation}
		\begin{split}
			Y(\omega) & = \alpha + \int_{0}^{1} {\color{blue}\beta(s)} X(\omega)(s)\mathrm{d}s + \epsilon(\omega)
			= \alpha + \int_{0}^{1}\left[{\color{blue}\left(\sum_{j \in \mathcal{I}} b_j \phi_j(s)\right)} X(\omega)(s) \right]\mathrm{d}s + \epsilon(\omega) \\
			& = \alpha + \sum_{j \in \mathcal{I}} \left[b_j \textcolor{red}{\int_{0}^{1} X(\omega)(s) \phi_j(s)\mathrm{d}s}\right] + \epsilon(\omega)
		      = \alpha + \sum_{j \in \mathcal{I}} b_j \textcolor{red}{Z_j(\omega)} + \epsilon(\omega)
		\end{split}
	\end{equation}
	
	This representation translates the original problem of regressing a scalar on a continuously observed function to a problem where a scalar is regressed on what is possibly a countably infinite sequence of regressors. Using a truncation of the basis at some parameter $J$ can be used to make this problem tractable if we assume that the approximation error created by this truncation is small.
	\begin{equation}
		\begin{split}
			Y(\omega) & = \alpha + \int_{0}^{1}\left[{\color{blue}\left(\sum_{j = 1}^{J} b_j \phi_j(s) + \delta_{\beta}^{J}(s)\right)} X(\omega)(s) \right]\mathrm{d}s + \epsilon(\omega) \\
			& = \alpha + \sum_{j = 1}^{J} b_j \int_{0}^{1} \phi_j(s) X(\omega)(s) \mathrm{d}s +  \int_{0}^{1} \delta_{\beta}^{J}(s) X(\omega)(s) \mathrm{d}s + \epsilon(\omega)
		\end{split}
	\end{equation}

	In practice, it is common to not only express the coefficient function in terms of a basis but also the observations. Therefore two bases, $\left(\phi_j(t)\right)_{j \in \mathcal{I}}$ and $\left(\psi_k(t)\right)_{k \in \mathcal{L}}$, and two corresponding truncation parameters, $J$ and $K$, can be chosen. This leads to the following representation.
	\begin{equation}\label{complicated_eq}
		\begin{split}
			Y(\omega) & = \alpha + \int_{0}^{1} {\color{blue}\beta(s)} {\color{red}X(\omega)(s)}\mathrm{d}s + \epsilon(\omega)
			 = \alpha + \int_{0}^{1}\left[{\color{blue}\left(\sum_{j \in \mathcal{I}} b_j  \phi_j(s)\right)} {\color{red}\left(\sum_{k \in \mathcal{L}} a_k(\omega)  \psi_k(s)\right)} \right]\mathrm{d}s + \epsilon(\omega) \\
			& = \alpha + \int_{0}^{1}\left[{\color{blue}\left(\sum_{j = 1}^{J} b_j  \phi_j(s) + \delta_{\beta}^{J}(s)\right)} {\color{red}\left(\sum_{k = 1}^K a_k(\omega)  \psi_k(s) + \delta_{X}^{K}(\omega)(s)\right)} \right]\mathrm{d}s + \epsilon(\omega)\\
			& = \alpha + \sum_{j = 1}^J b_j \left[\sum_{k = 1}^K a_k(\omega) \int_{0}^{1} \phi_j(s) \psi_k(s) \mathrm{d}s\right] + \sum_{j = 1}^{J} b_j  \int_{0}^{1} \phi_j(s) \delta_{X}^{K}(\omega)(s) \mathrm{d}s\\
			& \quad \quad + \sum_{k = 1}^{K} a_k(\omega)  \int_{0}^{1} \delta_{\beta}^{J}(s)\psi_k(s) \mathrm{d}s + \epsilon(\omega) + \int_{0}^{1}  \delta_{\beta}^{J}(s) \delta_{X}^{K}(\omega)(s) \mathrm{d}s
		\end{split}
	\end{equation}

	\newpage
	A typical choice in this scenario is to use the same functional basis $\left(\phi_j(t)\right)_{j \in \mathcal{I}}$ and the same truncation parameter $L$ for both the coefficient function and the approximation of the observations. Using the following notation 
	\begin{equation}
			\tilde{Z}_j(\omega) = \sum_{k = 1}^{L} \left[a_k(\omega) \int_{0}^{1} \phi_j(s) \phi_k(s) \mathrm{d}s \right] \quad j = 1, \dots, L
	\end{equation}

	this leads to a considerable simplification of Equation \ref{complicated_eq} and an approximation by omitting the terms containing truncation errors.
	\begin{equation}\label{simplified_model_basis_equation}
		\begin{split}
			Y(\omega) &= \alpha + \sum_{j = 1}^{L} b_j \tilde{Z}_j(\omega) + \sum_{j = 1}^{L} b_j  \int_{0}^{1} \phi_j(s) \delta_{X}^{L}(\omega)(s) \mathrm{d}s\\
			& \quad \quad + \sum_{k = 1}^{L} a_k  \int_{0}^{1} \delta_{\beta}^{L}(s)\phi_k(s) \mathrm{d}s + \epsilon(\omega) + \int_{0}^{1}  \delta_{\beta}^{L}(s) \delta_{X}^{L}(\omega)(s) \mathrm{d}s + \epsilon(\omega)\\
			& \approx \alpha + \sum_{j = 1}^{J} b_j \tilde{Z}_j(\omega) + \epsilon(\omega)
		\end{split}
	\end{equation}

	A model in the form of Equation \ref{simplified_model_basis_equation} naturally lends itself to be estimated using theory from multivariate linear regression. 
	Define therefore the following objects.
	\begin{equation}\label{regressor_mat_1}
		Y = \begin{pmatrix}
			y_1 \\ \vdots \\ y_n
		\end{pmatrix}, \quad
		Z = \begin{pmatrix}
			1 & \tilde{Z}_{1,1} & \dots & \tilde{Z}_{1,J} \\
			\vdots & \vdots & \ddots & \vdots \\
			1 & \tilde{Z}_{N,1} & \dots & \tilde{Z}_{N,J}
		\end{pmatrix}
	\end{equation}
	
	Then an OLS estimator can be calculated in the usual way to obtain an estimate for the values of $\alpha$ and $b_j$, and an estimate of the coefficient function can be derived accordingly.
	\begin{equation}
		b^L = \left(Z'Z\right)^{-1}Z'Y \in \mathbb{R}^{L+1} \quad \hat{\alpha} = b_{1}^{L} \quad \hat{\beta}^L(t) = \sum_{j = 1}^{J} b_{j+1}^L \phi_j(t)
	\end{equation}

	The performance of this estimation procedure depends in part on the quality of the approximation in Equation \ref{simplified_model_basis_equation}. Due to the nature of basis representations, the overall approximation error $\delta(t)$ can only decrease with the number of basis functions. However, in specific points $t_0$ which might coincide with high predictive power for the response, the error $\delta(t_0)$ could increase, potentially increasing the prediction error.
	In addition, we have the potential benefits of  smoothing such as reducing the potential of overfitting functional noise and reducing the degrees of freedom in the estimation. Because of this tradeoff between a reduction in the overall approximation error and beneficial smoothing properties, increasing the number of basis function and thereby decreasing the approximation error is not necessarily beneficial for the prediction accuracy. 
	Therefore, the behavior of the estimator when changing the truncation parameter $L$ is not intuitively clear even if we stay in the same basis system.
	
	\subsubsection{Estimation using Functional Principal Components}\label{fpc_exp_transf}
	
	Using the Karhunen-Lo\'{e}ve Expansion to represent $X(\omega)$, it is also possible to express the data generating process in terms of the Eigenfunctions of $X(\omega)$.
	\begin{equation}
		\begin{split}
			Y(\omega) &= \alpha + \int_{0}^{1} {\color{red}X(\omega)(s)} \beta(s) \mathrm{d}s + \epsilon(\omega)
			= \alpha + \int_{0}^{1} {\color{red}\left(\mu(s) + \sum_{m = 1}^{\infty} \xi^m(\omega) \nu^m(s)\right)} \beta(s) \mathrm{d}s + \epsilon(\omega)\\
			&= {\color{teal}\alpha + \int_{0}^{1} \mu(s) \beta(s) \mathrm{d}s} + \sum_{m = 1}^{\infty} \xi^m(\omega) {\color{violet}\int_{0}^{1} \nu^m(s) \beta(s) \mathrm{d}s} + \epsilon(\omega)
			= {\color{teal}\bar{\alpha}} + \sum_{m = 1}^{\infty} \xi^m(\omega) {\color{violet}\beta^m} + \epsilon(\omega)
		\end{split}
	\end{equation}

	As these theoretical Eigenfunctions and Eigenvalues are unknown, the corresponding equation in sample analogs is more interesting as a representation of an observation.
	\begin{equation}
		\begin{split}
			y_i &= \alpha + \int_{0}^{1} {\color{red}x_i(s)} \beta(s) \mathrm{d}s + \epsilon_i
			= \alpha + \int_{0}^{1} {\color{red}\left(\hat{\mu}(s) + \sum_{m = 1}^{\mathcal{O}} \hat{\xi}^m_i \hat{\nu}^m(s)\right)} \beta(s) \mathrm{d}s + \epsilon_i\\
			&= {\color{teal}\alpha + \int_{0}^{1} \hat{\mu}(s) \beta(s) \mathrm{d}s} + \sum_{m = 1}^{\mathcal{O}} \hat{\xi}^m_i {\color{violet}\int_{0}^{1} \hat{\nu}^m(s) \beta(s) \mathrm{d}s} + \epsilon_i
			= {\color{teal}\bar{\alpha}} + \sum_{m = 1}^{\mathcal{O}} \hat{\xi}^m_i {\color{violet}\hat{\beta}^m} + \epsilon_i
		\end{split}
	\end{equation}

	This is a simplification as in most implementations, the coefficient function and the principal components are also expressed or derived in terms of a basis. Introducing both concepts one step at a time leads to the following complication if we first introduce an expansion of the coefficient function.
	\begin{equation}
		\begin{split}
			y_i &= \alpha + \int_{0}^{1} {\color{red}x_i(s)} {\color{blue}\beta(s)} \mathrm{d}s + \epsilon_i
			= \alpha + \int_{0}^{1} {\color{red}\left(\hat{\mu}(s) + \sum_{m = 1}^{\mathcal{O}} \hat{\xi}^{m}_i \hat{\nu}^{m}(s)\right)} {\color{blue}\left(\sum_{j \in \mathcal{I}} b_j \phi_j(s)\right)} \mathrm{d}s + \epsilon_i\\
			&= \alpha + \int_{0}^{1} \left[ \sum_{j \in \mathcal{I}} b_j \phi_j(s) \hat{\mu}(s) + \sum_{m = 1}^{\mathcal{O}} \left[ \hat{\xi}^m_i \sum_{j \in \mathcal{I}} b_j \hat{\nu}^m(s) \phi_j(s) \right] \right] \mathrm{d}s + \epsilon_i \\
			&= \alpha + \sum_{j \in \mathcal{I}} b_j \int_{0}^{1} \phi_j(s) \hat{\mu}(s) \mathrm{d}s + \sum_{m = 1}^{\mathcal{O}} \left[ \hat{\xi}^m_i \sum_{j \in \mathcal{I}} b_j \int_{0}^{1}\hat{\nu}^m(s) \phi_j(s) \mathrm{d}s \right] + \epsilon_i
		\end{split}
	\end{equation}

	Truncating the basis used for expansion of the coefficient function introduces an approximation error.
	\begin{equation}\label{fpcr_reg_both_expansions}
		\begin{split}
			y_i &= \alpha + \int_{0}^{1} \left(\hat{\mu}(s) + \sum_{m = 1}^{\mathcal{O}} \hat{\xi}^{m}_i \hat{\nu}^{m}(s)\right) {\color{blue}\left(\sum_{j = 1}^{J} b_j \phi_j(s) + \delta_{\beta}^{J}(s)\right)} \mathrm{d}s + \epsilon_i\\
			&= \alpha + \sum_{j = 1}^{J} b_j \int_{0}^{1} \phi_j(s) \hat{\mu}(s) \mathrm{d}s + \int_{0}^{1} \delta_{\beta}^{J}(s) \hat{\mu}(s) \mathrm{d}s + \sum_{m = 1}^{\mathcal{O}} \left[ \hat{\xi}^m_i \sum_{j = 1}^{J} b_j \int_{0}^{1}\hat{\nu}^m(s) \phi_j(s) \mathrm{d}s \right] \\
			& \quad \quad + \sum_{m = 1}^{\mathcal{O}} \left[ \hat{\xi}^m_i \int_{0}^{1}\hat{\nu}^m(s) \delta_{\beta}^{J}(s) \mathrm{d}s \right] + \epsilon_i
		\end{split}
	\end{equation}

	If we additionally derive and approximate the principal components and corresponding scores using a truncated basis representation as in Equation \ref{score_approx}, we obtain the following. To not complicate things unnecessarily, the following equation assumes that the same basis $\left(\phi_j(t)\right)_{j \in \mathcal{I}}$ was used in the derivation of the principal components and the expansion of the coefficient function. Additionally, the following approximation truncates the basis for the expansion of the coefficient function at the same parameter $L$ that was used for the approximation of the principal components and scores.\\
	Defining the following notation
	\begin{equation}
		\tilde{\alpha}^L = \alpha + \sum_{j = 1}^{L} b_j \int_{0}^{1} \phi_j(s) \hat{\mu}(s) \mathrm{d}s + \int_{0}^{1} \delta_{\beta}^{L}(s) \hat{\mu}(s) \mathrm{d}s
	\end{equation}
	
	we can express Equation \ref{fpcr_reg_both_expansions} as follows.
	\begin{equation}
		\begin{split}
			y_i & = \tilde{\alpha}^L
			+ \sum_{m = 1}^{\mathcal{O}} \left[ \left({\color{red}\tilde{\xi}^{m,L}_{i}} + {\color{blue}\delta_{\xi, i}^{m, L} }\right) \sum_{j = 1}^{L} b_j \int_{0}^{1} \left({\color{teal}\tilde{\nu}^{m,L}(s)} + {\color{violet}\delta_{\nu}^{m,L}(s)} \right) \phi_j(s) \mathrm{d}s \right] 
			+ \epsilon_i \\
			& = \tilde{\alpha}^L
			+ \sum_{m = 1}^{\mathcal{O}} \left[ {\color{red}\tilde{\xi}^{m,L}_{i}} \sum_{j = 1}^{L} b_j \int_{0}^{1} {\color{teal}\tilde{\nu}^{m,L}(s)} \phi_j(s) \mathrm{d}s \right] 
			+ \sum_{m = 1}^{\mathcal{O}} \left[ {\color{red}\tilde{\xi}^{m,L}_{i}} \sum_{j = 1}^{L} b_j \int_{0}^{1} {\color{violet}\delta_{\nu}^{m,L}(s)} \phi_j(s) \mathrm{d}s \right] \\
			& \quad \quad \: \: + \sum_{m = 1}^{\mathcal{O}} \left[ {\color{blue}\delta_{\xi, i}^{m, L}} \sum_{j = 1}^{L} b_j \int_{0}^{1} {\color{teal}\tilde{\nu}^{m,L}(s)} \phi_j(s) \mathrm{d}s \right] 
			+ \sum_{m = 1}^{\mathcal{O}} \left[ {\color{blue}\delta_{\xi, i}^{m, L}} \sum_{j = 1}^{L} b_j \int_{0}^{1} {\color{violet}\delta_{\nu}^{m,L}(s)} \phi_j(s) \mathrm{d}s \right]
			+ \epsilon_i \\
			& \approx \tilde{\alpha}^L
			+ \sum_{m = 1}^{\mathcal{O}} \left[ {\color{red}\tilde{\xi}^{m,L}_{i}} \sum_{j = 1}^{L} b_j \int_{0}^{1} {\color{teal}\tilde{\nu}^{m,L}(s)} \phi_j(s) \mathrm{d}s \right] + \epsilon_i
		\end{split}
	\end{equation}

	The parameter $M \in \{1,\: \dots \:, \mathcal{O}\}$ corresponds to the chosen number of principal components and constitutes another choice in the approximation. Using $M$ functional principal components leads to the following approximation.
	\begin{equation}
		y_i \approx \tilde{\alpha}^L
		+ \sum_{m = 1}^{M} \left[ \tilde{\xi}^{m,L}_{i} {\color{red}\sum_{j = 1}^{L} b_j \int_{0}^{1} \tilde{\nu}^{m,L}(s) \phi_j(s) \mathrm{d}s} \right] + \epsilon_i 
		= \tilde{\alpha}^L
		+ \sum_{m = 1}^{M} \tilde{\xi}^{m,L}_{i} {\color{red}\bar{b}^{m,L}} + \epsilon_i
	\end{equation}
	
	As in the previous section, this equation lends itself for estimation with OLS and we can define the following objects.
	\begin{equation}
		Y = \begin{pmatrix}
			y_1 \\ \vdots \\ y_n
		\end{pmatrix}, \quad
		Z = \begin{pmatrix}
			1 & \tilde{\xi}^{1,L}_{1} & \dots & \tilde{\xi}^{M,L}_{1} \\
			\vdots & \vdots & \ddots & \vdots \\
			1 & \tilde{\xi}^{1,L}_{N} & \dots & \tilde{\xi}^{M,L}_{N}
		\end{pmatrix}
	\end{equation}
	
	We can then derive the following estimator for $\tilde{\alpha}^L$ and $\bar{b}^{m,L} \quad m = 1, \dots, M$
	\begin{equation}
		\tilde{b}^{L,M} = \left(Z'Z\right)^{-1}Z'Y \in \mathbb{R}^{M+1} \quad \hat{\tilde{\alpha}} = \tilde{b}_{1}^{L,M} \quad \hat{\beta}(t) = \sum_{m = 1}^{M} \tilde{b}_{m + 1}^{L,M} \tilde{\nu}^{m,L}(t)
	\end{equation}

	As in the previous case, the performance of this estimation depends in parts on the quality of the approximations in the derivation of this estimator. The interactions are even more complex. Even though the approximations exhibit smaller errors when a larger number of basis functions is used, the interplay of the chosen way of smoothing in the construction of the FPC's and the choice of a number of FPC's used in the linear regression makes the behavior of this estimator difficult to predict. 

	\nocite{alexanderian_KLexpansion_2015}
	\nocite{kokoszka_introduction_2017}
	\nocite{hsing_theoretical_2015}
	\nocite{ramsay_functional_2005}
	\nocite{horvath_inference_2012}
	\nocite{cai_prediction_2006}
	\nocite{levitin_introduction_2007}

	\section{Simulation Study}\label{Simulation}

	\subsection{Motivation}\label{sim_motivation}
	
	Instead of generating data from scratch, we use the gasoline data set from R package \textit{refund}, which consists of 60 samples of Near-infrared absorption curves measured in increments of 2 nm from 900 to 1,700 nm, and a response variable, the octane number. We chose this setup to improve the approach towards the application in which we predict the octane numbers from the gasoline data set.	
	We introduced different bases in Section \ref{bases} and demonstrated the importance of the truncation parameter $L$ for the estimation in Section \ref{Scalar_on_function_regression}. For the simulation study, we use Basis Expansion Regression and Functional Principal Component Regression (FPCR) with the introduced bases and focus on choosing the truncation parameter $L$ as well as the number of FPC's by ten-fold CV using the Mean-Squared Prediction Error (MSPE).
	
	In practice, the number of FPC's is often chosen by using the lowest number that explains a specified proportion of variance of the smoothed curves (cf. \cite{kokoszka_introduction_2017}). This might not be optimal since FPC's with smaller Eigenvalues may have a more significant influence on the prediction (cf. \cite{Jolliffe_1982}). In this simulation certain Eigenfunctions could correspond to certain chemical compounds and vibration overtones in the absorption bands of the spectrum that could have high predictive power, but explain only little variability in the NIR curves shown in Appendix \ref{NIR}.
	
	\subsection{Generating Similar Curves}\label{similar_curves}
	
	To avoid small sample problems, we generated 200 similar curves denoted by $NIR_{sim}$, from the absorption curves of the gasoline data set denoted $NIR_{orig}$. Our approach is motivated by \hyperlink{KL}{Karhunen-Lo\'{e}ve Expansion}. First, the initial curves are mapped to the interval $[0,1]$ as described in Section \ref{L_2} and expressed in terms of a cubic B-spline basis which is created using 50 knots. In the implementation of the \textit{fda} package, these 50 knots account for 52 basis functions ($50+4-2$). These smoothed curves are centered before applying the \hyperlink{KL}{Karhunen-Lo\'{e}ve Expansion}. For the purposes of data generation, we assume that the scores follow a multivariate normal distribution $\mathring{\xi} = \left(\mathring{\xi}^{1},\: \dots \:, \mathring{\xi}^{M}\right)' \sim \mathcal{N}(0_M, \; diag(\hat{\lambda}^1,\: \dots\:, \hat{\lambda}^M))$. Finally, we obtain the generated curves $NIR_{sim}$ as realizations of
	\begin{equation}
		\mathring{X}(\omega)(t) = \hat{\mu}(t) + \sum_{m = 1}^{M} \mathring{\xi}^m(\omega) \tilde{\nu}^{m,L}(t)
	\end{equation}
	
	where $\mathring{X}(\omega)(t)$, $\hat{\mu}(t)$ and $\tilde{\nu}^{m,L}(t)$  are approximated as vectors in $\mathbb{R}^{401}$ for $M =$ 30 FPC's.
    
    \subsection{Simulation setup}
	The simulation study follows \cite{Reiss_2007b} as a guideline. Two different true coefficient functions,  $f_1(t)$ and  $f_2(t)$, that differ in their smoothness, are created to compare our methods with differing true coefficient functions.
	
	\begin{equation}
    	f_1(t) = 401 \left[ 2\sin(0.5\pi t) + 4\sin(1.5 \pi t) + 5\sin(2.5 \pi t) \right]
    \end{equation}
    \begin{equation}
    	\begin{split}
    		f_2(t) = 401  \Bigg[ & 1.5 \exp{\left(\frac{-0.5(t-0.3)^2}{0.02^2}\right)} - 4 \exp{\left(\frac{-0.5(t-0.45)^2}{0.015^2}\right)} \\
    				 & + 8 \exp{\left(\frac{-0.5(t-0.6)^2}{0.02^2}\right)} -  \exp{\left(\frac{-0.5(t-0.8)^2}{0.03^2}\right)} \Bigg]
    	\end{split}
    \end{equation}
    
    The function $f_2(t)$ was generated referring to \cite{cardot_bumpyfunction_2002} while function $f_1(t)$ directly follows \cite{Reiss_2007b}. 
    %and its inner product $\langle NIR_{sim}, f_1 \rangle$ creates responses that are similar to the original octane numbers of the gasoline data set. 

		\begin{figure}[H]
			\centering
			\begin{minipage}{.5\textwidth}
				\centering
  				\includegraphics[width=\textwidth]{../Graphics/f1_plot.pdf}
  				\caption{$f_1(t)$, smooth function}
  				\label{fig:test1}
			\end{minipage}%
			\begin{minipage}{.5\textwidth}
	  			\centering
  				\includegraphics[width=\textwidth]{../Graphics/f2_plot.pdf}
  				\caption{$f_2(t)$, bumpy function}
  				\label{fig:test2}
			\end{minipage}
		\end{figure}
		
		 We created two different error-terms by generating i.i.d. standard normal errors  $Z \sim \mathcal{N}(0,1)$ and multiplying it by two error variations $\sigma_e $. The error variations represent different signal-to-noise ratios of the responses to test the methods with low and high amounts of noise in the responses. They are created such that the squared multiple correlation coefficient $R^2 = var(\langle X, f\rangle) / (var(\langle X, f\rangle) + \sigma^2_{e})$ is equal to 0.9 and 0.6. The two error terms are then used to generate two sets of responses for $f \in \{f_1(t), f_2(t)\}$.	
		\begin{equation}
			\begin{split}
				Y_{1,f} & = \langle NIR_{sim}, f\rangle + Z  \biggl\lbrack\frac{var(\langle NIR_{orig}, f\rangle)}{0.9} - var(\langle NIR_{orig}, f\rangle)\biggr\rbrack \\
				Y_{2,f} & = \langle NIR_{sim}, f\rangle + Z  \biggl\lbrack\frac{var(\langle NIR_{orig}, f\rangle)}{0.6} - var(\langle NIR_{orig}, f\rangle)\biggr\rbrack
			\end{split}
		\end{equation}
		
		 In total, we created four combinations, using the two coefficient functions and the two error terms. These four combinations are then used with a different number of Monomial basis functions, cubic B-spline basis functions and Fourier basis functions to predict the generated responses using the basis expansion approach and the FPCR approach.\\

		To obtain valid out of sample properties for the FPCR, within each of the ten ten-fold cross-validation splits, we calculated the first $n_{FPC} \in \{2,3,4\}$ FPC's for the training set $\mathcal{T}$, which was smoothed with the respective basis specification. The approximated Eigenfunctions $\tilde{\nu}^{m, L, \mathcal{T}}$ are then used to estimate the scores of the holdout set $\mathcal{H}$, $\check{\xi}_{i}^{m, L, \mathcal{H}}$,  by Equation \ref{Score_est}.
		\begin{equation}\label{Score_est}
			\check{\xi}_{i}^{m, \mathcal{H},L}
			= \int_{0}^{1} {\color{red}\left(\sum_{j = 1}^{L} a_{i,j}^{\mathcal{H}} \phi_j(s)\right)} {\color{blue}\left(\sum_{k = 1}^{L} d_{k}^{m, L, \mathcal{T}} \phi_{k}(s)\right)} \mathrm{d}s
		    = \sum_{j = 1}^{L} \left[ a_{i,j}^{\mathcal{H}}\sum_{k = 1}^{L}  d_{k}^{m, \mathcal{T}} \int_{0}^{1} \phi_j(s) \phi_{k}(s)\mathrm{d}s\right]
		\end{equation}
		Here we use the following basis expansions.
		\begin{equation}
			\begin{split}
				{\color{red} X_{i}^{\mathcal{H}}(t) - \hat{\mu}^{\mathcal{T}}(t)} 
				&= {\color{red}\sum_{j \in \mathcal{I}} a_{i,j}^{\mathcal{H}} \phi_j(t)}
				= {\color{red}\sum_{j = 1}^{L} a_{i,j}^{\mathcal{H}} \phi_j(t) + \delta_{i}^{L, \mathcal{H}}(t)}
				\approx {\color{red}\sum_{j = 1}^{L} a_{i,j}^{\mathcal{H}} \phi_j(t)} \\
				{\color{blue}\tilde{\nu}^{m, L, \mathcal{T}} } &= {\color{blue}\sum_{k = 1}^{L} d_{k}^{m, L, \mathcal{T}} \phi_{k}(s)}
			\end{split}
		\end{equation}
		
		In total, we conducted 5000 repetitions for each specified model, basis system and basis function. The models are denoted as a combination of the function $f \in \{f_1(t), f_2(t)\}$	and created responses $Y \in \{Y_1, Y_2\}$.
		
	\subsection{Results}	
	The discussed results and figures of $\hat{\beta}(t)$ for the simulation can be found in Appendix \ref{Tables_sim} and \ref{Estimates_sim}.
	 
	\subsubsection{Basis Expansion Regression}
	These results follow from the model introduced in Section \ref{basis_exp_transf}, in which we transform the smoothed curves to perform scalar-on-function regression. Examining the results, it appears that the cross-validated MSPE exhibits a convex behavior in the number of basis functions for all bases.
	
	\paragraph{Monomial Basis}
	Due to the increasing collinearity problems for higher numbers of Monomial basis functions, simulations were conducted up until the sixth monomial, which already shows signs of numerical instability. For reasons outlined in Section \ref{Monomial_basis}, they seem suited for $f_1$, where it shows a better performance than B-splines. A hypothesis for this could be that $f_1$ is an entire function, which can be well approximated with a power series. In the case of $f_2$, this basis shows the weakest performance out of all bases, for which Figures \ref{basis_expansion_2_1} and \ref{basis_expansion_2_2} provide visual evidence. It seems like $\hat{\beta}(t)$ is not changing in the amount of noise and shows exaggerated behavior at the boundaries. This weakness is especially pronounced in the MSPE for $f_2,Y_1$. For $f_1$, the simulation selects 5(3) and for $f_2$ 5(5) Monomial basis functions for the high (low) signal-to-noise ratio. 
	\vspace{-0.2cm}
	
	\paragraph{B-spline Basis}
	Simulations with B-spline basis functions were possible from 4 to 18 basis functions. From 18 onward, the simulations  ran into problems of numerical instability. This might be caused by the fact, that the B-splines are not pairwise orthogonal.
	Function $f_1$ chooses 5(4) B-spline basis functions for the high (low) signal-to-noise ratio to obtain the best fit and shows the worst performance of the three bases. An explanation might be its extreme behavior at the boundaries and the exaggeration of the peculiarities of $f_1$ (Figures \ref{basis_expansion_1_1} and \ref{basis_expansion_1_2}). This is especially pronounced for the lower signal-to-noise ratio. For $f_2$, 11(6) B-spline basis functions are chosen for the high (low) signal-to-noise ratio. The B-spline basis outperforms the Monomial basis in $f_2$ but comes second to the Fourier basis. While the basis seems to recognize the peculiarities in $f_2,Y_1$, it struggles for the noisy responses in $f_2,Y_2$ (Figures \ref{basis_expansion_2_1} and \ref{basis_expansion_2_2}). With the low signal-to-noise ratio in the responses, the simulation chooses a smaller number of basis functions, which could prevent overfitting the scalar noise using $\hat{\beta}(t)$.
	\vspace{-0.2cm}
	
	\paragraph{Fourier Basis}
	For $f_1$, the simulation chooses a smaller number of Fourier basis functions, 5(3) and a higher number for $f_2$, 9(7) for the setup with the high (low) signal-to-noise ratio. As for the B-splines basis, the simulation chooses a smaller number of basis functions for the noisy responses and a higher number for the high signal-to-noise responses. The Fourier basis performs the best for each setup for the Basis Expansion Regression. Several reasons could contribute to this. First, especially $f_1$ shows similar curvature across the domain while the curvature of $f_2$ does at least not display any erratic jumps. Second, both functions feature periodic behavior. Third, $f_2$ does have the same value at the start- and the end of the interval, lending itself to a periodic representation.
	\vspace{-0.2cm}
	
	\subsubsection{Functional Principal Component Regression}
	The model used for the FPCR is described in Section \ref{fpc_exp_transf}. The selection of the truncation parameter $L$ is difficult since the approximated Eigenfunctions from the decomposition are influenced by the choice of $L$. In addition, the choice of the number of FPC's adds to the complexity of the model. Some eigenfunctions, which might contain important information, could correspond to small Eigenvalues and, therefore, be omitted as described in Section \ref{sim_motivation}. But since the FPCR is ultimately estimated with a linear model containing the corresponding scores as regressors, the relevant degrees of freedom in the estimation of the ultimate model are not affected by $L$, but only by $n_{FPC} \in \{2, 3, 4 \}$. It seems that neither a convex behavior of the MSPE nor any clear relationship can be observed between the number of basis functions and the number of FPC's. This might be because this dependency is too complex to draw conclusions with this simulation study. Therefore, we will limit ourselves to a brief description. In this simulation, the cross-validated choice of basis functions indicates that the FPCR might not take the differing signal-to-noise ratios of the responses into account. This is indicated by the fact that the same number of basis functions is chosen for the different signal-to-noise ratios. A possible explanation might be that the FPC's, which affect the relevant degrees of freedom, are solely calculated from the smoothed curves, not considering the responses.
	
	\paragraph{Monomial Basis}
	For $f_1,Y_1$, the cross-validated MSPE seems to decrease in the number of FPC's and chosen basis functions for both the high and the low signal-to-noise ratio ($4,5,6$ basis functions for $n_{FPC} = 2, 3, 4$). For the noisy responses of $f_1,Y_2$ we find signs of overfitting for $n_{FPC}$ = 4. In  $f_2$, we also observe a decreasing MSPE in the number of FPC's, but no clear relationship for the chosen basis functions. The Monomial basis shows the weakest performance out of all three bases in each setting.
	\vspace{-0.2cm}
	
	\paragraph{B-spline Basis}
	For $f_1,Y_1$, the MSPE suggests that models with a higher number of FPC's perform better. While the number of basis functions stays at five for $n_{FPC} = 2,3$, it increases to 6 basis functions for $n_{FPC} = 4$. In $f_1,Y_2$, the cross-validated MSPE is only slightly affected by $n_{FPC}$, but lowest for $n_{FPC} = 3$, indicating the possibility of overfitting for $n_{FPC} = 4$. Similar to the two setups with $f_1$, the chosen number of basis functions for $f_2$ is increasing in the number of FPC's (4, 6, 23 for $n_{FPC} = 2, 3, 4$).
	\vspace{-0.2cm}
	
	\paragraph{Fourier Basis}
	In $f_1,Y_1$, the MSPE decreases in $n_{FPC}$ while the results in $f_1,Y_2$ might indicate overfitting for $n_{FPC} = 4$. Both specifications using $f_1$ select the lowest number of basis functions possible. This is interesting as the number of FPC's puts a binding lower bound on the number of basis functions used in their construction. An exploration of the implications of this fact is out of the scope of this paper. Both configurations of $f_2$ use 5, 15 and 7 basis functions for $n_{FPC} = 2, 3, 4$. The basis shows the lowest MSPE for all four settings.
	\vspace{-0.2cm}	
	
	\subsection{Interpretation and Relevance for Application}
	A possible explanation applicable to the setups performing Basis Expansion Regression might be the effect of the Bias-Variance tradeoff and the following hypothesis: For $f_1$, only little bias seems to be introduced when choosing a small number of basis functions. For $f_2$, a higher number of basis functions seems appropriate. This could result from the inherent peculiarities of $f_2$ that are more pronounced with higher $L$, therefore choosing higher numbers of basis functions since the bias is decreasing faster than the variance is increasing in the number of basis functions compared to $f_1$. The described convex behavior of the MSPE might also be partially attributable to the bias-variance tradeoff.  
	This convex behavior was not observed for the FPCR where no clear relationship between the basis functions and the number of FPC's could be found. The MSPE of FPCR seems to be comparatively more stable when changing the number of basis functions than for Basis Expansion Regression. It seems like the chosen number of FPC's is more significant, especially for the two high signal-to-noise ratio settings which display a higher absolute and relative decrease of MSPE in $n_{FPC}$.\\
	
	To examine this relationship further, we conducted additional simulations (Appendix \ref{Tables_sim_additional}) for B-spline basis specifications with a large truncation parameter $L \in \{50, 70\}$ for $n_{FPC} \in \{2, \dots, 7 \}$. We can find potential signs of overfitting in all setups for $L = 70$. For $L = 50$, signs of overfitting can be observed for the setups with the noisy responses. The additional simulations revealed evidence for a relationship from $n_{FPC} = 3$ onwards for all setups. For $n_{FPC} \in \{3, \dots, 7 \}$, the ten-fold cross-validated MSPE is always lower for 50 than for 70 basis functions. This might be the first sign of an $L$ that is chosen too large and creates undersmoothing. It seems that once a sufficient number of basis functions is used to expand the curves, FPCR with B-splines performs better using a lower number of basis functions. While a higher number of $n_{FPC}$ decreases the MSPE of $f_1,Y_1$ and $f_2,Y_1$ for $L = 50$ compared to the main simulations, this does not hold true for the two setups with noisy responses. This strengthens our hypothesis of the high importance of $n_{FPC}$ for the high signal-to-noise ratio setup. For $n_{FPC} \in \{2, 3, 4 \}$, the additional B-splines simulations perform worse for these high number of basis functions compared to the main simulation.  This might indicate beneficial attributes of a lower truncation parameter in settings with a lower number of $n_{FPC}$.
	
	\section{Application}\label{Application}
		Our application uses the methods and insights from the previous sections to predict the octane numbers of the gasoline data set.
		Although the simulation study granted valuable insights into the different methods in our four different settings, it is not enough to determine the optimal method and choice of basis for the application since there is too much uncertainty involved. To point out some sources of uncertainty: First, differing from the simulation setup, we do not know the true coefficient function. Visual inspection of the estimated coefficient functions shown in Appendix \ref{Estimates_Appl} fuels the hypothesis that the real coefficient function might be more similar to $f_2$ than to $f_1$, but the insights from the two functions are not sufficient to draw any conclusion. Second, we have no information about the signal-to-noise ratio of the measured octane numbers. Third, to generate similar curves, we made assumptions about the distribution of $\mathring{\xi}$ that are not applicable in this application where we do not know their distribution.
		 
		Therefore, we rerun all specifications for the gasoline data set and use the results of the simulation study to improve our understanding of the results. The application is designed analogously to the simulation study: The 60 spectra of the gasoline data set are used to estimate a coefficient function, predict the reported octane numbers, and evaluate the results via MSPE using 12 fold CV with 5 elements per fold. In total, we conducted 1000 repetitions for each setting, choosing different fold partitions for each run. Over all specifications, the best-performing model is FPCR using 4 FPC's built on 7 Fourier basis functions (0.0435), followed by Basis Expansion Regression using 10 B-spline basis functions (0.04574).
		

		
	\subsection{Interpretation of Results}
		\subsubsection{Basis Expansion Regression}
		The cross-validated MSPE selects 5 basis functions for the Monomial basis. For B-Splines, the cross-validation selects 10 basis functions and 9 basis functions for the Fourier basis. Comparing $\hat{\beta}(t)$ for the different bases away from the boundaries (Figure \ref{basis_expansion_estimate_appl}), the B-spline and Fourier bases show similar behavior, differing from the Monomial estimate, which might be attributable to the lower number of basis functions. Especially at the lower boundary, the Monomial basis shows exaggerated behavior. However, we must exercise caution since from $L=6$ onwards, we could not calculate stable results for the Monomial basis. The MSPE for B-splines (0.04574) and Fourier (0.04808) are similar, while the Monomial basis (0.24181) shows distinctly worse properties. 
		In contrast to the simulation study, the B-splines basis outperforms the Fourier basis. Driving factors for this improved performance could be that, first, we do not assume a periodic coefficient function with the same start- and end value and second, that the Fourier basis enforces identical start and end values on the curves of NIR. Note that the reported MSPE for the B-spline and Fourier basis are close to the errors reported in the simulation study for $f_2,Y_1$, which might be caused by the hypothesized similarities between the actual coefficient function and $f_2$, but also by a similar high signal-to-noise ratio of the octane numbers. 
	
		\subsubsection{Functional Principal Component Regression}
		The best performance for all numbers of FPC's was achieved with the Fourier Basis, as for the FPCR in the simulation study. In contrast to the simulation study, no evidence of overfitting was found for the best basis choices in any of the three bases. The MSPE for the optimal specification appears to be strictly decreasing in the number of FPC's. As for the simulation study, the interpretation of the results with respect to the chosen basis is difficult: Referring to the plotted estimates $\hat{\beta}(t)$ (Figures \ref{FPCR2_estimate_appl}, \ref{FPCR3_estimate_appl} and \ref{FPCR4_estimate_appl}), it appears to be the case that the higher $n_{FPC}$ is, the more similar the behavior of the corresponding estimates becomes. Differing from the simulation study, the number of basis functions steadily increases for the Monomial basis. The behavior for the B-spline basis is similar to the one reported in the simulation study (basis functions increasing in $n_{FPC}$). 
	
	%\nocite{carey_life_2002}

	\section{Outlook}\label{Outlook}
	
	\subsection*{Limitations}
	
	\paragraph{Insights from Simulation cannot be Extended to More General Functions}
	As described, the properties of the two used functions $f \in \{f_1(t), f_2(t)\}$ influence the results. Although the resulting MSPE and visual inspection provided some evidence to support the hypothesis that the true coefficient function of the original gasoline data set might be similar to $f_2$, other factors that were not addressed could have caused this. Therefore, this simulation is not sufficient to base reasonable claims concerning more general coefficient functions on it.
	\vspace{-0.2cm}
	
	\paragraph{Collinearity Problems in Basis Expansion Regression}
	Except for the Fourier basis, the Basis Expansion Regression was in part limited by the numerical instability of the estimation procedure. This is mainly due to an increase in collinearity of the derived regressor matrix shown in Definition \ref{regressor_mat_1}. This problem is inherent to basis systems whose functions are not pairwise orthogonal, such as the Monomial or B-splines bases, but gets more pronounced the more functions we add to the basis system and the higher the correlation between those functions.
	The numeric instability of the inversion of this matrix makes the estimates unreliable and therefore can make this approach infeasible for non-orthogonal bases in settings where the characteristics of the data set demand a higher number of basis functions than is feasible due to the properties of the estimation procedure.
	\vspace{-0.2cm}
	
	\subsection*{Possible Extensions}
	
	\paragraph{Orthogonal Polynomials to Solve Collinearity Problems of Monomial Basis}
	To address the collinearity problems described earlier, one possible idea would be to use a system of pair-wise orthogonal polynomials as a basis instead of the Monomial basis. One example is the system of Legendre polynomials which are orthogonal by construction and have the same closed span as the monomials (cf. \cite{Dattoli_2001}). Due to their orthogonality, the problem of collinearity in the regressor matrix is greatly reduced, which could allow for larger numbers of basis functions in the Basis Expansion Regression approach. The first eight Legendre polynomials are shown in Appendix \ref{Basis_Plots}.
	\vspace{-0.2cm}
	
	\paragraph{Comparison to Penalty Based Smoothing Procedures}
	In contrast to the more typical approach of using a arbitrary, large number of basis functions and smoothing using a penalty term involving, e.g. the second derivative, this paper focuses on smoothing by using a smaller number of basis functions. As the next step to the analysis of this paper, we could compare both methods to see in which settings different approaches to smoothing perform better and if a possible combination of both approaches could be advantageous.
	\vspace{-0.2cm}
	
	\paragraph{Larger Range for the Number of Specifications for FPCR}
	The chosen specifications for the number of principal components and the number of basis functions used for FPCR might be insufficient to obtain a complete picture of the performance of this procedure when combined with smoothing using basis truncation. It would therefore be interesting to focus more attention on this limitation of this paper in a future extension to explore a broader array of possible specifications which might allow the derivation of more detailed insights.
	\vspace{-0.2cm}
	
	\paragraph{Input from Physics}
	It could be exciting to combine the findings of this paper with theoretical expertise from the field of physics. This could inform the choice of models and be conducive to a more meaningful interpretation of the estimates. For example, specific parts of the NIR spectrum could possibly be linked to specific molecules associated with the octane number. This could guide, for example, the construction of a B-spline basis that focuses on the relevant parts of the spectrum by using a vector of knots founded by field expertise. Alternatively, it could be used to link specific principal components to chemical compounds, which would be useful for the model selection and interesting for the interpretation of estimates.
	\vspace{-0.2cm}
	
	\nocite{James.2009} %(shape-restrictions)
	
	\newpage
	\section{Appendix}
	
	\subsection{Near-infrared (NIR) Spectroscopy}\label{NIR}
	NIR spectroscopy is a spectroscopic method that uses the near-infrared region of the electromagnetic spectrum (From 780 nm to 2500 nm). It measures the absorption and interaction of this spectrum of radiation with the sample. NIR spectroscopy is not only faster and cheaper than the standard test procedure – another significant advantage is that it does not need a reagent and thus does not destroy the sample. It is used for analysis in different sectors and fields, like the agrochemical industry and healthcare. Its non-invasive nature makes it also an asset for medical applications like the monitoring of diabetes in which NIR spectroscopy can detect the worsening of the blood glucose metabolic dysfunction (cf . \cite{FR_li_et_al_2020}). \\
	In the context of this paper, the gasoline data set which is used for the simulation and the application is constructed using NIR spectroscopy. According to \cite{Bohacs_Ovadi_Salgo1998}, NIR spectroscopy is a feasible method for the analysis of gasoline since most of the absorption that is observed within the described interval of wavelengths is due to overtones and interactions of the radiation with chemical combinations (carbon–hydrogen, carbon–carbon, carbon–oxygen, carbonyl associated groups, aromatic stretching, and deformation vibration of the hydrocarbon molecules). While this paper focuses on the prediction of the octane number of gasoline, other research focuses  on different properties of gasoline such as the olefin, naphtaenic and aromatic content (Parisi et al. 1990, as cited in \cite{Bohacs_Ovadi_Salgo1998}) or the distillation characteristics (Pauls 1985, as cited in \cite{Bohacs_Ovadi_Salgo1998})
	\vspace{0.5cm}

	\begin{figure}[H]
		\begin{center}
			\begin{overpic}[width = \textwidth]{../Graphics/NIR_data.pdf}
				\put(7, 30){
					\frame{\includegraphics[width = 0.4\textwidth]{../Graphics/FinderSD.jpg}}
				}
			\end{overpic}
			\caption{NIR absorption spectra from the Gasoline Data Set and Finder SD (a Near-Infrared-Spectroscope built by HiperScan GmbH)	\\
			(Source: https://www.hiperscan.com/files/apoident/uploads/Bilder/Neue\_Website/Produkte/FinderSD.jpg)}
		\end{center}
	\end{figure}

	\newpage
	
	\subsection{Basis Plots}\label{Basis_Plots}
	
	\begin{figure}[H]\label{Fourier_basis}
		\includegraphics[width = \textwidth]{../Graphics/Fourier_Basis.pdf}
		\caption{Fourier basis functions for $i = 1,\dots,7$}
	\end{figure}
	
	\begin{figure}[H]\label{B-spline_basis}
		\includegraphics[width = \textwidth]{../Graphics/Bspline_Basis.pdf}
		\caption{B-spline basis functions of order 4 for 8 equidistant knots on $[0,1]$}
	\end{figure}

	\begin{figure}[H]\label{monomial_basis}
		\includegraphics[width = \textwidth]{../Graphics/Monomial_Basis.pdf}
		\caption{Monomial basis functions of degree 0 to 7}
	\end{figure}

	\begin{figure}[H]\label{Legendre_basis}
		\includegraphics[width = \textwidth]{../Graphics/Legendre_Plot.pdf}
		\caption{Legendre Polynomials of degree 0 to 7}
	\end{figure}

	\input{tables_simulation.tex}
	
	\newpage
	\subsection{Simulation - Coefficient Function Estimates}\label{Estimates_sim}
	
	\begin{figure}[H]
		\centering
		\begin{minipage}{.5\textwidth}
			\centering
			\includegraphics[width=\textwidth]{../Graphics/Curve\_Estimates/basis_expansion_1_1.pdf}
			\caption{Basis Expansion Regression - $f_1, Y_1$}
			\label{basis_expansion_1_1}
		\end{minipage}%
		\begin{minipage}{.5\textwidth}
			\centering
			\includegraphics[width=\textwidth]{../Graphics/Curve\_Estimates/basis_expansion_1_2.pdf}
			\caption{Basis Expansion Regression - $f_1, Y_2$}
			\label{basis_expansion_1_2}
		\end{minipage}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\begin{minipage}{.5\textwidth}
			\centering
			\includegraphics[width=\textwidth]{../Graphics/Curve\_Estimates/basis_expansion_2_1.pdf}
			\caption{Basis Expansion Regression - $f_2, Y_1$}
			\label{basis_expansion_2_1}
		\end{minipage}%
		\begin{minipage}{.5\textwidth}
			\centering
			\includegraphics[width=\textwidth]{../Graphics/Curve\_Estimates/basis_expansion_2_2.pdf}
			\caption{Basis Expansion Regression - $f_2, Y_2$}
			\label{basis_expansion_2_2}
		\end{minipage}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\begin{minipage}{.5\textwidth}
			\centering
			\includegraphics[width=\textwidth]{../Graphics/Curve\_Estimates/fpcr_nharm2_1_1.pdf}
			\caption{FPC Regression, 2 harmonics - $f_1, Y_1$}
			\label{fpcr_nharm2_1_1}
		\end{minipage}%
		\begin{minipage}{.5\textwidth}
			\centering
			\includegraphics[width=\textwidth]{../Graphics/Curve\_Estimates/fpcr_nharm2_1_2.pdf}
			\caption{FPC Regression, 2 harmonics - $f_1, Y_2$}
			\label{fpcr_nharm2_1_2}
			
		\end{minipage}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\begin{minipage}{.5\textwidth}
			\centering
			\includegraphics[width=\textwidth]{../Graphics/Curve\_Estimates/fpcr_nharm2_2_1.pdf}
			\caption{FPC Regression, 2 harmonics - $f_2, Y_1$}
			\label{fpcr_nharm2_2_1}
		\end{minipage}%
		\begin{minipage}{.5\textwidth}
			\centering
			\includegraphics[width=\textwidth]{../Graphics/Curve\_Estimates/fpcr_nharm2_2_2.pdf}
			\caption{FPC Regression, 2 harmonics - $f_2, Y_2$}
			\label{fpcr_nharm2_2_2}
		\end{minipage}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\begin{minipage}{.5\textwidth}
			\centering
			\includegraphics[width=\textwidth]{../Graphics/Curve\_Estimates/fpcr_nharm3_1_1.pdf}
			\caption{FPC Regression, 3 harmonics - $f_1, Y_1$}
			\label{fpcr_nharm3_1_1}
		\end{minipage}%
		\begin{minipage}{.5\textwidth}
			\centering
			\includegraphics[width=\textwidth]{../Graphics/Curve\_Estimates/fpcr_nharm3_1_2.pdf}
			\caption{FPC Regression, 3 harmonics - $f_1, Y_2$}
			\label{fpcr_nharm3_1_2}
		\end{minipage}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\begin{minipage}{.5\textwidth}
			\centering
			\includegraphics[width=\textwidth]{../Graphics/Curve\_Estimates/fpcr_nharm3_2_1.pdf}
			\caption{FPC Regression, 3 harmonics - $f_2, Y_1$}
			\label{fpcr_nharm3_2_1}
		\end{minipage}%
		\begin{minipage}{.5\textwidth}
			\centering
			\includegraphics[width=\textwidth]{../Graphics/Curve\_Estimates/fpcr_nharm3_2_2.pdf}
			\caption{FPC Regression, 3 harmonics - $f_2, Y_2$}
			\label{fpcr_nharm3_2_2}
		\end{minipage}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\begin{minipage}{.5\textwidth}
			\centering
			\includegraphics[width=\textwidth]{../Graphics/Curve\_Estimates/fpcr_nharm4_1_1.pdf}
			\caption{FPC Regression, 4 harmonics - $f_1, Y_1$}
			\label{fpcr_nharm4_1_1}
		\end{minipage}%
		\begin{minipage}{.5\textwidth}
			\centering
			\includegraphics[width=\textwidth]{../Graphics/Curve\_Estimates/fpcr_nharm4_1_2.pdf}
			\caption{FPC Regression, 4 harmonics - $f_1, Y_2$}
			\label{fpcr_nharm4_1_2}
		\end{minipage}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\begin{minipage}{.5\textwidth}
			\centering
			\includegraphics[width=\textwidth]{../Graphics/Curve\_Estimates/fpcr_nharm4_2_1.pdf}
			\caption{FPC Regression, 4 harmonics - $f_2, Y_1$}
			\label{fpcr_nharm4_2_1}
		\end{minipage}%
		\begin{minipage}{.5\textwidth}
			\centering
			\includegraphics[width=\textwidth]{../Graphics/Curve\_Estimates/fpcr_nharm4_2_2.pdf}
			\caption{FPC Regression, 4 harmonics - $f_2, Y_2$}
			\label{fpcr_nharm4_2_2}
		\end{minipage}
	\end{figure}

\newpage
\input{tables_application.tex}

	\newpage
	
	\subsection{Application - Coefficient Function Estimates}\label{Estimates_Appl}
	\begin{figure}[H]
		\centering
		\begin{minipage}{.5\textwidth}
			\centering
			\includegraphics[width=\textwidth]{../Graphics/Appl\_Curve\_Estimates/appl\_basis\_expansion.pdf}
			\caption{Basis Expansion Regression}
			\label{basis_expansion_estimate_appl}
		\end{minipage}%
		\begin{minipage}{.5\textwidth}
			\centering
			\includegraphics[width=\textwidth]{../Graphics/Appl\_Curve\_Estimates/fpcr\_nharm2.pdf}
			\caption{2 Functional Principal Components}
			\label{FPCR2_estimate_appl}			
		\end{minipage}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\begin{minipage}{.5\textwidth}
			\centering
			\includegraphics[width=\textwidth]{../Graphics/Appl\_Curve\_Estimates/fpcr\_nharm3.pdf}
			\caption{3 Functional Principal Components}
			\label{FPCR3_estimate_appl}	
		\end{minipage}%
		\begin{minipage}{.5\textwidth}
			\centering
			\includegraphics[width=\textwidth]{../Graphics/Appl\_Curve\_Estimates/fpcr\_nharm4.pdf}
			\caption{4 Functional Principal Components}
			\label{FPCR4_estimate_appl}	
		\end{minipage}
	\end{figure}
	
	\section{Definitions and Proofs}
	The following proofs are adapted from \cite{alexanderian_KLexpansion_2015}.
	
	\subsection{Definition (Hilbert-Schmidt Integral Operator)}\label{def_HS}
	Given a bounded domain $\mathcal{A} \subset \mathbb{R}^n$, we call a function $c : \mathcal{A} \times \mathcal{A} \rightarrow \mathbb{R}$ a Hilbert-Schmidt kernel if
	\begin{equation}
		\int_{\mathcal{A}} \int_{\mathcal{A}} \vert c(x,y) \vert ^2 \mathrm{d}x \mathrm{d}y < \infty
	\end{equation}
	where $c \in \mathbb{L}^2(\mathcal{A} \times \mathcal{A})$. Let $K$ be an integral operator on $\mathbb{L}^2(\mathcal{A})$ such that $K : \nu \rightarrow K \nu$ for $\nu \in \mathbb{L}^2(\mathcal{A})$, defined by Equation \ref{HS_OP}.
	\begin{equation}\label{HS_OP}
		[K\nu](x) = \int_{\mathcal{A}} c(x,y) \nu(y) \mathrm{d}y
	\end{equation}

	When an integral operator $K$ is linear and bounded, it is called a Hilbert-Schmidt integral operator. The linearity of the operator $K$ is proved as shown in Equation \ref{Linearity}. Additionally, assume that $\alpha, \beta \in \mathbb{R}$ and $\theta \in \mathbb{L}^2(\mathcal{A})$.
	
	\begin{equation}\label{Linearity}
		\begin{split}
			[K (\alpha \nu + \beta \theta)](x) =& \int_{\mathcal{A}} c(x,y)(\alpha \nu(y) + \beta \theta(y)) \mathrm{d}y\\
			= & \int_{\mathcal{A}} c(x,y) \alpha \nu(y) \mathrm{d}y + \int_{\mathcal{A}} c(x,y) \beta \theta(y) \mathrm{d}y\\
			= & \alpha\int_{\mathcal{A}} c(x,y) \nu (y) \mathrm{d}y + \beta\int_{\mathcal{A}} c(x,y) \theta (y) \mathrm{d}y\\
			= & \alpha[K\nu](x) + \beta[K\theta](x)
		\end{split}
	\end{equation}
	For boundedness of the operator $K$ we need to show the following.
	\begin{equation}
		\begin{split}
			\Vert K\nu \Vert^{2}_{\mathbb{L}^2(\mathcal{A})} = & \int_{\mathcal{A}} \biggl\vert [K\nu](x) \biggr\vert^2 \mathrm{d}x\\
			= &\int_{\mathcal{A}} \biggl\vert \int_{\mathcal{A}}c(x,y)\nu(y)\mathrm{d}y \biggr\vert^2\mathrm{d}x\\
			\leq & \int_{\mathcal{A}}\biggl(\int_{\mathcal{A}}\vert c(x,y) \vert^2\mathrm{d}y\biggr) \biggl(\int_{\mathcal{A}}\vert \nu(y) \vert ^2 \mathrm{d}y \biggr)\mathrm{d}x \quad \text{(Cauchy-Schwarz)}\\
			= & \Vert c \Vert_{\mathbb{L}^2(\mathcal{A} \times \mathcal{A})}\Vert \nu \Vert_{\mathbb{L}^2} < \infty
		\end{split}
	\end{equation}
	
	\subsection{Lemma (Three Traits of Scores)} \label{Proof1}
	The random function $X(t)$ realizing in $\mathbb{L}^2[0,1]$ is expanded by its Eigenfunctions $\{\nu^m\}$ as shown in Equation \ref{KarhunenLoeve}. The coefficients $\xi^{m}$ corresponding to Eigenfunctions $\nu^m$ satisfy the following properties:
	
	\begin{multicols}{2}
		\begin{enumerate}
			\item $\mathbb{E}\left[\xi^m(\omega)\right] = 0$
			\item $Cov\left(\xi^m(\omega), \xi^n(\omega)\right) = \delta^{m,n}\lambda^m$% if $m \neq n$
			\item $Var\left(\xi^m(\omega)\right) = \lambda^m$
		\end{enumerate}
	\end{multicols}

	where $\delta^{m,n} = 0$ if $m \neq n$, otherwise 1.
	
	\begin{proof}
		Assume that $F(t)$ is the centered process of $X(t)$, namely, $F(t) = X(t) - \int_{\Omega}X(t)\mathrm{d}P(\omega)$. To obtain the first result, we can show that
		
		\begin{equation}\label{Lemma1}
			\begin{split}
				\mathbb{E}[\xi^m] = & \mathbb{E} \biggl\lbrack \int_{0}^{1} F(t) \nu_{j}(t)\mathrm{d}t\biggr\rbrack\\
				= & \int_{\Omega} \int_{0}^{1} F(t) \nu^m(t) \mathrm{d}t \mathrm{d}P(\omega)\\
				= & \int_{0}^{1} \int_{\Omega} F(t) \nu^m(t) \mathrm{d}P(\omega) \mathrm{d}t \quad
				\text{(Fubini)}\\
				= & \int_{0}^{1} {\color{red}\int_{\Omega} F(t) \mathrm{d}P(\omega)} \nu^m(t) \mathrm{d}t\\
				= & \int_{0}^{1} {\color{red}\mathbb{E}[F(t)]} \nu^m(t) \mathrm{d}t = 0
			\end{split}
		\end{equation}
	
		where $\mathbb{E}[F(t)]$ is 0 since $F(t)$ is a centered process.
		
		The second claim is proved as
		\begin{equation}\label{Lemma2}
			\begin{split}
				\mathbb{E} [\xi^m \xi^n] = & \mathbb{E}  \biggl\lbrack \int_{0}^{1} F(s) \nu^m(s)\mathrm{d}s \int_{0}^{1} F(t) \nu^n(t)\mathrm{d}t  \biggr\rbrack\\
				= & \mathbb{E} \biggl\lbrack {\int_{0}^{1} \int_{0}^{1} F(s) \nu^m(s) F(t) \nu^n(t) \mathrm{d}s \mathrm{d}t} \biggr\rbrack \quad \text{(Fubini)}\\
				= & \int_{0}^{1} {\color{red}\int_{0}^{1} \mathbb{E}[{F(s)F(t)}] \nu^m(s)} \nu^n(t) {\color{red}\mathrm{d}s} \mathrm{d}t\\
				= & \int_{0}^{1} {\color{red}\left(\int_{0}^{1}c(s,t)\nu^m(s)\mathrm{d}s \right)} \nu^n(t) \mathrm{d}t \\
				= & \int_{0}^{1}{\color{red}[K\nu^m](t)}\nu^n(t)\mathrm{d}t\\
				= & \langle K \nu^m, \nu^n \rangle\\
				= & \langle \lambda^m \nu^m, \nu^n \rangle = \delta^{m,n}\lambda^{m}
			\end{split}
		\end{equation}
	
		where $\delta^{m,n} = 1$ if $m = n$, otherwise 0. The result is produced from orthonormality of the Eigenfunctions.
		
		\begin{equation}
			Cov\left(\xi^m, \xi^n\right) = \mathbb{E}[\xi^m \xi^n] - \mathbb{E}[\xi^m]\mathbb{E}[\xi^n] = \delta^{m,n}\lambda^{m}
		\end{equation}
	
		where $\mathbb{E}[\xi^m] = \mathbb{E}[\xi^n] = 0$ as shown in the first property.
		The last assertion is confirmed from the two properties above.
		\begin{equation}\label{Lemma3}
			Var[\xi^m] = \mathbb{E}\left[(\xi^m - \mathbb{E}[\xi^m])^{2}\right] = \mathbb{E}[(\xi^m)^{2}] =\lambda^m
		\end{equation}
	
		The original process $X(t)$ also has the same properties as the centered one since
		\begin{equation}
			X(t) = \mathbb{E}[X(t)] + F(t) = \mu(t) + \sum_{m=1}^{\infty}\xi^m\nu^m(t)
		\end{equation}
	
	\end{proof}
	
	
	\subsection{Theorem (Karhunen-Lo\'{e}ve Expansion)} \label{Proof2}
	
	Let $X : [0,1]  \rightarrow \mathbb{R}$ be a mean-square continuous stochastic process, meaning 
	\begin{equation}
		\lim\limits_{\epsilon \rightarrow 0} \mathbb{E}[(X(t+\epsilon) - X(t))^2] = 0
	\end{equation}
	such that $X \in \mathbb{L}^{2}[0,1]$. Then there exists a basis ${\nu^m}$ of $\mathbb{L}^2[0,1]$ such that for all $t \in [0,1]$ we have the following representation.
	\begin{equation}
		X(t) = \mu(t) + \sum_{m=1}^{\infty} \xi^m \nu^m(t),
	\end{equation}

	Here, $\mu(t)$ is the mean function of $X(t)$ and coefficients $\xi^m$ are given by $\int_{0}^{1} (X(t) - \mu(t)) \nu^m(t)\mathrm{d}t$. These coefficients satisfy the following conditions.
	
	\begin{multicols}{2}
		\begin{enumerate}
			\item $\mathbb{E}\left[\xi^m(\omega)\right] = 0$
			\item $Cov\left(\xi^m(\omega), \xi^n(\omega)\right) = \delta^{m,n}\lambda^m$ %if $m \neq n$
			\item $Var\left(\xi^m(\omega)\right) = \lambda^m$
		\end{enumerate}
	\end{multicols}

	\begin{proof}
		
		Let $K$ be a Hilbert-Schmidt operator as in Equation \ref{HSKernal}.We know that $K$ has a complete set of Eigenfunctions ${\nu^m}$ in $\mathbb{L}^{2}[0,1]$  and non-negative Eigenvalues $\lambda^m$ since $K$ is a positive compact self-adjoint operator. With the reminder that $\xi^m$ satisfy the three conclusions by Lemma \ref{Proof1}, we prove this expansion by considering
		
		\begin{equation}
			\epsilon_{N}(t) := \mathbb{E} \left[\bigg( X(t) -\mu(t)- \sum_{m=1}^{N} \xi^m \nu^m(t)\bigg)^2 \right]
			= \mathbb{E} \left[\bigg( F(t) - \sum_{m=1}^{N} \xi^m \nu^m(t)\bigg)^2 \right]
		\end{equation}
	
		where $F(t)$ is the centered process of $X(t)$.
		Once it is shown that $\lim\limits_{N \rightarrow \infty} \epsilon_{N}(t) = 0$ uniformly in [0,1], the proof is completed.
		
		\begin{equation}\label{Thr1}
			\begin{split}
				\epsilon_{N}(t) &= \mathbb{E} \left[\bigg( F(t) - \sum_{m=1}^{N} \xi^m 	\nu^m(t)\bigg)^2 \right]\\
				&= \mathbb{E}[F(t)^{2}] - 2\mathbb{E}\bigg[F(t)\sum_{m=1}^{N}\xi^m\nu^m(t)\bigg] + \mathbb{E}\bigg[\sum_{m=1}^{N}\sum_{n=1}^{N}\xi^m\xi^n\nu^m(t)\nu^n(t)\bigg]
			\end{split}
		\end{equation}
		
		Here, $\mathbb{E}[F(t)^{2}] = c(t,t)$ as in Equation \ref{CovarianceFunction} since $F(t)$ is the centered process. Now, take the second term
		
		\begin{equation}\label{Thr2}
			\begin{split}
				\mathbb{E} \bigg[ F(t) \sum_{m=1}^{N} {\color{red}\xi^m}\nu^m(t) \bigg] &= \mathbb{E} \left[ F(t) \sum_{m=1}^{N} {\color{red}\bigg(\int_{0}^{1} F(s)\nu^m(s)\mathrm{d}s\bigg)} \nu^m(t) \right]\\
				&= \mathbb{E} \left[ \sum_{m=1}^{N} \bigg(\int_{0}^{1} F(t)F(s)\nu^m(s)\mathrm{d}s\bigg) \nu^m(t) \right]\\
				&= \sum_{m=1}^{N} \bigg(\int_{0}^{1} {\color{blue}\mathbb{E}[F(t)F(s)]}\nu^m(s)\mathrm{d}s\bigg)\nu^m(t)\\
				&= \sum_{m=1}^{N} {\color{teal}\bigg(\int_{0}^{1} {\color{blue}c(t,s)} \nu^m(s)\mathrm{d}s\bigg)}\nu^m(t)\\
				&= \sum_{m=1}^{N}{\color{teal}[K\nu^m](t)}\nu^m(t) \\
				&= \sum_{m=1}^{N}{\color{teal}\lambda^m\nu^m(t)}\nu^m(t) = \sum_{m=1}^{N}\lambda^m\nu^m(t)^{2}
			\end{split}
		\end{equation} 
		
		where the covariance function $c(t,s)$ has the Hilbert-Schmidt operator as in Equation \ref{HSKernal}. For the last term, we derive from Equation \ref{Lemma2} that
		
		\begin{equation}\label{Thr3}
			\begin{split}
				\mathbb{E}\bigg[\sum_{m=1}^{N} \sum_{n=1}^{N} \xi^m \xi^n \nu^m(t) \nu^n(t)\bigg] = & \sum_{m=1}^{N} \sum_{n=1}^{N} \mathbb{E}[\xi^m \xi^n] \nu^m(t) \nu^n(t)\\
				= & \sum_{m=1}^{N} \sum_{n=1}^{N} \delta^{m,n} \lambda^m \nu^m(t) \nu^n(t) = 		\sum_{m=1}^{N} \lambda^m \nu^m(t)^{2}
			\end{split}	
		\end{equation}
	
		where $\delta_{m,n} = 1$ if $m=n$, otherwise 0. 
		
		Therefore, by Equations \ref{Thr1}, \ref{Thr2}, and \ref{Thr3} we obtain
		\begin{equation}
			\epsilon_{N}(t) = c(t,t) - \sum_{m=1}^{N} \lambda^m \nu^m(t) \nu^m(t)
		\end{equation}
		
		implementing Mercer's Theorem this proof is concluded by
		
		\begin{equation}
			\lim\limits_{N \rightarrow \infty} \epsilon_{N}(t) = \lim\limits_{n \rightarrow \infty} \mathbb{E} \left[\bigg( F(t) - \sum_{m=1}^{N} \xi^m \nu^m(t)\bigg)^2 \right] = 0
		\end{equation}
	
	\end{proof}

	\newpage
	
	\section{Bibliography}
	\printbibliography[heading=none]	
	
	\newpage
	\section*{Affidavit}
	
	\vspace{2cm}
	"I hereby confirm that the work presented has been performed and
	interpreted solely by myself except for where I explicitly identified the
	contrary. I assure that this work has not been presented in any other
	form for the fulfillment of any other degree or qualification. Ideas
	taken from other works in letter and in spirit are identified in every
	single case."
	
	\vspace{2cm}
	Bonn, 11.02.2021 \hrulefill \\
	\hspace*{0mm}Jonghun Baek
	
	\vspace{2cm}
	Bonn, 11.02.2021 \hrulefill \\
	\hspace*{0mm}Jakob R. Juergens
	
	\vspace{2cm}
	Bonn, 11.02.2021 \hrulefill \\
	\hspace*{0mm}Jonathan Willnow
	
	
\end{document}