\documentclass[11pt,twoside,a4paper]{article}
\usepackage[hmargin=2cm, vmargin=2cm]{geometry}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[english]{babel}
\usepackage{setspace}
\usepackage{xcolor}
\usepackage{multicol}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage[font=small,skip=2pt]{caption}
\usepackage[
backend=biber,
style=authoryear-comp,
]{biblatex}


\newcommand{\independent}{\perp\!\!\!\!\perp} 

\addbibresource{RMbibliography.bib}
\onehalfspacing
\parindent=0pt

\begin{document}
	\title{{\LARGE Model Selection for Scalar-on-Function Regression \\ with Applications to Near-Infrared Spectroscopy}}
	\author{Jonghun Baek, Jakob R. Juergens, Jonathan Willnow}
	\date{11.02.2022}
	\maketitle
	\vspace{1.5 cm}
	\begin{center}
		Research Module in Econometrics and Statistics \\
		Winter Semester 2021/2022
	\end{center}
	
	\newpage
	
	\tableofcontents
	
	\newpage
	
	\section{Colour Guide}
		\begin{itemize}
			\item {\color{red} RED}: is for general comments for your own text
			\item {\color{green} GREEN}: is for Jona's comments
			\item {\color{orange} ORANGE}: is for Jonghun's comments
			\item {\color{blue} BLUE}: is for Jakob's comments
		\end{itemize}
	
	\section{Introduction}
	
	\begin{itemize}
		\item Describe the idea of regressing a scalar on functional data
		\item Describing the difference to multiple linear regression intuitively
		\item Giving an intuitive example
	\end{itemize}	
	Functional Data Analysis (FDA) is a relatively new field {\color{red} (roots in the 1940s Grenander and Karhunen)} which is gaining more attention as researchers from different fields collect data that is functional in nature. Classical statistical methods can often process this data, but only FDA allows extracting information given by the smoothness of the underlying process (cf. \cite{levitin_introduction_2007}).
	 As \cite{kokoszka_introduction_2017} describe, FDA should be considered when one can view variables or units of a given data set as smooth curves or functions and the interest is in analyzing samples of curves (cf. \cite[S.~17]{kokoszka_introduction_2017}).\\
	 To motivate scalar-on-function regression, consider the case of a data set containing a scalar response and observations of an underlying continuous process. In economics, one application could be the regression of stock market correlations on the Global Crisis Index (GCI), where the regression allows to assess the relationship between the correlation and the GCI at every point within a window (cf. \cite{Das_2019}).\\
	 The focus of this paper is to introduce Functional Linear Regression (FLR) in a scalar-on-function setting. We will be using the standard FLR framework, which relates functional predictors to a scalar response as follows:
	 
	 \begin{equation}
	 	Y(\omega) = \alpha + \int_{0}^{1}{X(\omega)(s)\beta(s) \mathrm{d}s} + \epsilon(\omega),
	 	\qquad i = 1, ..., n
	 \end{equation}
 
	 where the $X_{i}$ are realizations of a random function $\mathbf{X}$, $Y_i$ are the corresponding realizations of the response variable and $\beta(s)$ is the coefficient function. The distinct feature of this framework is that the regressor is a function, which necessitates a different approach to estimation. As in the well-known framework of scalar linear regression, this is motivated by an interest in $\beta(s)$ for prediction. For instance, fluctuation in $X_i(s)$ at a point $s_0$ will not have any effect on $Y_i$ if $\beta(s_0)$ = 0. \\
	 Estimation of $\beta(s)$ is inherently an infinite-dimensional problem. In Section 2, after introducing the necessary theoretical concepts, we describe three methods of estimating a scalar coefficient function using a concept called truncated basis expansion. We report the results of the Monte-Carlo simulation regarding these three different methods in Section 3. Finally, in Section 4, we test the prediction of FLR in a real-world setting. {\color{red} (We may put some simple descriptions of results about each of MC and Application)}

	\section{Theory}
	In multivariate regression, data is often observed in the form of elements from Euclidean space, $\mathbb{R}^p$. However, the statistics derived from infinite-dimensional random functions cannot be defined on a finite dimensional space. To understand functional linear regression and the differences between the methods presented in this paper, it is therefore necessary to introduce some concepts and extend known aspects of linear regression theory to include functional objects. One integral concept in inferential statistics are random variables. Paraphrasing a definition by \cite{bauer_wahrscheinlichkeitstheorie_2020}, a random variable $X:\Omega \rightarrow \Omega'$ is an $\mathcal{A} \text{-} \mathcal{A'} \text{-measurable}$ function, where $(\Omega, \mathcal{A}, P)$ is a probability space and $(\Omega', \mathcal{A'})$ is a measure space.\\
	A typical case known to every undergraduate student of economics in less formal detail is $(\Omega', \mathcal{A'}) = (\mathbb{R}, \mathcal{B})$, where $\mathcal{B}$ is the canonical $\sigma$-algebra on the real numbers. As a first intuition, it is possible to imagine a similar concept where a random variable does not realize as an element of the real numbers but as a function in a function space. A formalization of this idea makes some more theoretical considerations necessary. The following theoretical introduction closely follows chapters 2.3 and 2.4 from \cite{hsing_theoretical_2015}. 
	
	\subsection{Inner Products and Hilbert Spaces}
	Let $\mathbb{V}$ be a vector space over some field of scalars $\mathbb{F}$. A function $\langle \cdot, \cdot \rangle : \mathbb{V} \times  \mathbb{V} \rightarrow \mathbb{F}$ is called an inner product, if $\forall v, v_1, v_2 \in \mathbb{V}$ and $a_1, a_2 \in \mathbb{F}$ the following properties hold.
	
	\begin{multicols}{2}
		\begin{enumerate}
			\item $\langle v, v \rangle \geq 0$
			\item $\langle v, v \rangle = 0$ if $v = 0$
			\item $\langle a_1 v_1 + a_2 v_2, v \rangle = a_1 \langle v_1, v \rangle + a_2 \langle v_2, v \rangle$
			\item $\langle v_1, v_2 \rangle = \langle v_2, v_1 \rangle$
		\end{enumerate}
	\end{multicols}

	A vector space with an associated inner product is called an inner product space. {\color{red}[verbatim quote!]}
	The inner product naturally defines a norm and an associated distance on the vector space as follows. In the following we restrict our analysis to the case of $\mathbb{F} = \mathbb{R}$.
	
	\begin{equation}
		\lvert \lvert v \rvert \rvert = {\langle v, v \rangle}^{\frac{1}{2}}
	\end{equation}

	\begin{equation}
		d(v_1, v_2) = {\langle v_2 - v_1, v_2 - v_1 \rangle}^{\frac{1}{2}}
	\end{equation}
	
	If the inner product space is complete with respect to the induced distance, it is called a Hilbert space, denoted $\mathbb{H}$ in the following. To extend the known concept of a basis in a finite dimensional space to the potentially infinite Hilbert spaces, it is necessary to define the closed span of a sequence of elements of $\mathbb{H}$. Recall that the span of a set of vectors $S \subseteq \mathbb{R}^P$ is given by
	
	\begin{equation}
		span(S) = \left\{\sum_{i = 1}^{k} \lambda_i v_i \: \bigg\vert \: k \in \mathbb{N}, \: v_i \in S, \: \lambda_i \in \mathbb{R} \right\}
	\end{equation}
			
	The closed span $\overline{span}(S)$ of a sequence $S$ in $\mathbb{H}$ is defined as the closure of the span with respect to the distance induced by the norm. $S$ is called a basis of $\mathbb{H}$ if $\overline{span}(S) = \mathbb{H}$. \\
	It is called an orthonormal basis, if in addition the following properties hold. 
	
	\begin{multicols}{2}
		\begin{enumerate}
			\item $\langle v_i, v_j \rangle = 0 \quad \forall v_i, v_j \in S \quad i \neq j$
			\item $\lvert \lvert v \rvert \rvert = 1 \quad \forall v \in S$
		\end{enumerate}
	\end{multicols}

	As in the case of a Banach space, each element of a Hilbert space can be expressed in terms of a corresponding basis. This can be done using a Fourier expansion of an element $x \in \mathbb{H}$ w.r.t. a basis $S = \{s_n\}$ as follows.
	
	\begin{equation}
		x = \sum_{j = 1}^{\infty}{\langle x, s_j \rangle}s_j
	\end{equation}
	
	As can be seen, differing from the case of a Banach space, these representations can be limits of series as previously hinted at by using the closed span of the basis. As using an infinite number of basis functions is infeasible in applied contexts, an intuitive way to approximate elements of a Hilbert space, is to use a truncated series.
	
	\begin{equation}
		x \approx \sum_{j = 1}^{K}{\langle x, s_j \rangle}s_j
	\end{equation}
	
	\subsection{Hilbert Space of Square-Integrable Functions}
	In functional data analysis, one Hilbert space of particular importance is the space of square-integrable functions on $[0,1]$ denoted $\mathbb{L}^2[0,1]$. To define it, look first at the measure space given by $([0,1], \mathcal{B}, \mu)$ where $\mathcal{B}$ is the Borel $\sigma$-algebra on $[0,1]$ and $\mu$ is the Lebesgue-measure.\\
	Then $\mathbb{L}^2[0,1]$ is the collection of all measurable functions $f$ on $[0,1]$ that fulfill the following condition.
	
	\begin{equation}
		{\color{orange} \lvert \lvert f \rvert \rvert^2 =} \int_{0}^{1} \lvert f \rvert^2 \mathrm{d}\mu < \infty
	\end{equation}
	
	All functions $f$ satisfying the above condition are called square-integrable functions. Moreover, it ensures that a random function has a finite second moment so that the variance and the covariance function can be defined.\\
	Its inner product is defined as
	
	\begin{equation}
		\langle f_1, f_2 \rangle = \int_{0}^{1} f_1 f_2 \mathrm{d}\mu.
	\end{equation}
	
	The Hilbert space of square integrable functions on $[0,1]$ {\color{orange} (instead, $\mathbb{L}^2[0,1]$)} is the function space that is most often used for theoretical considerations in functional data analysis  {\color{orange} without loss of generality}. 
	
	%Trivially, it is possible to extend this to other closed intervals on the reals line, but there are also more complex generalizations. For the purpose of this paper we will focus on the typical case and assume that random functions are random variables realizing in $\mathbb{L}^2$ for some closed interval of the real numbers. 
	
	The random function defined on $\mathbb{L}^2[0,1]$ can be represented as
	
	\begin{equation}
	\{f(t,\omega) : t \in [0, 1], \omega \in \Omega\}
	\end{equation}
	
	The realized $f(t)$ for every $t \in [0,1]$ is called a sample curve for the process. The collection of such sample curves constitutes a functional data set.
	
	\subsection{Bases of $\mathbb{L}^2$} % Maybe change title due to monomial and bspline basis not ful;filling this property
	As previously described, a basis of a Hilbert space can be used to express elements of the space using the corresponding Fourier expansion. Two examples of bases that are often used in practice to express / approximate elements of $\mathbb{L}^2[0,1]$  are explained in the following.
	
	Both of these can be used to express or in the case of the b-spline basis approximate elements of $\mathbb{L}^2[0,1]$ as a weighted sum of basis functions. Let therefore $\{\phi_i(t) \: \vert \: i \in \mathcal{I}\}$ be the basis used to express / approximate a realization $X(\omega_0) = x(t)$ of $X(\omega)$.
	
	\begin{equation}
		X(\omega_0) = x(t) = \sum_{j \in \mathcal{I}} A_{j}(\omega_0) \phi_j(t)
	\end{equation}
	
	\paragraph{Monomial Basis}
This is the basis that is needed to build polynomials, one of the best-known ways to expand a function.
The polynomial of the form 

	\begin{equation}
		x(t) = \sum_{i \in \mathcal{I}} c_i(t -w)^{i}
	\end{equation}
	can be expressed as a finite linear combination of the monomials $(t - w)^{i}$ with coefficient $c_i$ which form the basis 
	
	\begin{equation}
		\phi_{i}^{M}(t) = (t-w)^k \quad i \in \mathcal{I}
	\end{equation}
	The single basis function $1$ within the monomial basis system, for $k=0$, is called the constant basis system (cf. \cite{horvath_inference_2012}). {\color{blue} Taylor expansion as motivation}\\ 
	The shift parameter $w$ usually is specified to be the center of the interval subject to approximation.
	 Problems of collinearity arise for higher degrees since the monomial basis becomes more correlated as the degrees increase. This restricts the number of degrees and therefore the use of this basis: For simple functions this may be useful, but the small number of degrees makes it impossible to capture pronounced local peculiarities and leads to undesirable behaviour at the tails. (cf. \cite{ramsay_functional_2005})


	
	\paragraph{Fourier Basis}
	The Fourier basis for $\mathbb{L}^2[0,1]$ is given by the following sequence of functions defined on $[0,1]$.
	\begin{equation}
		\phi_{i}^{F}(x) = 
		\begin{cases}
			1 & \text{if} \quad i = 1\\
			\sqrt{2} \cos(\pi i x) & \text{if} \quad i \quad \text{is even} \\
			\sqrt{2} \sin(\pi (i-1)x) & \text{otherwise}
		\end{cases}
	\end{equation}
	
	Therefore, its basis functions inherits a repeating behaviour which is useful to expand functions that represent an periodic or seasonal underlying process over the period $T$. Rephrasing from \cite{ramsay_functional_2005}, the Fourier basis functions are orthogonal when the values $t_j$ are equally spaced within $T$. This basis is suitable to expand functions with a similar curvature order across the domain, resulting generally in uniformly smooth expansions.  {\color{blue} Motivate from fourier series}

	\paragraph{B-spline Basis} Following chapter 3.5 from \cite{ramsay_functional_2005}, splines are defined by first dividing the interval of interest $[\tau_0, \tau_L]$ into $L$ subintervals of non-negative length divided by a non-decreasing sequence of points $(\tau_l)_{l = 1,\dots, L-1}$ called knots. On each subinterval, a spline is a polynomial of chosen order $m = n+1$ where $n$ is its degree. Additionally, at each $\tau_l$ the the polynomials on neighbouring subintervals must match derivatives up to order $m-2$.
	A B-spline is a spline belonging to a basis system developed by \cite{de_boor_practical_1978}. Let $S_{l,m}^{BS}(x) \quad l = 1,\dots,L-1$ be the B-spline of order $m$ for an interval $[\tau_0, \tau_L]$ and knots $\{\tau_l \: \vert \: l = 1,\dots, L-1\}$, then it is defined by the Cox-de Boor recursion formula as follows. 

	\begin{equation}
		\begin{split}
			\phi_{l,0}^{BS}(x) = &
			\begin{cases}
				1 & \text{if} \quad x \in \left[\tau_l, \tau_{l+1}\right)\\
				0 & \text{otherwise}
			\end{cases}\\ \\
			\phi_{l,m}^{BS}(x) = &\frac{x - \tau_l}{\tau_{l+m} - \tau_l} \phi_{l,m-1}^{BS}(x) + \frac{\tau_{l+m+1} - x}{\tau_{l+m+1} - \tau_{l+1}} \phi_{l+1,m-1}^{BS}(x)
		\end{split}
	\end{equation}
	
	This, however, does not really yield a basis of $\mathbb{L}^2[0,1]$ as the closed span of this finite sequence of functions is not equal to $\mathbb{L}^2[0,1]$. To really obtain a basis of $\mathbb{L}^2[0,1]$ from B-splines, further theoretical considerations about, for example, infinite series of B-splines and specific knot choices would have to be made. As this is out of the scope of this paper, for the sake of simplicity, we will assume that a B-spline basis representation of a function in $\mathbb{L}^2[0,1]$ will serve as a sufficient approximation for an appropriately chosen B-spline basis. 
	Even though, this approach is not theoretically exact, in practice, this is often a reasonable approach and yields satisfactory results in cases where the functional form of B-splines makes them an appropriate approximation tool. 
	
	{\color{red}Explain what happens when $l+m+1 > L$ !!!\\
		Additionally, modify for multiple knots at the boundaries to get better behavior at the boundaries. Needs some more explanation.}	
	
	\subsection{Approximation and Smoothing via Basis Truncation}
	{\color{red} Write something about how to approximate functions by truncating the basis.}
	As above-mentioned, the realized curves can be estimated with basis functions. For the basis expansion, it is technically possible to use all basis functions $b_{j \in \mathcal{I}}$. On the one hand, this way is not efficient since the technique with too many basis functions can even approximate a noise of the sample curves which possibly interrupts the analysis. On the other hand, the important information on the curves would be missed with a too-small number of basis functions. This discussion challenges the researcher to seek a point at which they truncate the basis function in order to remove noise and, at the same time, maintain significant fluctuation. The basis expansion with truncation is by:
	\begin{equation}
		X(\omega_0) = x(t) = \sum_{j \in \mathcal{I}} A_j(\omega_0) \phi_j(t) = \sum_{j = 1}^{L} A_j(\omega_0) \phi_j(t) + \delta(t) \approx \sum_{j = 1}^{L} A_j(\omega_0) \phi_j(t)
	\end{equation}
	where $\delta(t)$ is the truncation error. The number $L$ is subjectively chosen but we can fix the nubmer through a more reasonable method, Cross-Validation, which minimizes Mean Squared Error (MSE). 
	
	\subsection{Functional Data Sets}
	In functional data analysis the concept of a data set can include not only realizations of scalar random variables, but also realizations of random functions. In the following, all random functions are assumed to realize in $\mathbb{L}^2$. As in the finite dimensional setting the concept of identically distributed and independent data is important. This concept generalizes intuitively to the case of functional data using the concepts for general random variables. {\color{red} Explain this better!}\\
	An example of one such data set could be a set of $n$ independent realizations of a Wiener-process and $n$ associated scalar variables that could be subject of a regression analysis with respect to the corresponding realizations of the Wiener process. A definition and a plotted example for the Wiener process are given in \ref{Wiener}.
	
	\subsection{Karhunen-Lo\'{e}ve Expansion and Empirical Eigenbases}\hypertarget{KL}{}
	Given a realization of a random function realizing in $\mathbb{L}^2[0,1]$, it is possible to represent this realization in terms of its generating stochastic process. To motivate it, recall the representation of a random vector derived from the spectral decomposition of its covariance matrix. Let therefore $V(\omega)$ be a random vector realizing in $\mathbb{R}^{p}$ with $\mathbb{E}(V(\omega)) = \mu_V$ and $Cov(V(\omega)) = \Sigma_V$. Then $\Sigma_V$ can be expressed in terms of its Eigenvalues and orthonormal Eigenvectors as $\Sigma_V = \Gamma \Lambda \Gamma'$ using its Eigendecomposition.\\
		
	This decomposition can be used to express the random vector $V(\omega)$ in the following way.
	
	\begin{equation}
		V(\omega) = \mu_V + \Gamma \Lambda^{\frac{1}{2}} D(\omega) = \mu_V + \sum_{i = 1}^{p} \sqrt{\lambda_i} D_i(\omega) \gamma_i
	\end{equation}
	
	where $D(\omega)$ is a random vector with $\mathbb{E}(D(\omega)) = 0_p$ and $Cov(D(\omega)) = \mathbb{I}_p$.\\
	
	To obtain the analogous concept for a random function, it is necessary to define the covariance function of a random function realizing in $\mathbb{L}^2[0,1]$. Therefore, let $X: \Omega \mapsto \mathbb{L}^2[0,1]$ be such a random function.
	Then the mean and covariance functions of $X$ are defined as follows.
	
	\begin{equation}\label{MeanFunction}
		\mu(t) = \mathbb{E}\left[ X(\omega)(t) \right]
	\end{equation}
	
	\begin{equation}\label{CovarianceFunction}
		c(t,s) = \mathbb{E}\big[ \left( X(\omega)(t) - \mu(t) \right) \left( X(\omega)(s) - \mu(s) \right) \big]
	\end{equation}
	{\color{orange}suggestion from here}
	where the $c(t,s)$ are Hilbert-Shmidt Kernels defined through c : $[0,1] \times [0,1] \rightarrow \mathbb{R}$. Let $K$ be the integral operater on $\mathbb{L}^{2}[0,1]$ such that $K : \nu \rightarrow K \nu$ for $\nu \in \mathbb{L}^{2}[0,1]$, by
	\begin{equation}
		[K \nu](t) = \int_{0}^{1}c(t,s) \nu(s)ds
	\end{equation}

	Then, the operator $K$ has orthonormal basis functions $\nu_{i} \in \mathbb{L}^{2}[0,1]$ corresponding to eigenvalues $\lambda_{i}$ for all $i$ since it is a positive compact self-adjoint operator. Moreover, $K$ holds following traits:
	
		\begin{enumerate}
			\item The eigenspaces corresponding to distinct eigenvalues are mutually orthogonal.
			\item The eigenspaces corresponding to non-zero eigenvalues are finite-dimentsional.
			\item The eigenvalues can be ordered in nonincreasing order as follows $\lambda_{1} \geq \lambda_{2} \geq \dots \geq 0$.
		\end{enumerate}
	
	Therefore, we can approximate the function $X$ with first few eigenfunctions. The functions $X$ are approximated enough well by first few principal components since the order of them are sorted in descending order (e.g. $Var(\xi_{j}) \geq Var(\xi_{k})$ for all $j > k$). Theoretical considerations lead to the result that $X$ can be represented in the following form, called its Karhunen-Lo\'{e}ve expansion. {\color{orange} The proofs are provided at \ref{Proof1} and \ref{Proof2}.}
	
	\begin{equation}\label{KarhunenLoeve}
		X(\omega)(t) = \mu(t) + \sum_{j = 1}^{\infty} \xi_j(\omega) \nu_j(t)
	\end{equation}

	where the $\nu_j$ are defined by the countable set of solutions $\{(\lambda_i, \nu_i) \: \vert \: i \in \mathbb{N}\}$ of the following equation.
	
	\begin{equation}
		{\color{orange} [K \nu](t) =}\int_{0}^{1}c(t,s)\nu(s) \mathrm{d}s = \lambda \nu(t)
	\end{equation}
	
	The $\xi_j(\omega)$ are then given as 
	\begin{equation}
		\xi_j(\omega) = \langle X(\omega) - \mu, \nu_j\rangle = \int_{0}^{1} \left(X(\omega)(s) - \mu(s)\right) \nu_j(s) \mathrm{d}s
	\end{equation} 
	and thereby random variables realizing in $\mathbb{R}$ and have the following properties akin to $\Lambda^{\frac{1}{2}} D(\omega)$ for the case of a random vector. {\color{orange} We would need to assume that the process is centered, otherwise we may not derive below results(not sure though).}
	
	\begin{multicols}{2}
		\begin{enumerate}
			\item $\mathbb{E}\left[\xi_j(\omega)\right] = 0$
			\item $Var\left(\xi_j(\omega)\right) = \lambda_i$
			\item $Cov\left(\xi_j(\omega), \xi_k(\omega)\right) = 0$ if $j \neq k$
		\end{enumerate}
	\end{multicols}
	
	In the typical scalar setting, a similar consideration leads to the concept of principal components. This is also possible in a functional setting. Let $\{x_1(t), \dots, x_n(t)\}$ be a set of i.i.d. realizations generated by a random function $X(\omega) \mapsto \mathbb{L}^2[0,1]$.
	Define the following sample analogues for the mean and covariance functions.
	
	\begin{equation}
		\hat{\mu}(t) = \frac{1}{n}\sum_{j = 1}^{n}x_j(t)
	\end{equation}

	\begin{equation}
		\hat{c}(t,s) = \frac{1}{n} \sum_{j = 1}^{n} \left(x_j(t) - \hat{\mu}(t)\right) \left(x_j(s) - \hat{\mu}(s)\right)
	\end{equation}

	With these it is possible to derive a set of sample analogs $\{(\hat{\lambda}_i, \hat{\nu}_i) \: \vert \: i \in \mathbb{N}\}$ for $\{(\lambda_i, \nu_i) \: \vert \: i \in \mathbb{N}\}$ as the solutions of the following equation. {\color{red} I think the number of principal components has to be smaller then the number of observations. So I can give more information about $\mathbb{N}$.}
	
	\begin{equation}
		\int_{0}^{1}\hat{c}(t,s)\hat{\nu}(s) \mathrm{d}s = \hat{\lambda} \hat{\nu}(t)
	\end{equation}
	
	This naturally leads to the following representation.
	
	\begin{equation}
		x_i(t) = \hat{\mu}(t) + \sum_{j = 1}^{{\color{red} !!!}} \hat{\xi}_{i,j} \hat{\nu}(t)
	\end{equation}
	
	\subsection{Scalar-on-Function Regression}
	In the simple scalar setting one of the most important tools in econometrics is the linear regression. Its goal is to predict the value of a dependent variable given a set of associated variables. For reference assume a data generating process as follows. {\color{red} Structure is bad... This will be changed.}
	
	\begin{equation}
		Y = X\beta + \epsilon
	\end{equation}
	
	Where $Y$ is the vector of response variables, $X$ is the matrix containing the corresponding regressors in its columns and $\beta = (\beta_0, \beta_1, \: \dots, \beta_p)'$ is the vector containing the unknown coefficients.
	In this finite dimensional setting one important question is how to estimate the unknown coefficients $\beta$. The most well known estimator in all of econometrics, the Ordinary Least Squares (OLS) estimator, fulfills this purpose under a set of assumptions. {\color{red}List Assumptions? Then we need to list the assumptions for functional linear regression as well I think.}
	
	\begin{equation}
		\hat{\beta}_{OLS} = (X'X)^{-1}X'Y
	\end{equation}
	
	The concept of linear regression can be extended to a setting of functional data, where a scalar response variable is supposed to be predicted from a functional variable. 
	A general data generating process in this functional scenario could look like the following equation.
	
	\begin{equation}
		Y(\omega) = \alpha + \Psi\left(X(\omega)\right) + \epsilon(\omega)
	\end{equation}
	
	Here $\Psi$ is a functional that maps a realization of a random function in $\mathbb{L}^2[0,1]$ into $\mathbb{R}$. One simple example to illustrate the principle is the maximum $\Psi(f) = \max_{x \in [0,1]}f(x)$.
	However, the typical setup is often as follows mimicking the structure of the multivariate linear model extended from summation to integration. This structure is crucial for the extension of linear regression to the case of functional regressors. Therefore, in this paper we always implicitly assume that a data generating process has the following structure. {\color{red} maybe a better motivation is the one Jona gave: from very dense observations that run into problems of colinearity when using the original OLS estimator}
	
	\begin{equation}\label{DGP}
		Y(\omega) = \alpha + \int_{0}^{1} \beta(s)X(\omega)(s) \mathrm{d}s + \epsilon(\omega)
	\end{equation}
	
	Where $x(t)$ is the realization of a random function in $\mathbb{L}^2[0,1]$ and $\beta(t)$ is an unknown coefficient function. 
	Similar to the finite dimensional setting, an interesting question is how to estimate the unknown function $\beta(t)$ given a data set containing realizations of a random function and associated scalar response variables. However, a simple extension of the OLS estimator to allow for infinite dimensional objects is not possible. Therefore, other options have to be considered.
	
	\subsubsection{Estimation using Basis-Representation}
	The most common way to make this problem tractable is via a basis representation of $\beta(t)$. Therefore, let $\{b_i(t) \: \vert \: i \in \mathbb{N}\}$ be a basis of $\mathbb{L}^2[0,1]$ and represent $\beta(t)$ in terms of this basis.
	
	\begin{equation}
		\beta(t) = \sum_{j = 1}^{\infty} b_j \phi_j(t)
	\end{equation}
	
	This enables us to write equation \ref{DGP} with $\beta(t)$ represented in this way to obtain a formulation as a sum of scalar random variables $Z_j(\omega)$.
	
	\begin{equation}
		\begin{split}
			Y(\omega) & = \alpha + \int_{0}^{1} {\color{blue}\beta(s)} X(\omega)(s)\mathrm{d}s + \epsilon(\omega)
			= \alpha + \int_{0}^{1}\left[{\color{blue}\left(\sum_{j \in \mathcal{I}} b_j \phi_j(s)\right)} X(\omega)(s) \right]\mathrm{d}s + \epsilon(\omega) \\
			& = \alpha + \sum_{j \in \mathcal{I}} \left[b_j \textcolor{red}{\int_{0}^{1} X(\omega)(s) \phi_j(s)\mathrm{d}s}\right] + \epsilon(\omega)
		      = \alpha + \sum_{j \in \mathcal{I}} b_j \textcolor{red}{Z_j(\omega)} + \epsilon(\omega)
		\end{split}
	\end{equation}
	
	This representation translates the original problem of regressing a scalar on a continuously observed function to a problem where a scalar is regressed on a countably infinite sequence of regressors. Using a truncation of the basis at some parameter $L$ can be used to make this problem tractable with typical theory from multivariate regression while staying reasonably accurate.
	
	\begin{equation}
			Y(\omega) \approx \alpha + \sum_{j = 1}^{L} b_j Z_j(\omega) + \epsilon(\omega)
	\end{equation}

	{\color{red} This fancy equation will be very useful to explain our simulation.}
	
	\begin{equation}\label{basis_exp_transf}
		\begin{split}
			Y(\omega) & = \alpha + \int_{0}^{1} {\color{blue}\beta(s)} {\color{red}X(\omega)(s)}\mathrm{d}s + \epsilon(\omega)
			 = \alpha + \int_{0}^{1}\left[{\color{blue}\left(\sum_{j \in \mathcal{I}} b_j  \phi_j(s)\right)} {\color{red}\left(\sum_{k \in \mathcal{L}} d_k(\omega)  \psi_k(s)\right)} \right]\mathrm{d}s + \epsilon(\omega) \\
			& = \alpha + \sum_{j \in \mathcal{I}} b_j \left[\sum_{k \in \mathcal{L}} \textcolor{teal}{d_k(\omega) \int_{0}^{1} \phi_j(s) \psi_k(s) \mathrm{d}s}\right] + \epsilon(\omega) 
			= \alpha + \sum_{j \in \mathcal{I}} \left[ b_j \sum_{k \in \mathcal{L}} \textcolor{teal}{Z_{j, k}(\omega)} \right] + \epsilon(\omega) \\
			& \approx \alpha + \sum_{j = 1}^{J} \left[b_j {\color{violet}\sum_{k = 1}^{K}  Z_{j, k}(\omega)} \right] + \epsilon(\omega) 
			= \alpha + \sum_{j = 1}^{J} b_j {\color{violet}\tilde{Z}_j(\omega)} + \epsilon(\omega) 
		\end{split}
	\end{equation}

	A typical choice in this scenario is to use the same functional basis and the same truncation parameter $L$ for both the coefficient function and the approximation of the observations. This leads to the following simplification of Equation \ref{basis_exp_transf}.

	\begin{equation}
		\begin{split}
			Z_{j, k}(\omega) & = d_k(\omega) \int_{0}^{1} \phi_j(s) \phi_k(s) \mathrm{d}s \quad j = 1, \dots, L \quad k = 1, \dots, L \\
			\tilde{Z}_j(\omega) & = \sum_{k = 1}^{L} \left[d_k(\omega) \int_{0}^{1} \phi_j(s) \phi_k(s) \mathrm{d}s \right] \quad j = 1, \dots, L
		\end{split}
	\end{equation}
	
	\subsubsection{Estimation using Functional Principal Components}
	
	% maybe use n for coefficents in expansion of nu? before b for beta, so this could be a consistent choice
	\begin{equation}\label{FPCA_basis_expansion}
		\begin{split}
			\hat{\xi}_{i}^m & = \int_{0}^{1} {\color{red}\left( f_i(s) - \hat{\mu}(s)\right)} {\color{blue}\hat{\nu}^m(s)} \mathrm{d}s
			= \int_{0}^{1} {\color{red}\left(\sum_{j \in \mathcal{I}} a_{i,j} \phi_j(s)\right)} {\color{blue}\left(\sum_{k \in \mathcal{L}} b_{k}^m \psi_{k}(s)\right)} \mathrm{d}s \\
			& = \sum_{j \in \mathcal{I}} \left[ a_{i,j}\sum_{k \in \mathcal{L}} {\color{teal}  b_{k}^m \int_{0}^{1} \phi_j(s) \psi_{k}(s)\mathrm{d}s}\right]
			= \sum_{j \in \mathcal{I}} \left[a_{i,j}\sum_{k \in \mathcal{L}} {\color{teal} Z_{j,k}^m} \right]
			\approx \sum_{j = 1}^{J} \left[a_{i,j}{\color{violet}\sum_{k = 1}^{K} Z_{j,k}^m} \right]
			= \sum_{j = 1}^{J} a_{i,j}{\color{violet}\tilde{Z}_{j}^m}
		\end{split}
	\end{equation}

	Again using the same basis and the same truncation parameter $L$ leads to the following simplifications of Equation \ref{FPCA_basis_expansion}.
	
	\begin{equation}
		\begin{split}
			Z_{j, k}^m & =  b_{k}^m \int_{0}^{1} \phi_j(s) \phi_k(s) \mathrm{d}s \quad j = 1, \dots, L \quad k = 1, \dots, L \\
			\tilde{Z}_{j}^m& = \sum_{k = 1}^{L} \left[ b_k^m \int_{0}^{1} \phi_j(s) \phi_k(s) \mathrm{d}s \right] \quad j = 1, \dots, L\\
		\end{split}
	\end{equation}

	%\subsection{Detailed Draft}
	%\begin{itemize}
	%	\item Motivate random functions from introduction and the general concept of random variables
	%	\item Formalize random function in this context as random variables realizing in a Hilbert space
	%	\item Introduce $\mathbb{L}^2[0,1]$ as the Hilbert space of square integrable functions on $[0,1]$
	%	\item Specialize to Hilbert space being $\mathbb{L}^2[0,1]$ for this context
	%	\item Define mean and covariance function of a random function realizing in $\mathbb{L}^2[0,1]$
	%	\item Introduce the concept of a basis of a Hilbert space and specialize to $\mathbb{L}^2[0,1]$
	%	\item Introduce b-spline and Fourier bases
	%	\item Introduce eigenfunctions and FPCA on the basis of covariance function (Karhunen-Lo\'{e}ve expansion)
	%	\item explain similarities to Eigenvalues and Eigenvectors of matrix + PCA (fraction of explained variance etc...)
	%	\item Introduce functional observations in this context as realizations of a random variable realizing in $\mathbb{L}^2[0,1]$
	%	\item Explain the concept of iid data in a functional setting		
	%	\item Define point-wise mean (sample), point-wise standard deviation (sample) and sample covariance function
	%	\item Explain approximations of functional observations using truncated basis representations
	%	\item Introduce linear operator $L_1$ and sufficient condition associated with it
	%	\item Motivate Scalar-on-function regression from multivariate linear regression with a scalar response variable
	%\end{itemize}

%There are several important aspects of functional regression in this functional setting that separate it from usual multiple regression according to \cite{kokoszka_introduction_2017}: In functional linear regression, the aim is not only to obtain an estimate of the function $\beta(s)$ — this estimate also needs to have a useful interpretation. Without it, there might be prediction, but the increase in understanding of the underlying question will be minimal. One aspect of a useful interpretation is that the estimate $\beta(s)$ should not jump in a seemingly random fashion, because an interpretation of this erratic behavior will often be impossible.

	%A common setting in non-functional regression is akin to the following. Assume a model as follows:
	
	%\begin{equation}
	%	Y = X'\beta + \epsilon
	%\end{equation}

	%where $X \in \mathbb{R}^{n\times J}$ is a matrix containing the regressors, $\beta \in \mathbb{R}^J$ is a coefficient vector and $epsilon$ is a vector containing the error term. For simplicity, assume that the data generating process fulfills the Markov assumptions. Then the famous OLS-estimator is given by:
	
	%\begin{equation}
	%	\hat{\beta}_{OLS} = (X'X)^{-1}X'Y
	%\end{equation}

	%A naive approach to FLR would be to try to generalize this to the functional setting.
	%Assuming a data generating process of the form:
	
	%\begin{equation}
   % 	 Y =  \int \beta(s)X(s) \,\mathrm{d}t \ +\epsilon
    %\end{equation}

    %it becomes clear that we cannot compute the estimate of $\beta(t)$ as we would do in a classical multivariate setup because of the infinite dimensionality of the underlying objects. Where in the finite dimensional setting the OLS estimator can be derived as a method of moments estimator by solving a system of equations of sample moment restrictions, this leads to a system of infinitely many equations in the functional setting. 
    %In practice, functional observations are never truly continuously observed. If we assume that the functional observations are observed at a finite set of points $\{t_1, \dots, t_J\}$ this makes the derivation of an OLS estimator possible as before.
    
    %\begin{equation}\label{discrete_time_model}
    %	Y_i = \sum_{j = 1}^{n} \beta(t_{j})X_i(t_{j}) + \epsilon_{i},
    %\end{equation}

    %However, this often still results in a large and difficult to solve system of equations. Even if solved, the result is often a noisy function $\hat{\beta}(s)$ that is not useful for interpretation since it does not use the intuition of smooth functions. Another reason why estimation is not feasible using this approach is colinearity.
    %Looking at equation \ref{discrete_time_model} and assuming continuous functions $X_i$ it becomes clear that if $t_{j}$ is close to $t_{j'}$, $X_{i}(t_{j})$ is close to $X_{i}(t_{j'})$. Thereby, there will be vectors $X_{i} = (X_i(t_1), \dots, X_i(t_J))'$ that are highly correlated and thus lead to large variances of $\beta$. (cf. ~\cite{kokoszka_introduction_2017})\\
	
	\subsection{Draft-Overview}
	\begin{itemize}
		\item Motivate Karhunen-Loeve-Expansion and Eigenbasis from PCA		
		\item Explain Scalar-on-Function Regression
		\item Estimation through basis-expansion (incl. Eigenbasis) [and estimation with roughness penalty]
		\item Address approximation error due to basis-truncation
	\end{itemize}

	\subsection{Literature}
	\begin{itemize}
		\item \cite{alexanderian_KLexpansion_2015}
		\item \cite{kokoszka_introduction_2017}
		\item \cite{hsing_theoretical_2015}
		\item \cite{ramsay_functional_2005}
		\item \cite{horvath_inference_2012}
		\item \cite{cai_prediction_2006}
		\item \cite{levitin_introduction_2007}
	\end{itemize}
	
	\newpage
	\section{Simulation Study}
	
	%\subsection{Draft-Overview}
	%\begin{itemize}
	%	\item Motivate Simulation for some data generating process from application
	%	\item Describe Simulation Setting from technical standpoint (DGP, set-up for replication, ...)
	%			\item Prediction not Inference (Alternative: Focused on a testing procedure motivated by the application)
	%	\item Present Results
	%	\item Explain relevance for application
	%\end{itemize}

	\subsection{Motivation}
	For the simulation study, we deviate from the standard simulation setting. Instead of generating data by ourselves, we use the gasoline data which consists of 60 samples of Near Infrared (NIR) spectra measured by 2-nm from 900 to 1,700 nm, and a response variable,  the octane rating. NIR- spectroscopy allows to analyse samples of gasoline much faster, cheaper and with the same reproducibility as standard tests while not destroying the sample (cf. \cite{Bohacs_Ovadi_Salgo1998}). We chose this setup to improve the approach towards the application in which we  predict the octane ratings from the gasoline dataset.
	
	\subsection{Generating Similar Curves}
	To avoid small sample problems, we generated 200 similar curves from the gasoline dataset, motivated by \hyperlink{KL}{Karhunen-Lo\'{e}ve Expansion}. First, the initial curves are expressed in terms of a generated bspline basis which is created using 50 knots. These smooth curves are then centered, before applying the \hyperlink{KL}{Karhunen-Lo\'{e}ve Expansion}. It is assumed that the scores follow a normal distribution, thus, the new realizations for the scores are drawn from a multivariate normal $\tilde{\xi} = \left(\tilde{\xi}_{1},\: \dots \:, \tilde{\xi}_{J}\right)' \sim \mathcal{N}(0_J, \; diag(\hat{\lambda}_1,\: \dots\:, \hat{\lambda}_J))$. Finally, we obtain the generated curves $NIR_{sim}$
	
	
		$$\tilde{X}(\omega)(t) = \hat{\mu}(t) + \sum_{j = 1}^{J} \tilde{\xi}_j(\omega) \hat{\nu}_j(t)$$ 

		where
			$\tilde{X}(\omega)(t)$, $\hat{\mu}(t)$ and $\hat{\nu}_j(t)$ are approximated as vectors in $\mathbb{R}^{401}$.
		
	
    %{\color{blue} I shifted this from theory to over here, as it makes more sense in the simulation part.\\
    %Very short summary: take fpc's and eigenvalues, use that the scores are independent from each other. Use the eigenvalues as variances and draw from a multivariate normal with the right varcov matrix (normal just because of convenience... No theoretical reasoning)}
    
    \subsection{Simulation setup}
	The simulation study follows \cite{Reiss_2007b} as a guideline. Two different true coefficient functions,  $f_1(t)$ and  $f_2(t)$, are created that differ in their smoothness, to compare the introduced methods with differing true coefficient functions:
	
	\begin{equation}
    	f_1(t) = 2\sin(0.5\pi t) + 4\sin(1.5 \pi t) + 5\sin(2.5 \pi t) 
    \end{equation}

    \begin{equation}
    	\begin{split}
    		f_2(t) = & 1.5 \exp{\left(\frac{-0.5(t-0.3)^2}{0.02^2}\right)} - 4 \exp{\left(\frac{-0.5(t-0.45)^2}{0.015^2}\right)} \\
    				 & + 8 \exp{\left(\frac{-0.5(t-0.6)^2}{0.02^2}\right)} -  \exp{\left(\frac{-0.5(t-0.8)^2}{0.03^2}\right)}
    	\end{split}
    \end{equation}
    \vspace{0.2cm}\\
    
    The bumpy function, $f_2(t)$, was generated by referring to \cite{cardot_bumpyfunction_2002}. 
    Both functions were created such that the inner product $\langle NIR_{sim}, f \rangle, f \in \{f_1(t), f_2(t)\}$ creates responses similar to the original octane ratings of the gasoline dataset. 

		\vspace{0.1cm}
		\begin{figure}
			\centering
			\begin{minipage}{.5\textwidth}
				\centering
  				\includegraphics[width=\textwidth]{../Graphics/smooth_function.png}
  				\caption{$f_1(t)$, smooth function}
  				\label{fig:test1}
			\end{minipage}%
			\begin{minipage}{.5\textwidth}
	  			\centering
  				\includegraphics[width=\textwidth]{../Graphics/bumpy_function.png}
  				\caption{$f_2(t)$, bumpy function}
  				\label{fig:test2}
			\end{minipage}
		\end{figure}
		
		 Two different error-terms $\epsilon$ were created by first generating an $i.i.d.$ standard normal error term and then multiplying it by two error variations $\sigma_e $. The error variations represent different signal-to-noise ratios to test the methods both with a low and a high amount of noise. They are created such that the squared multiple correlation coefficient $R^2 = var(Xf) / (var(Xf) + \sigma^2_{e})$ is equal to 0.9 and 0.6. The two error-terms are then used to generate two sets of responses for $f \in \{f_1(t), f_2(t)\}$
		
		\begin{equation}
			\begin{split}
				Y_{1,f} & = \langle NIR, f\rangle + Z  \biggl\lbrack\frac{var(\langle NIR, f\rangle)}{0.9} - var(\langle NIR, f\rangle)\biggr\rbrack \\
				Y_{2,f} & = \langle NIR, f\rangle + Z  \biggl\lbrack\frac{var(\langle NIR, f\rangle)}{0.6} - var(\langle NIR, f\rangle)\biggr\rbrack
			\end{split}
		\end{equation}
		
		where $Z \sim \mathcal{N}(0,1)$. In total, we created four combinations for the simulations, using the two true coefficient functions and the two sets of responses. These four combinations are then used with a different number of monomial basis functions $ \in \{1,2, \dots, 5\}$, cubic bspline basis-function $\{5,6,...,25\}$ and fourier functions $\{1,3,...,25\}$ to predict the generated responses using the basis expansion approach and the FPCR approach. For the evaluation, we used the prediction RMSE calculated by 10 fold cross-validation.
		 To obtain valid out of sample properties for the FPCR, within each of the ten 10 fold cross-validation splits, we first calculate the Functional Principal Components of the trainig-set $\mathcal{T}$ for each curve. These scores are then used to estimate the scores of the holdout set $\mathcal{H}$, $\hat{\xi}_{i}^{m, \mathcal{H}}$  by the equation:
		 
	\begin{equation}
		\begin{split}
			\hat{\xi}_{i}^{m, \mathcal{H}} &=  \int_{0}^{1} {\color{red} \left(X_{i}^{\mathcal{H}}(s) - \hat{\mu}^{\mathcal{T}}(s)\right)} {\color{blue}\hat{\nu}^{m, \mathcal{T}} }\mathrm{d}s) 
						    = \int_{0}^{1} {\color{red}\left(\sum_{j \in \mathcal{I}} a_{i,j}^{\mathcal{H}} \phi_j(s)\right)} {\color{blue}\left(\sum_{k \in \mathcal{L}} b_{k}^{m, \mathcal{T}} \psi_{k}(s)\right)} \mathrm{d}s \\
			 &= \sum_{j \in \mathcal{I}} \left[ a_{i,j}^{\mathcal{H}}\sum_{k \in \mathcal{L}}  b_{k}^{m, \mathcal{T}} \int_{0}^{1} \phi_j(s) \psi_{k}(s)\mathrm{d}s\right]
		\end{split}
	\end{equation}
	
	
	
		 The simulation was done with R (version...). In total, 5000 repetitions were done for each set of simulations. 
		
	
	\subsection{Results}
	{\color{green}DESCRIBE RESULTS: make paragraph for each setup? so 6 paragraphs? Like this: }
	
	
	\subsubsection{Basis Expansion Regression}
	This follows from the \hyperref[basis_exp_transf]{Basis Expansion Transformation}, in which we transform the observed functions to perform regression of a scalar on a countable sequence of regressors, tractable with typical multivariate regression theory. 
	\paragraph{Monomial Basis}
	\paragraph{Bspline Basis}
	\paragraph{Fourier Basis}
	
	\subsubsection{Functional Principal Component Regression}
	\paragraph{Monomial Basis}
	\paragraph{Bspline Basis}
	\paragraph{Fourier Basis}
	
	\subsubsection{Interpretation and Relevance for Application}

	\subsection{Literature}
	\begin{itemize}
		\item \cite{shonkwiler_explorations_2009}
		\item R-packages: fda, refund, mgcv
	\end{itemize}
	
	\newpage
	\section{Application}
		The application uses the insights from the previous sections to predict the octane ratings of the introduced gasoline dataset. Following the results from the simulation study,... 
		\begin{itemize}
			\item {\color{green} incorporate results of simulation study: number of basis, components, etc...}
			\item point out difficulties estimating sderror
			\item describe setup and results
\end{itemize}
	
	\subsection{Literature}
	\begin{itemize}
		\item \cite{carey_life_2002}
	\end{itemize}

	\section{Outlook}
	
	\subsection{Literature}
	\begin{itemize}
		\item \cite{James.2009} (shape-restrictions)
	\end{itemize}
	
	\newpage
	\section{Appendix}
	
	\subsection{Basis Plots}
	
	\begin{figure}[H]\label{fourier_basis}
		\includegraphics[width = \textwidth]{../Graphics/Fourier_Basis.pdf}
		\caption{Fourier basis functions for $i = 1,\dots,6$}
	\end{figure}
	
	\begin{figure}[H]\label{bspline_basis}
		\includegraphics[width = \textwidth]{../Graphics/Bspline_Basis.pdf}
		\caption{B-spline basis functions of order 3 for 6 equidistant knots on $[0,1]$}
	\end{figure}

\subsection{Wiener Process}\label{Wiener}
A Wiener process $W_t$ is a real-valued continuous-time stochastic process {\color{red} ... This is wikipedia, look for right citation!}
It is characterized by the following properties.

\begin{multicols}{2}
	\begin{enumerate}
		\item $W_0 = 0$
		\item $\forall t > 0 W_{t+u} - W_t \perp\!\!\!\perp W_s \forall s \leq t$
		\item $W_{t+u} - W_t \sim \mathcal{N}(0,u)$
		\item $W_t$ is continuous in $t$
	\end{enumerate}
\end{multicols}

\begin{figure}[H]\label{Wiener_plot}
	\includegraphics[width = \textwidth]{../Graphics/Wiener_plot.pdf}
	\caption{25 i.i.d. realizations of a Wiener Process on $[0,1]$}
\end{figure}

	\section{Proofs}
	\subsection{Lemma} \label{Proof1}
	{\color{orange} We might put the below theorems for KL. The deterministic basis expansions are pretty clear about why we can use the truncation version by itself. In contrast, I feel like that for the case of KL it does not. And in the centered case?} 
	The coefficients $\xi_{j}$ satisfy the following:
	\begin{enumerate}
		\item $\mathbb{E}[\xi_{j}] = 0$
		\item $\mathbb{E}[\xi_{j} \xi_{k}] = \lambda_{j}$, if $j = k$, otherwise 0
		\item Var[$\xi_{j}$] = $\lambda_{j}$
	\end{enumerate}
	
	\begin{proof}
		To obtain the first result, we can show that
		\begin{equation}
			\begin{split}
				\mathbb{E}[\xi_{j}] = & \mathbb{E} \biggl\lbrack \int_{0}^{1} F(t) \nu_{j}(t)dt \biggr\rbrack\\
				= & \int_{\Omega} \int_{0}^{1} F(t) \nu_{j}(t) dt dP(\omega)\\
				= & \int_{0}^{1} \int_{\Omega} F(t) \nu_{j}(t) dP(\omega) dt \quad \text{(Fubini)}\\
				= & \int_{0}^{1} \mathbb{E}[F(t)] \nu_{j}(t) dt\\
				= & \mu(t) \int_{0}^{1} \nu_{j}(t)\\
				= & \mu(t) \langle \nu_{j}(t), 1 \rangle = 0
			\end{split}
		\end{equation}
		where the constant function is always an eigenfunction of the operater $K$. It leads the inner product to 0 due to orthonormality of eigenfunctions. {\color{red} Another way to prove or use centered mean}
		For the case of centered process
		\begin{equation}\label{Lemma1}
			\begin{split}
				\mathbb{E}[\xi_{j}] = & \mathbb{E} \biggl\lbrack \int_{0}^{1} F(t) \nu_{j}(t)dt\biggr\rbrack\\
				= & \int_{\Omega} \int_{0}^{1} F(t) \nu_{j}(t) dt dP(\omega)\\
				= & \int_{0}^{1} \int_{\Omega} F(t) \nu_{j}(t) dP(\omega) dt \quad \text{(Fubini)}\\
				= & \int_{0}^{1} \mathbb{E}[F(t)] \nu_{j}(t) dt = 0
			\end{split}
		\end{equation}
		where $\mathbb{E}[F(t)]$ is 0 since $F(t)$ is a centered process.
		The second claim is proved as:
		\begin{equation}\label{Lemma2}
			\begin{split}
				\mathbb{E} [\xi_{j} \xi_{k}] = & \mathbb{E}  \biggl\lbrack \int_{0}^{1} F(s) \nu_{j}(s)ds \int_{0}^{1} F(t) \nu_{k}(t)dt  \biggr\rbrack\\
				= & \mathbb{E} \biggl\lbrack {\int_{0}^{1} \int_{0}^{1} F(s) \nu_{j}(s) F(t) \nu_{k}(t) ds dt} \biggr\rbrack \quad \text{(Fubini)}\\
				= & \int_{0}^{1} \int_{0}^{1} \mathbb{E}[{F(s)F(t)}] \nu_{j}(s) \nu_{k}(t) ds dt\\
				= & \int_{0}^{1} \left(\int_{0}^{1}c(s,t)\nu_{j}(s)ds \right) \nu_{k}(t) dt \\
				= & \int_{0}^{1}[K\nu_{j}](t)\nu_{k}(t)dt\\
				= & \langle K \nu_{j}, \nu_{k} \rangle\\
				= & \langle \lambda_{j} \nu_{j}, \nu_{k} \rangle = \lambda_{j} \quad \text{if $j = k$, otherwise 0}
			\end{split}
		\end{equation}
		where the result is produced from orthonormaility of the eigenfunctions. The last assertion is confirmed from the other two properties.
		\begin{equation}\label{Lemma3}
			Var[\xi_{i}] = \mathbb{E}\left[(\xi_{i} - \mathbb{E}[\xi_{i}])^{2}\right] = \mathbb{E}[\xi_{i}^{2}] = \lambda_{i}
		\end{equation}
	\end{proof}
	
	
	\subsection{Karhunen-Lo\'{e}ve expansion} \label{Proof2}
	Let $F : [0,1]  \rightarrow \mathbb{R}$ be a centered mean-square continuous stochastic process with $F \in \mathbb{L}^{2}[0,1]$. Then there exists a basis ${\xi_{i}}$ of $\mathbb{L}^2[0,1]$ such that for all $t \in [0,1]$,
	\begin{equation}
		F(t) = \sum_{j=1}^{\infty} \xi_{j} \nu_{j}(t),
	\end{equation}
	where coefficients $\xi_{j}$ are given by $\xi_{j}(\omega) = \int_{0}^{1} F(t)(\omega) \nu_{j}(t)dt$ and satisfy the following conditions.
	\begin{enumerate}
		\item $\mathbb{E}[\xi_{j}] = 0$
		\item $\mathbb{E}[\xi_{j} \xi_{k}] = \lambda_{j}$, if $j = k$, otherwise 0
		\item Var[$\xi_{j}$] = $\lambda_{j}$
	\end{enumerate}
	
	\begin{proof}
		We know that $K$ has a complete set of eigenvectors ${\nu_{j}}$ in $\mathbb{L}^{2}[0,1]$ and non-negative eigenvalues $\lambda_{j}$. With the fact that $\xi_{j}(\omega) = \int_{0}^{1} F(\omega)(t) \nu_{j}(t)dt$ satisfy the three conclusions by Lemma. We prove it by considering
		\begin{equation}
			\epsilon_{n}(t) := \mathbb{E} \left[\bigg( F(t) - \sum_{j=1}^{n} \xi_{j} \nu_{j}(t)\bigg)^2 \right]
		\end{equation}
		
		Once it is shown that $\lim\limits_{n \rightarrow \infty} \epsilon_{n}(t) = 0$ uniformly in [0,1], the proof is completed.
		\begin{equation}\label{Thr1}
			\begin{split}
				\epsilon_{n}(t) = &\mathbb{E} \left[\bigg( F(t) - \sum_{j=1}^{n} \xi_{j} 	\nu_{j}(t)\bigg)^2 \right]\\
				= & \mathbb{E}[F(t)^{2}] - 2\mathbb{E}\bigg[F(t)\sum_{j=1}^{n}\xi_{j}\nu_{j}(t)\bigg] + \mathbb{E}\bigg[\sum_{j=1}^{n}\sum_{k=1}^{n}\xi_{j}\xi_{k}\nu_{j}(t)\nu_{k}(t)\bigg]
			\end{split}
		\end{equation}
		
		Now, $\mathbb{E}[F(t)^{2}] = c(t,t)$ as in (\ref{CovarianceFunction}),
		\begin{equation}\label{Thr2}
			\begin{split}
				\mathbb{E} \bigg[ F(t) \sum_{j=1}^{n} \xi_{j}\nu_{j}(t) \bigg] = & \mathbb{E} \left[ F(t) \sum_{j=1}^{n} \bigg(\int_{0}^{1} F_{s}\nu_{i}(s)ds\bigg) \xi_{j}(t) \right]\\
				= & \sum_{j=1}^{n} \bigg(\int_{0}^{1} \mathbb{E}[F(t)F(s)]\nu_{j}(s)ds\bigg)\nu_{j}\\
				= & \sum_{j=1}^{n} \bigg(\int_{0}^{1} c(t,s) \nu_{j}(s)ds\bigg)\nu_{j}\\
				= & \sum_{j=1}^{n}[K\nu_{j}](t)\nu_{j} = \sum_{j=1}^{n}\lambda_{j}\nu_{j}^{2}
			\end{split}
		\end{equation} 
		
		We derive from (\ref{Lemma2}) that
		\begin{equation}\label{Thr3}
			\begin{split}
				\mathbb{E}\bigg[\sum_{j=1}^{n} \sum_{k=1}^{n} \xi_{j} \xi_{k} \nu_{j}(t) \nu_{k}(t)\bigg] = & \sum_{j=1}^{n} \sum_{k=1}^{n} \mathbb{E}[\xi_{j} \xi_{k}] \nu_{j}(t) \nu_{k}(t)\\
				= & \sum_{j=1}^{n} \sum_{k=1}^{n} \delta_{jk} \lambda_{j} \nu_{j}(t) \nu_{k}(t) = 		\sum_{j=1}^{n} \lambda_{j} \nu_{j}(t)^{2}
			\end{split}	
		\end{equation}
		where $\delta_{jk} = 1$ if $j=k$, otherwise 0. Therefore, by (\ref{Thr1}), (\ref{Thr2}), and (\ref{Thr3}) we obtain
		\begin{equation}
			\epsilon_{n}(t) = c(t,t) - \sum_{j=1}^{n} \lambda_{j} \nu_{j}(t) \nu_{j}(t)
		\end{equation}
		
		implementing Mercer's Theorem this proof is concluded by
		\begin{equation}
			\lim\limits_{n \rightarrow \infty} \epsilon_{n}(t) = \lim\limits_{n \rightarrow \infty} \mathbb{E} \left[\bigg( F(t) - \sum_{j=1}^{n} \xi_{j} \nu_{j}(t)\bigg)^2 \right] = 0
		\end{equation}
	\end{proof}

	\newpage
	
	\section{Bibliography}
	\printbibliography[heading=none]	
	
	\newpage
	\section{Affidavit}
	
	\vspace{2cm}
	"I hereby confirm that the work presented has been performed and
	interpreted solely by myself except for where I explicitly identified the
	contrary. I assure that this work has not been presented in any other
	form for the fulfillment of any other degree or qualification. Ideas
	taken from other works in letter and in spirit are identified in every
	single case."
	
	\vspace{2cm}
	Bonn, 11.02.2021 \hrulefill \\
	\hspace*{0mm}Jonghun Baek
	
	\vspace{2cm}
	Bonn, 11.02.2021 \hrulefill \\
	\hspace*{0mm}Jakob R. Juergens
	
	\vspace{2cm}
	Bonn, 11.02.2021 \hrulefill \\
	\hspace*{0mm}Jonathan Willnow
	
	
\end{document}