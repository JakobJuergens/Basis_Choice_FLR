% Inbuilt themes in beamer
\documentclass{beamer}

% Theme choice:
\usetheme{Berlin}
\usecolortheme{seahorse}

% packages
%\usepackage[hmargin=2cm, vmargin=2cm]{geometry}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[english]{babel}
\usepackage{setspace}
%\usepackage{xcolor}
\usepackage{multicol}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[font=small,skip=2pt]{caption}
\usepackage[
backend=biber,
style=authoryear-comp,
]{biblatex}

\addbibresource{../RMbibliography.bib}
\parindent=0pt

% Title page details: 
\title{Scalar on Function Regression}
\author{Jonathan Willnow, Jakob Juergens, Jonghun Baek}
\date{{\color{red}}Presentation Day}

\begin{document}
	
	% Title page frame
	\begin{frame}
		\titlepage 
	\end{frame}
	
	% Remove logo from the next slides
	\logo{}
	
	% I'm not really a fan of a table of contents in short presentations (Jakob)
	% Outline frame
	%\begin{frame}{Outline}
	%	\tableofcontents
	%\end{frame}
	
	\begin{frame}{Introduction}
		Jona \\
		Introductory Example $\rightarrow$ Octane/NIR-spectrum
	\end{frame}
	
	\begin{frame}{Theory}
		Jona \\
		Motivation from multivariate regression (multivariate dgp).
	\end{frame}

	\begin{frame}{Theory}
		Jonghun
		\begin{itemize}
			\item Random Functions (name square integrable functions)
			\item Motivate continuous stochastic processes (growth curves/electricity consumption/yield curves/stonks)
			\item Use curves to predict a scalar response (show typical dgp)
		\end{itemize}
	\end{frame}

	\begin{frame}{Theory}
		Jonghun
		\begin{itemize}
			\item Basis expansions (b-splines and fourier)
			\item Talk about purposes
			\item Plots and show bias variance tradeoff
		\end{itemize}
	\end{frame}

	\begin{frame}{Theory}
		Jakob
		\begin{itemize}
			\item Random function represented as linear combination of basis functions
			\item Just transform to multiple linear regression setting
			\item You already know that from the beginning
		\end{itemize}
	\end{frame}

	\begin{frame}{Theory - FPCA}
		Jakob
		\begin{itemize}
			\item Let's assume you know the theory of PCA (pc from varcov matrix)
			\item Introduce mean and covariance functions of random functions
			\item There is another cool basis $\rightarrow$ Eigenbasis (Karhunen-Loeve Expansion)
			\item Sample Analog! (create a basis from observations and use for basis regression)
			\item Plot fpcs and approximation of function realization
		\end{itemize}
	\end{frame}

	\begin{frame}{Spectral Representation of Random Vectors}
		Let $X(\omega)$ be a random vector realizing in $\mathbb{R}^p$.

		\begin{itemize}
			\item Let $\mu_x = \mathbb{E}(X)$ and $\Sigma_X = Cov(X)$
			\item Let $\{\gamma_i \: \vert \: i = 1, \dots, p\}$ be the orthonormal \textbf{Eigenvectors} of $\Sigma_X$
			\item Let $\{\lambda_i \: \vert \: i = 1, \dots, p\}$ be the corresponding \textbf{Eigenvalues} of $\Sigma_X$
		\end{itemize}
	
		\vspace{0.2cm}
		Then $X$ can also be represented as
		$$X(\omega) = \mu_x + \sum_{i = 1}^{p} \xi_i(\omega) \gamma_i$$
		where the $\xi_i(\omega)$ have the following properties
		
		\begin{multicols}{2}
			\begin{enumerate}
				\item $\mathbb{E}[\xi_i(\omega)] = 0$
				\item $var(\xi_i(\omega)) = \lambda_i$
				\item $Cov(\xi_i(\omega), \xi_j(\omega)) = 0$ for $i \neq j$
			\end{enumerate}
		\end{multicols}
	\end{frame}

	\begin{frame}{Principal Component Analysis}
		$\Sigma_X$ unknown $\rightarrow$ \textbf{sample analogues}
		
		\begin{itemize}
			\item Let $\mathbf{X} \in \mathbb{R}^{n \times p}$ be the matrix containing the standardized regressors in the usual configuration.
			\item Let $\hat{\Sigma}_X = \frac{\mathbf{X}'\mathbf{X}}{n}$
			\item Let $\{\hat{\gamma}_i \: \vert \: i = 1, \dots, p\}$ be the orthonormal \textbf{Eigenvectors} of $\hat{\Sigma}_X$
			\item Let $\{\hat{\lambda}_i \: \vert \: i = 1, \dots, p\}$ be the corresponding \textbf{Eigenvalues} of $\hat{\Sigma}_X$
		\end{itemize}
		
		
	\end{frame}

	\begin{frame}{Karhunen-Lo\'{e}ve Expansion}
		
		\textbf{Mean Function}: $$\mu(t) = \mathbb{E}\left[ F(\omega)(t) \right]$$

		\textbf{Autocovariance Function}: $$c(t,s) = \mathbb{E}\big[ \left( F(\omega)(t) - \mu(t) \right) \left( F(\omega)(s) - \mu(s) \right) \big]$$
		
		The \textbf{Eigenvalues} and \textbf{Eigenfunctions}: $\{(\lambda_i, \nu_i) \: \vert \: i \in \mathcal{I}\}$  are solutions of the following equation:
		$$ \int_{0}^{1}c(t,s)\nu(s) \mathrm{d}s = \lambda \nu(t) $$
	\end{frame}
	
	\begin{frame}{Karhunen-Lo\'{e}ve Expansion}
		A random function $F$ can be expressed in terms of its mean function and its Eigenfunctions:
		$$F(\omega)(t) = \mu(t) + \sum_{j = 1}^{\infty} \xi_j(\omega) \nu_j(t)$$
		
		Where the $\xi_j$ are scalar-valued random variables with the following properties.
		\begin{multicols}{2}
			\begin{enumerate}
				\item $\mathbb{E}[\xi_i(\omega)] = 0$
				\item $var(\xi_i(\omega)) = \lambda_i$
				\item $Cov(\xi_i(\omega), \xi_j(\omega)) = 0$ for $i \neq j$
			\end{enumerate}
		\end{multicols}
		
		This representation is called the \textbf{Karhunen-Lo\'{e}ve Expansion} of the random function $F$ and the Eigenfunctions can serve as a basis to represent the function.
	\end{frame}
	
	\begin{frame}{Simulation Setup \& Application}
		Jona
		\begin{itemize}
			\item Compare b-spline / fourier regression chosen via criterion (cv/aic/...)
			\item Similar for fpca
			\item generate new curves from observed curves motivated by Karhunen-Loeve expansion
			\item Compare optimal variants with test and training sets
			\item Connect to Application
		\end{itemize}
	\end{frame}

	\begin{frame}{Summary}
		Jona \\
		Just summarize what we have done...
	\end{frame}

	\begin{frame}{further reading}
		Put footnotes here!
	\end{frame}
	
\end{document}