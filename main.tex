\documentclass[11pt,twoside,a4paper]{article}
\usepackage[hmargin=2cm, vmargin=2cm]{geometry}
\usepackage{microtype}
\usepackage{hyperref}
\usepackage[english]{babel}
\usepackage{setspace}
\usepackage{xcolor}
\usepackage[
backend=biber,
style=authoryear-comp,
]{biblatex}


\addbibresource{RMbibliography.bib}
\onehalfspacing
\parindent=0pt

\begin{document}
	\title{{\LARGE {\color{red}PLACEHOLDER-TITLE:} Functional Linear Regression in a Scalar-on-Function Setting with Applications to SOMETHING}}
	\author{Jonghun Baek, Jakob Juergens, Jonathan Willnow}
	\date{{\color{red}whenever}}
	\maketitle
	\vspace{1.5 cm}
	\begin{center}
		Research Module in Econometrics and Statistics \\
		Winter Semester 2021/2022
	\end{center}
	
	\newpage
	
	\tableofcontents
	
	\newpage
	
	\section{Introduction}
	
	\begin{itemize}
		\item Describe the idea of regressing a scalar on functional data
		\item Describing the difference to multiple linear regression intuitively
		\item Giving an intuitive example
	\end{itemize}	
	Function Data Analysis (FDA) is a relatively new field {\color{red} (roots in the 1940s Grenander and Karhunen)} which is getting more attention as researchers from different fields collect more data from a continuous underlying process. This data still can be processed by classical statistical methods, but only FDA allows answering questions that are tied to the information given by the smoothness of the underlying continuous process. (cf. \cite{levitin_introduction_2007})\\
	 As \cite{kokoszka_introduction_2017} describe, FDA should be considered when one can view one or more of the variables or units of a given data set as a smooth curve or function and the interest is in analyzing samples of curves (cf. \cite[S.~17]{kokoszka_introduction_2017}).
	 To motivate scalar-on-function regression, consider the case of a data set containing a scalar response and observations of a continuous underlying process. In economics, one application could be the regression of stock market correlations on the Global Crisis Index (GCI), where the regression allows to assess the relationship between the correlation and the GCI at every point within a window (cf. \cite{Das_2019}). 

	\section{Theory}
	
	\subsection{Detailed Draft}
	\begin{itemize}
		\item Motivate random functions from introduction and the general concept of random variables
		\item Formalize random function in this context as random variables realizing in a Hilbert space
		\item Introduce $\mathbf{L}^2[0,1]$ as the Hilbert space of square integrable functions on $[0,1]$
		\item Specialize to Hilbert space being $\mathbf{L}^2[0,1]$ for this context
		\item Define mean and covariance function of a random function realizing in $\mathbf{L}^2[0,1]$
		\item Introduce the concept of a basis of a Hilbert space and specialize to $\mathbf{L}^2[0,1]$
		\item Introduce b-spline and Fourier bases
		\item Introduce eigenfunctions and FPCA on the basis of covariance function (Karhunen-Lo\'{e}ve expansion)
		\item explain similarities to Eigenvalues and Eigenvectors of matrix + PCA (fraction of explained variance etc...)
		\item Introduce functional observations in this context as realizations of a random variable realizing in $\mathbf{L}^2[0,1]$
		\item Explain the concept of iid data in a functional setting
		\item Define point-wise mean (sample), point-wise standard deviation (sample) and sample covariance function
		\item Explain approximations of functional observations using truncated basis representations
		\item Introduce linear operator $L_1$ and sufficient condition associated with it
		\item Motivate Scalar-on-function regression from multivariate linear regression with a scalar response variable
	\end{itemize}
\subsection{Kokoszka Reimherr (2017) p51-53}
There are several important aspects of functional regression in this functional setting that separate it from usual multiple regression according to Kokoszka and Reimherr (2017): In functional regression, the aim is not only to compute an estimate of the function $\beta$ because this function needs also to have an useful interpretation. Without this useful interpretation, there can be no effective and feasible prediction of the scalar responses from new explanatory functions. Hereby applies, that intervals with larger values of $|\beta(s)|$ are contributing more to to the response than small values of  $|\beta(s)|$. The sign of $\beta(s)$ within the intervals of the value s show either negative or positive association of  $|\beta(s)|$ for this interval. To get an useful interpretation, the estimate $\beta$ cannot jump in a seemingly random fashion, because then an useful interpretation is not possible and predictions from this model tend to have large variances and center around the mean of the responses. (see ~\cite{kokoszka_introduction_2017})
	
	\begin{itemize}
		\item Explain problem of naively extending multivariate linear regression to infinite dimensions
	\end{itemize}
	Considering the population model of scalar-on-function linear regression
	\begin{equation}
     Y =  \int \beta(s)X(s) \,dt \ +\epsilon
    \end{equation}
    it becomes clear that we cannot compute the estimate of $\beta(t)$ as we would do in a classical multivariate setup because there are infinitely many solutions for finding the minimizing argument for $\hat{\beta}$. Define 
    \begin{equation}
  	c_{X}(t,s) = E[X(t)X(s)],\: c_{XY}(t) = E[X(t)Y], 
    \end{equation}
   Under the assumption that $X$ is independent from $\epsilon$ we obtain
   \begin{equation}
     c_{XY}(t) = E[X(t)\int \beta(t)X(s) \,dt \ +\epsilon
   \end{equation}
    \begin{equation}
     c_{XY}(t) = \int \beta(s)\,dt \: E[X(s)X(t)| X] + E[\epsilon |X]
  	\end{equation}
   	\begin{equation}
    	c_{XY}(t) = \int c_{X}(t,s) \beta(s) \,ds 
   \end{equation}
   
  This results in practice in a large number of equations, which are difficult to solve. Even if solved, this results in a noisy function $\beta(\dot{â€¢})$ that is not useful for interpretation since it does bot utilize the intuition of smooth functions. Another reason why estimation is not feasible using this approach is colinearity. Approximate the scalar-on-functional regression as
  	\begin{equation}
     Y_i = \sum_{i = 1}^{n} \beta(t_{i})X_1(t_{i}) + \epsilon_{i}
    \end{equation}, it becomes obvious that if $t_{i}$ is close to $t_{i'}$, $X_{i}(t_{i})$ is close to $X_{i}(t_{i'})$, so there will be vectors $X_{i}$ thatare stronlgx correlated and thus lead to large variances and not feasible estimation.
   
    
	
	\begin{itemize}
		\item Solution: estimation using truncated basis expansion to approximate data (theoretical description)
		\item Problem: truncation error $\delta$ and how to deal with it?
		\item Explain how to address truncation error in standard errors
		\item Motivate three estimation procedures
		\begin{enumerate}
			\item truncated b-spline basis expansion without addressing truncation error
			\item truncated b-spline basis expansion WITH addressing truncation error
			\item truncated Eigenbasis expansion (advantages: low number of basis functions get low approximation error)
		\end{enumerate}
	\end{itemize}
	
	\subsection{Draft-Overview}
	\begin{itemize}
		\item Motivate Karhunen-Loeve-Expansion and Eigenbasis from PCA		
		\item Explain Scalar-on-Function Regression
		\item Estimation through basis-expansion (incl. Eigenbasis) [and estimation with roughness penalty]
		\item Address approximation error due to basis-truncation
	\end{itemize}

	\subsection{Literature}
	\begin{itemize}
		\item \cite{kokoszka_introduction_2017}
		\item \cite{hsing_theoretical_2015}
		\item \cite{ramsay_functional_2005}
		\item \cite{horvath_inference_2012}
		\item \cite{cai_prediction_2006}
		\item \cite{levitin_introduction_2007}
	\end{itemize}
	
	\newpage
	\section{Simulation}
	
	\subsection{Draft-Overview}
	\begin{itemize}
		\item Motivate Simulation for some data generating process from application
		\item Describe Simulation Setting from technical standpoint (DGP, set-up for replication, ...)
		
		\item Compare estimation with \begin{enumerate}
			\item b-spline basis without addressing approximation error
			\item ... including proper treatment of approximation error
			\item Eigenbasis constructed from observations
			\end{enumerate}
	
		\item Prediction not Inference (Alternative: Focused on a testing procedure motivated by the application)
		\item Present Results
		\item Explain relevance for application
	\end{itemize}

	\subsection{Literature}
	\begin{itemize}
		\item \cite{shonkwiler_explorations_2009}
		\item R-packages: fda, refund, mgcv
	\end{itemize}
	
	\newpage
	\section{Application}

	\subsection{Draft-Overview}
	\begin{itemize}
		\item Prediction not Inference (Alternative: Focused on a testing procedure motivated by the data set)
		\item IID data set (no dependence between the curves, don't want to do functional time series)
		\item Not necessarily data from economics (like biology, sports, whatever)
		\item Smooth curves or random walk (both fine)
		\item \href{https://functionaldata.wordpress.ncsu.edu/resources/}{https://functionaldata.wordpress.ncsu.edu/resources/}
	\end{itemize}
	
	\subsection{Literature}
	\begin{itemize}
		\item \cite{carey_life_2002}
	\end{itemize}

	\section{Outlook}
	
	\subsection{Literature}
	\begin{itemize}
		\item \cite{James.2009} (shape-restrictions)
	\end{itemize}
	
	\section{Appendix}
	
	\newpage
	
	\section{Bibliography}
	\printbibliography[heading=none]	
	
\end{document}