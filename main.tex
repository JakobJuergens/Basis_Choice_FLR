\documentclass[11pt,twoside,a4paper]{article}
\usepackage[hmargin=2cm, vmargin=2cm]{geometry}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[english]{babel}
\usepackage{setspace}
\usepackage{xcolor}
\usepackage{multicol}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[font=small,skip=2pt]{caption}
\usepackage[
backend=biber,
style=authoryear-comp,
]{biblatex}


\addbibresource{RMbibliography.bib}
\onehalfspacing
\parindent=0pt

\begin{document}
	\title{{\LARGE {\color{red}PLACEHOLDER-TITLE:} Functional Linear Regression in a Scalar-on-Function Setting with Applications to SOMETHING}}
	\author{Jonghun Baek, Jakob Juergens, Jonathan Willnow}
	\date{{\color{red}whenever}}
	\maketitle
	\vspace{1.5 cm}
	\begin{center}
		Research Module in Econometrics and Statistics \\
		Winter Semester 2021/2022
	\end{center}
	
	\newpage
	
	\tableofcontents
	
	\newpage
	
	\section{Colour Guide}
		\begin{itemize}
			\item {\color{red} RED}: is for general comments for your own text
			\item {\color{green} GREEN}: is for Jona's comments
			\item {\color{orange} ORANGE}: is for Jonghun's comments
			\item {\color{blue} BLUE}: is for Jakob's comments
		\end{itemize}
	
	\section{Introduction}
	
	\begin{itemize}
		\item Describe the idea of regressing a scalar on functional data
		\item Describing the difference to multiple linear regression intuitively
		\item Giving an intuitive example
	\end{itemize}	
	Functional Data Analysis (FDA) is a relatively new field {\color{red} (roots in the 1940s Grenander and Karhunen)} which is gaining more attention as researchers from different fields collect data that is functional in nature. Classical statistical methods can often process this data, but only FDA allows extracting information given by the smoothness of the underlying process (cf. \cite{levitin_introduction_2007}).
	 As \cite{kokoszka_introduction_2017} describe, FDA should be considered when one can view variables or units of a given data set as smooth curves or functions and the interest is in analyzing samples of curves (cf. \cite[S.~17]{kokoszka_introduction_2017}).\\
	 To motivate scalar-on-function regression, consider the case of a data set containing a scalar response and observations of an underlying continuous process. In economics, one application could be the regression of stock market correlations on the Global Crisis Index (GCI), where the regression allows to assess the relationship between the correlation and the GCI at every point within a window (cf. \cite{Das_2019}).\\
	 The focus of this paper is to introduce Functional Linear Regression (FLR) in a scalar-on-function setting. We will be using the standard FLR framework, which relates functional predictors to a scalar response as follows:  {\color{red} (I don't set up any interval for s here we might do later...)}
	 
	 \begin{equation}
	 	Y_{i} = \beta_{0} + \int{X_{i}(s)\beta(s)ds} + \epsilon_{i},
	 	\qquad i = 1, ..., n
	 \end{equation}
 
	 where the $X_{i}$ are realizations of a random function $\mathbf{X}$, $Y_i$ are the corresponding realizations of the response variable and $\beta(s)$ is the coefficient function. The distinct feature of this framework is that the regressor is a function, which necessitates a different approach to estimation. As in the well-known framework of scalar linear regression, this is motivated by an interest in $\beta(s)$ for prediction. For instance, fluctuation in $X_i(s)$ at a point $s_0$ will not have any effect on $Y_i$ if $\beta(s_0)$ = 0. \\
	 Estimation of $\beta(s)$ is inherently an infinite-dimensional problem. In Section 2, after introducing the necessary theoretical concepts, we describe three methods of estimating a scalar coefficient function using a concept called truncated basis expansion. We report the results of the Monte-Carlo simulation regarding these three different methods in Section 3. Finally, in Section 4, we test the prediction of FLR in a real-world setting. {\color{red} (We may put some simple descriptions of results about each of MC and Application)}

	\section{Theory}
	In multivariate regression, data is often observed in the form of elements from Euclidean space, $\mathbb{R}^p$. However, the statistics derived from infinite-dimensional random functions cannot be defined on a finite dimensional space. To understand functional linear regression and the differences between the methods presented in this paper, it is therefore necessary to introduce some concepts and extend known aspects of linear regression theory to include functional objects. One integral concept in inferential statistics are random variables. Paraphrasing a definition by \cite{bauer_wahrscheinlichkeitstheorie_2020}, a random variable $X:\Omega \rightarrow \Omega'$ is an $\mathcal{A} \text{-} \mathcal{A'} \text{-measurable}$ function, where $(\Omega, \mathcal{A}, P)$ is a probability space and $(\Omega', \mathcal{A'})$ is a measure space.\\
	A typical case known to every undergraduate student of economics in less formal detail is $(\Omega', \mathcal{A'}) = (\mathbb{R}, \mathcal{B})$, where $\mathcal{B}$ is the canonical $\sigma$-algebra on the real numbers. As a first intuition, it is possible to imagine a similar concept where a random variable does not realize as an element of the real numbers but as a function in a function space. A formalization of this idea makes some more theoretical considerations necessary. The following theoretical introduction closely follows chapters 2.3 and 2.4 from \cite{hsing_theoretical_2015}. 
	
	\subsection{Inner Products and Hilbert Spaces}
	Let $\mathbb{V}$ be a vector space over some field of scalars $\mathbb{F}$. A function $\langle \cdot, \cdot \rangle : \mathbb{V} \times  \mathbb{V} \rightarrow \mathbb{F}$ is called an inner product, if $\forall v, v_1, v_2 \in \mathbb{V}$ and $a_1, a_2 \in \mathbb{F}$ the following properties hold.
	
	\begin{multicols}{2}
		\begin{enumerate}
			\item $\langle v, v \rangle \geq 0$
			\item $\langle v, v \rangle = 0$ if $v = 0$
			\item $\langle a_1 v_1 + a_2 v_2, v \rangle = a_1 \langle v_1, v \rangle + a_2 \langle v_2, v \rangle$
			\item $\langle v_1, v_2 \rangle = \langle v_2, v_1 \rangle$
		\end{enumerate}
	\end{multicols}

	A vector space with an associated inner product is called an inner product space. {\color{red}[verbatim quote!]}
	The inner product naturally defines a norm and an associated distance on the vector space as follows. In the following we restrict our analysis to the case of $\mathbb{F} = \mathbb{R}$.
	
	\begin{equation}
		\lvert \lvert v \rvert \rvert = {\langle v, v \rangle}^{\frac{1}{2}}
	\end{equation}

	\begin{equation}
		d(v_1, v_2) = {\langle v_2 - v_1, v_2 - v_1 \rangle}^{\frac{1}{2}}
	\end{equation}
	
	If the inner product space is complete with respect to the induced distance, it is called a Hilbert space, denoted $\mathbb{H}$ in the following. To extend the known concept of a basis in a finite dimensional space to the potentially infinite Hilbert spaces, it is necessary to define the closed span of a sequence of elements of $\mathbb{H}$. Recall that the span of a set of vectors $S \subseteq \mathbb{R}^P$ is given by
	
	\begin{equation}
		span(S) = \left\{\sum_{i = 1}^{k} \lambda_i v_i \: \bigg\vert \: k \in \mathbb{N}, \: v_i \in S, \: \lambda_i \in \mathbb{R} \right\}
	\end{equation}
			
	The closed span $\overline{span}(S)$ of a sequence $S$ in $\mathbb{H}$ is defined as the closure of the span with respect to the distance induced by the norm. $S$ is called a basis of $\mathbb{H}$ if $\overline{span}(S) = \mathbb{H}$. \\
	It is called an orthonormal basis, if in addition the following properties hold. 
	
	\begin{multicols}{2}
		\begin{enumerate}
			\item $\langle v_i, v_j \rangle = 0 \quad \forall v_i, v_j \in S \quad i \neq j$
			\item $\lvert \lvert v \rvert \rvert = 1 \quad \forall v \in S$
		\end{enumerate}
	\end{multicols}

	As in the case of a Banach space, each element of a Hilbert space can be expressed in terms of a corresponding basis. This can be done using a Fourier expansion of an element $x \in \mathbb{H}$ w.r.t. a basis $S = \{s_n\}$ as follows.
	
	\begin{equation}
		x = \sum_{j = 1}^{\infty}{\langle x, s_j \rangle}s_j
	\end{equation}
	
	As can be seen, differing from the case of a Banach space, these representations can be limits of series as previously hinted at by using the closed span of the basis. As using an infinite number of basis functions is infeasible in applied contexts, an intuitive way to approximate elements of a Hilbert space, is to use a truncated series.
	
	\begin{equation}
		x \approx \sum_{j = 1}^{K}{\langle x, s_j \rangle}s_j
	\end{equation}
	
	\subsection{Hilbert Space of Square-Integrable Functions}
	In functional data analysis, one Hilbert space of particular importance is the space of square-integrable functions on $[0,1]$ denoted $\mathbb{L}^2[0,1]$. To define it, look first at the measure space given by $([0,1], \mathcal{B}, \mu)$ where $\mathcal{B}$ is the Borel $\sigma$-algebra on $[0,1]$ and $\mu$ is the Lebesgue-measure.\\
	Then $\mathbb{L}^2[0,1]$ is the collection of all measurable functions $f$ on $[0,1]$ that fulfill the following condition.
	
	\begin{equation}
		\int_{0}^{1} \lvert f \rvert^2 d\mu < \infty
	\end{equation}
	
	Its inner product is defined as
	
	\begin{equation}
		\langle f_1, f_2 \rangle = \int_{0}^{1} f_1 f_2 d\mu.
	\end{equation}
	
	The Hilbert space of square integrable functions on $[0,1]$ is the function space that is most often used for theoretical considerations in functional data analysis. Trivially, it is possible to extend this to other closed intervals on the reals line, but there are also more complex generalizations. For the purpose of this paper we will focus on the typical case and assume that random functions are random variables realizing in $\mathbb{L}^2$ for some closed interval of the real numbers. 
	
	\subsection{Different Bases of $\mathbb{L}^2$}
	As previously described, a basis of a Hilbert space can be used to express elements of the space using the corresponding Fourier expansion. Two examples of orthonormal bases of $\mathbb{L}^2[0,1]$ that are often used in practice are explained in the following.
	
	\paragraph{B-spline Basis} Following chapter 3.5 from \cite{ramsay_functional_2005}, splines are defined by first dividing the interval of interest $[\tau_0, \tau_L]$ into $L$ subintervals divided by a non-decreasing sequence of points $(\tau_l)_{l = 1,\dots, L-1}$ called knots. On each subinterval, a spline is a polynomial of chosen order $m = n+1$ where $n$ is its degree. Additionally, at each $\tau_l$ the the polynomials on neighbouring subintervals must match derivatives up to order $m-2$.
	A B-spline is a spline belonging to a basis system developed by \cite{de_boor_practical_1978}. Let $S_{l,m}^{BS}(x) \quad l = 1,\dots,L-1$ be the B-spline of order $m$ for an interval $[\tau_0, \tau_L]$ and knots $\{\tau_l \: \vert \: l = 1,\dots, L-1\}$, then it is defined by the Cox-de Boor recursion formula as follows. 
	
	\begin{equation}
		\begin{split}
			S_{l,0}^{BS}(x) = &
			\begin{cases}
				1 & \text{if} \quad x \in \left[\tau_l, \tau_{l+1}\right)\\
				0 & \text{otherwise}
			\end{cases}\\ \\
			S_{l,m}^{BS}(x) = &\frac{x - \tau_l}{\tau_{l+m} - \tau_l} S_{l,m-1}^{BS}(x) + \frac{\tau_{l+m+1} - x}{\tau_{l+m+1} - \tau_{l+1}} S_{l+1,m-1}^{BS}(x)
		\end{split}
	\end{equation}
	
	{\color{red}Explain what happens when $l+m+1 > L$ !!!}
	
	\begin{figure}[H]\label{bspline_basis}
		\includegraphics[width = \textwidth]{Graphics/Bspline_Basis.pdf}
		\caption{B-spline basis functions of order 3 for 6 equidistant knots on $[0,1]$}
	\end{figure}
	
	\paragraph{Fourier Basis}
	The Fourier basis for $\mathbb{L}^2[0,1]$ is given by the following sequence of functions defined on $[0,1]$.
	\begin{equation}
		S_{i}^{FB}(x) = 
		\begin{cases}
			1 & \text{if} \quad i = 1\\
			\sqrt{2} \cos(\pi i x) & \text{if} \quad i \quad \text{is even} \\
			\sqrt{2} \sin(\pi (i-1)x) & \text{otherwise}
		\end{cases}
	\end{equation}
	
	\begin{figure}[H]\label{fourier_basis}
	\includegraphics[width = \textwidth]{Graphics/Fourier_Basis.pdf}
	\caption{Fourier basis functions for $i = 1,\dots,6$}
	\end{figure}
	
	\subsection{Karhunen-Lo\'{e}ve Expansion and Empirical Eigenbases}

	\subsection{Detailed Draft}
	\begin{itemize}
		\item Motivate random functions from introduction and the general concept of random variables
		\item Formalize random function in this context as random variables realizing in a Hilbert space
		\item Introduce $\mathbb{L}^2[0,1]$ as the Hilbert space of square integrable functions on $[0,1]$
		\item Specialize to Hilbert space being $\mathbb{L}^2[0,1]$ for this context
		\item Define mean and covariance function of a random function realizing in $\mathbb{L}^2[0,1]$
		\item Introduce the concept of a basis of a Hilbert space and specialize to $\mathbb{L}^2[0,1]$
		\item Introduce b-spline and Fourier bases
		\item Introduce eigenfunctions and FPCA on the basis of covariance function (Karhunen-Lo\'{e}ve expansion)
		\item explain similarities to Eigenvalues and Eigenvectors of matrix + PCA (fraction of explained variance etc...)
		\item Introduce functional observations in this context as realizations of a random variable realizing in $\mathbb{L}^2[0,1]$
		\item Explain the concept of iid data in a functional setting
		
-----------------------------------		
		
		\item Define point-wise mean (sample), point-wise standard deviation (sample) and sample covariance function
		\item Explain approximations of functional observations using truncated basis representations
		\item Introduce linear operator $L_1$ and sufficient condition associated with it
		\item Motivate Scalar-on-function regression from multivariate linear regression with a scalar response variable
	\end{itemize}

There are several important aspects of functional regression in this functional setting that separate it from usual multiple regression according to \cite{kokoszka_introduction_2017}: In functional linear regression, the aim is not only to obtain an estimate of the function $\beta(s)$ — this estimate also needs to have a useful interpretation. Without it, there might be prediction, but the increase in understanding of the underlying question will be minimal. One aspect of a useful interpretation is that the estimate $\beta(s)$ should not jump in a seemingly random fashion, because an interpretation of this erratic behavior will often be impossible.

	\begin{itemize}
		\item Explain problem of naively extending multivariate linear regression to infinite dimensions
	\end{itemize}

	A common setting in non-functional regression is akin to the following. Assume a model as follows:
	
	\begin{equation}
		Y = X'\beta + \epsilon
	\end{equation}

	where $X \in \mathbb{R}^{n\times J}$ is a matrix containing the regressors, $\beta \in \mathbb{R}^J$ is a coefficient vector and $epsilon$ is a vector containing the error term. For simplicity, assume that the data generating process fulfills the Markov assumptions. Then the famous OLS-estimator is given by:
	
	\begin{equation}
		\hat{\beta}_{OLS} = (X'X)^{-1}X'Y
	\end{equation}

	A naive approach to FLR would be to try to generalize this to the functional setting.
	Assuming a data generating process of the form:
	
	\begin{equation}
    	 Y =  \int \beta(s)X(s) \,dt \ +\epsilon
    \end{equation}

    it becomes clear that we cannot compute the estimate of $\beta(t)$ as we would do in a classical multivariate setup because of the infinite dimensionality of the underlying objects. Where in the finite dimensional setting the OLS estimator can be derived as a method of moments estimator by solving a system of equations of sample moment restrictions, this leads to a system of infinitely many equations in the functional setting. 
    In practice, functional observations are never truly continuously observed. If we assume that the functional observations are observed at a finite set of points $\{t_1, \dots, t_J\}$ this makes the derivation of an OLS estimator possible as before.
    
    \begin{equation}\label{discrete_time_model}
    	Y_i = \sum_{j = 1}^{n} \beta(t_{j})X_i(t_{j}) + \epsilon_{i},
    \end{equation}

    However, this often still results in a large and difficult to solve system of equations. Even if solved, the result is often a noisy function $\hat{\beta}(s)$ that is not useful for interpretation since it does not use the intuition of smooth functions. Another reason why estimation is not feasible using this approach is colinearity.
    Looking at equation \ref{discrete_time_model} and assuming continuous functions $X_i$ it becomes clear that if $t_{j}$ is close to $t_{j'}$, $X_{i}(t_{j})$ is close to $X_{i}(t_{j'})$. Thereby, there will be vectors $X_{i} = (X_i(t_1), \dots, X_i(t_J))'$ that are highly correlated and thus lead to large variances of $\beta$. (cf. ~\cite{kokoszka_introduction_2017})\\
    A different approach is necessary.
    
    Define therefore
    
    \begin{equation}
  		c_{\mathbf{X}}(t,s) = E[\mathbf{X}(t)\mathbf{X}(s)],\: c_{\mathbf{X}\mathbf{Y}}(t) = E[\mathbf{X}(t)\mathbf{Y}], 
    \end{equation}

   Under the assumption that $X$ is independent from $\epsilon$ we obtain

   \begin{equation}
	   c_{\mathbf{X}\mathbf{Y}}(t) = E[\mathbf{X}(t)\int \beta(t)\mathbf{X}(s) \,ds \ +\epsilon]
   \end{equation}

   \begin{equation}
 	  c_{\mathbf{X}\mathbf{Y}}(t) = E[\int \beta(s) \: \mathbf{X}(s)\mathbf{X}(t) \, ds \: | \: X]  + E[\epsilon |\mathbf{X}]
   \end{equation}
  
   \begin{equation}
      c_{\mathbf{X}\mathbf{Y}}(t) = \int c_{\mathbf{X}}(t,s) \beta(s) \,ds
   \end{equation}
   
   In practice, this results in a large and often difficult to solve system of equations. If solved, a  perfect fit is possible,but results in a noisy and erratic function $\hat{\beta}(s)$ that is not useful for interpretation since it does not utilize the intuition of smooth functions (cf.~\cite{horvath_inference_2012}). Another reason why estimation is not feasible using this approach is colinearity.
   If we approximate the scalar-on-functional regression by assuming a set of discrete observation points for all realizations of the data generating process as
   
  	\begin{equation}
    	Y_i = \sum_{j = 1}^{n} \beta(t_{j})X_i(t_{j}) + \epsilon_{i},
    \end{equation} 

	it becomes clear that if $t_{j}$ is close to $t_{j'}$, $X_{i}(t_{j})$ is close to $X_{i}(t_{j'})$ there will be vectors $X_{i} = (X_i(t_1), \dots, X_i(t_J))'$ that are highly correlated and thus lead to large variances of $\beta$. (cf. ~\cite{kokoszka_introduction_2017}) {\color{red} isn't something missing here? Like "and employ standard multivariate linear regression" // Will be done (Jona)}
  	
	\begin{itemize}
		\item Solution: estimation using truncated basis expansion to approximate data (theoretical description)
	\end{itemize}
		
The simplest approach to regularize the noisy estimate of $\hat{\beta}(s)$	is to expand it with deterministic basis function. Assume 

	\begin{equation}
     	\beta(t) =  \sum_{k=1}^{K} c_{k}B_{k}(t)
    \end{equation}

    to expand
    
    \begin{equation}
    	\int \beta(s)X(s) \,dt \ = \sum_{k=1}^{K} c_{k} \int B_{k}(t)X_{i}(t)\,dt =: \sum_{k=1}^{K} x_{ik} c_{k} 
    \end{equation}

    to the linear model of equation 2 with $\mathbf{c} = [\alpha, c_1, c_2,...,c_K]^T$ (corresponding to $\mathbf{\beta}$) estimated by $\mathbf{\hat{c}} = \mathbf{(X^{T}X)^{-1}X^{T}Y}$. Hence, the estimate $\hat{\beta}$ depends on the basis function $B_{k}$ and its corresponding shape, where $K$ is a subjective choice of the researcher. This subjective choice highly depends on the researchers intuition of the smoothness of the estimate. It is best practice to use the same basis functions as in {\color{red} add link to smoothing} (cf. ~\cite{kokoszka_introduction_2017})
    
    {\color{red} include Part about CI´s? its about inference and not about prediction}
    
	\begin{itemize}
		\item Problem: truncation error $\delta$ and how to deal with it?
	\end{itemize}

	\begin{itemize}	
		\item Explain how to address truncation error in standard errors
		\item Motivate three estimation procedures
		\begin{enumerate}
			\item truncated b-spline basis expansion without addressing truncation error
			\item truncated b-spline basis expansion WITH addressing truncation error
			\item truncated Eigenbasis expansion (advantages: low number of basis functions get low approximation error)
		\end{enumerate}
	\end{itemize}
	
	\subsection{Draft-Overview}
	\begin{itemize}
		\item Motivate Karhunen-Loeve-Expansion and Eigenbasis from PCA		
		\item Explain Scalar-on-Function Regression
		\item Estimation through basis-expansion (incl. Eigenbasis) [and estimation with roughness penalty]
		\item Address approximation error due to basis-truncation
	\end{itemize}

	\subsection{Literature}
	\begin{itemize}
		\item \cite{kokoszka_introduction_2017}
		\item \cite{hsing_theoretical_2015}
		\item \cite{ramsay_functional_2005}
		\item \cite{horvath_inference_2012}
		\item \cite{cai_prediction_2006}
		\item \cite{levitin_introduction_2007}
	\end{itemize}
	
	\newpage
	\section{Simulation Study}
	
	\subsection{Draft-Overview}
	\begin{itemize}
		\item Motivate Simulation for some data generating process from application
		\item Describe Simulation Setting from technical standpoint (DGP, set-up for replication, ...)
		
		\item Compare estimation with \begin{enumerate}
			\item b-spline basis without addressing approximation error
			\item ... including proper treatment of approximation error
			\item Eigenbasis constructed from observations
			\end{enumerate}
	
		\item Prediction not Inference (Alternative: Focused on a testing procedure motivated by the application)
		\item Present Results
		\item Explain relevance for application
	\end{itemize}

	\subsection{Motivate Simulation for some data generating process from application}
	For the simulation study, we use the gasoline dataset to predict the octane ratings of gasoline samples relying on the introduced methods. This has become more relevant as modern internal combustion engines become more complex and rely on precisely tuned fuels. The dataset, which contains 60 i.i.d. observations with each 400 measurements, was constructed using Near infra-red (NIR) spectroscopy, which allows to analyse samples of gasoline much faster and with the same reproducibility as standard tests (cf. \cite{Bohacs_Ovadi_Salgo1998}). The study follows Reiss and Ogden (2007) as a guideline. Similar to Reiss and Ogden (2007), two different true coefficient functions $f_1$ and $f_2$ were chosen that differ in their smoothness: 
	
	\begin{equation}
    	f_1 = 2\sin(0.5\pi t) + 4\sin(1.5 \pi t) + 5\sin(2.5 \pi t) 
    \end{equation}

    \begin{equation}
    	f_2 = 1.5^{\frac{-0,5(t-0.3)^2}{0.02^2}} - 4^{\frac{-0,5(t-0.45)^2}{0.015^2}} +  8^{\frac{-0,5(t-0.6)^2}{0.02^2}} -  1^{\frac{-0,5(t-0.8)^2}{0.03^2}}
    \end{equation}

Two different error-terms $\epsilon $ were created by first generating iid standard normal error and then multiplying them by $\sigma_e $ which is calculated such that the squared multiple correlation coefficient $R^2 = var(Xf) / (var(Xf) + \sigma^2_{e})$ is equal to 0.9 and 0.6. The two error-terms are then computed to generate two sets of responses with different signal-to-noise ratios for each true function, using the gasoline dataset. These four combinations are then used with different number of basis-function $(5,6,...,25)$ of the order 5 to predict the responses using the b-spline basis approach and the FPCR approach. Within one repetition, the data is randomly sampled into a training and a test set to calculate the reported test MSE. The simulation was done with R (version...). In total we carried out 2000 simulations for each combination of data and number of basis functions. {\color{green} Information about version and link to repo / package in footnote??}

\subsection{Results}

\subsubsection{Interpretation and Relevance for Application}

	\subsection{Literature}
	\begin{itemize}
		\item \cite{shonkwiler_explorations_2009}
		\item R-packages: fda, refund, mgcv
	\end{itemize}
	
	\newpage
	\section{Application}
The application uses the insights from the previous sections to predict the octane ratings of the introduced gasoline dataset. Following the results from the simulation study,... 
\begin{itemize}
\item
{\color{green} incorporate results of simulation study: number of basis, components, etc...}
\item
point out difficulties estimating sderror
\item
describe setup and results
\end{itemize}


	\subsection{Draft-Overview}
	\begin{itemize}
		\item Prediction not Inference (Alternative: Focused on a testing procedure motivated by the data set)
		\item IID data set (no dependence between the curves, don't want to do functional time series)
		\item Not necessarily data from economics (like biology, sports, whatever)
		\item Smooth curves or random walk (both fine)
		\item \href{https://functionaldata.wordpress.ncsu.edu/resources/}{https://functionaldata.wordpress.ncsu.edu/resources/}
	\end{itemize}
	
	\subsection{Literature}
	\begin{itemize}
		\item \cite{carey_life_2002}
	\end{itemize}

	\section{Outlook}
	
	\subsection{Literature}
	\begin{itemize}
		\item \cite{James.2009} (shape-restrictions)
	\end{itemize}
	
	\section{Appendix}
	
	\newpage
	
	\section{Bibliography}
	\printbibliography[heading=none]	
	
\end{document}