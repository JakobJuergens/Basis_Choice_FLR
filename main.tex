\documentclass[11pt,twoside,a4paper]{article}
\usepackage[hmargin=2cm, vmargin=2cm]{geometry}
\usepackage{microtype}
\usepackage{hyperref}
\usepackage[english]{babel}
\usepackage{setspace}
\usepackage{xcolor}
\usepackage{amssymb}
\usepackage[
backend=biber,
style=authoryear-comp,
]{biblatex}


\addbibresource{RMbibliography.bib}
\onehalfspacing
\parindent=0pt

\begin{document}
	\title{{\LARGE {\color{red}PLACEHOLDER-TITLE:} Functional Linear Regression in a Scalar-on-Function Setting with Applications to SOMETHING}}
	\author{Jonghun Baek, Jakob Juergens, Jonathan Willnow}
	\date{{\color{red}whenever}}
	\maketitle
	\vspace{1.5 cm}
	\begin{center}
		Research Module in Econometrics and Statistics \\
		Winter Semester 2021/2022
	\end{center}
	
	\newpage
	
	\tableofcontents
	
	\newpage
	
	\section{Introduction}
	
	\begin{itemize}
		\item Describe the idea of regressing a scalar on functional data
		\item Describing the difference to multiple linear regression intuitively
		\item Giving an intuitive example
	\end{itemize}	
	Functional Data Analysis (FDA) is a relatively new field {\color{red} (roots in the 1940s Grenander and Karhunen)} which is gaining more attention as researchers from different fields collect data that is functional in nature. This data can often be processed by classical statistical methods, but only FDA allows extracting information given by the smoothness of the underlying process (cf. \cite{levitin_introduction_2007}).
	 As \cite{kokoszka_introduction_2017} describe, FDA should be considered when one can view variables or units of a given data set as smooth curves or functions and the interest is in analyzing samples of curves (cf. \cite[S.~17]{kokoszka_introduction_2017}).\\
	 To motivate scalar-on-function regression, consider the case of a data set containing a scalar response and observations of an underlying continuous process. In economics, one application could be the regression of stock market correlations on the Global Crisis Index (GCI), where the regression allows to assess the relationship between the correlation and the GCI at every point within a window (cf. \cite{Das_2019}).\\
	 The focus of this paper is to introduce Functional Linear Regression (FLR) in a scalar-on-function setting. We will be using the standard FLR framework, which relates functional predictors to a scalar response as follows:  {\color{red} (I don't set up any interval for s here we might do later...)}
	 \begin{equation}
	 	Y_{i} = \beta_{0} + \int{X_{i}(s)\beta(s)ds} + \epsilon_{i},
	 	\qquad i = 1, ..., n
	 \end{equation}
	 where the $X_{i}$ are realizations of a random function $\mathbf{X}$, $Y_i$ are the corresponding realizations of the response variable and $\beta(s)$ is the coefficient function. The distinct feature of this framework is that the regressor is a function, which necessitates a different approach to estimation. As in the well-known framework of scalar linear regression, this is motivated by an interest in $\beta(s)$ for prediction. For instance, fluctuation in $X_i(s)$ at a point $s_0$ will not have any effect on $Y_i$ if $\beta(s_0)$ = 0. \\
	 Estimation of $\beta(s)$ is inherently an infinite dimensional problem. In Section 2, after introducing the necessary theoretical concepts, we describe three methods of estimating a scalar coefficient function using a concept called truncated basis expansion. The results of the Monte-Carlo simulation regarding these three different methods are reported in Section 3. Finally, in Section 4, we test the prediction of FLR in a real world setting. {\color{red} (We may put some simple descriptions of results about each of MC and Application)}

	\section{Theory}
	
	\subsection{Detailed Draft}
	\begin{itemize}
		\item Motivate random functions from introduction and the general concept of random variables
		\item Formalize random function in this context as random variables realizing in a Hilbert space
		\item Introduce $\mathbb{L}^2[0,1]$ as the Hilbert space of square integrable functions on $[0,1]$
		\item Specialize to Hilbert space being $\mathbb{L}^2[0,1]$ for this context
		\item Define mean and covariance function of a random function realizing in $\mathbb{L}^2[0,1]$
		\item Introduce the concept of a basis of a Hilbert space and specialize to $\mathbb{L}^2[0,1]$
		\item Introduce b-spline and Fourier bases
		\item Introduce eigenfunctions and FPCA on the basis of covariance function (Karhunen-Lo\'{e}ve expansion)
		\item explain similarities to Eigenvalues and Eigenvectors of matrix + PCA (fraction of explained variance etc...)
		\item Introduce functional observations in this context as realizations of a random variable realizing in $\mathbb{L}^2[0,1]$
		\item Explain the concept of iid data in a functional setting
		\item Define point-wise mean (sample), point-wise standard deviation (sample) and sample covariance function
		\item Explain approximations of functional observations using truncated basis representations
		\item Introduce linear operator $L_1$ and sufficient condition associated with it
		\item Motivate Scalar-on-function regression from multivariate linear regression with a scalar response variable
	\end{itemize}
\subsection{Kokoszka Reimherr (2017) p51-53}
There are several important aspects of functional regression in this functional setting that separate it from usual multiple regression according to \cite{kokoszka_introduction_2017}: In functional linear regression, the aim is not only to obtain an estimate of the function $\beta(s)$ â€” this estimate also needs to have a useful interpretation. Without it, there might be prediction, but the increase in understanding of the underlying question will be minimal. One aspect of a useful interpretation is that the estimate $\beta(s)$ should not jump in a seemingly random fashion, because an interpretation of this erratic behavior will often be impossible.

	\begin{itemize}
		\item Explain problem of naively extending multivariate linear regression to infinite dimensions
	\end{itemize}
	A common setting in non-functional regression is akin to the following. Assume a model as follows:
	\begin{equation}
		Y = X'\beta + \epsilon
	\end{equation}
	where $X \in \mathbb{R}^{n\times J}$ is a matrix containing the regressors, $\beta \in \mathbb{R}^J$ is a coefficient vector and $epsilon$ is a vector containing the error term. For simplicity, assume that the data generating process fulfills the Markov assumptions. Then the famous OLS-estimator is given by:
	\begin{equation}
		\hat{\beta}_{OLS} = (X'X)^{-1}X'Y
	\end{equation}
	A naive approach to FLR would be to try to generalize this to the functional setting.
	Assuming a data generating process of the form:
	\begin{equation}
     Y =  \int \beta(s)X(s) \,dt \ +\epsilon
    \end{equation}
    it becomes clear that we cannot compute the estimate of $\beta(t)$ as we would do in a classical multivariate setup because of the infinite dimensionality of the underlying objects. Where in the finite dimensional setting the OLS estimator can be derived as a method of moments estimator by solving a system of equations of sample moment restrictions, this leads to a system of infinitely many equations in the functional setting. 
    In practice, functional observations are never truly continuously observed. If we assume that the functional observations are observed at a finite set of points $\{t_1, \dots, t_J\}$ this makes the derivation of an OLS estimator possible as before.
    
    \begin{equation}\label{discrete_time_model}
    	Y_i = \sum_{j = 1}^{n} \beta(t_{j})X_i(t_{j}) + \epsilon_{i},
    \end{equation}

    However, this often still results in a large and difficult to solve system of equations. Even if solved, the result is often a noisy function $\hat{\beta}(s)$ that is not useful for interpretation since it does not use the intuition of smooth functions. Another reason why estimation is not feasible using this approach is colinearity.
    Looking at equation \ref{discrete_time_model} and assuming continuous functions $X_i$ it becomes clear that if $t_{j}$ is close to $t_{j'}$, $X_{i}(t_{j})$ is close to $X_{i}(t_{j'})$. Thereby, there will be vectors $X_{i} = (X_i(t_1), \dots, X_i(t_J))'$ that are highly correlated and thus lead to large variances of $\beta$. (cf. ~\cite{kokoszka_introduction_2017})\\
    A different approach is necessary.
    
    Define therefore
    \begin{equation}
  	c_{\mathbf{X}}(t,s) = E[\mathbf{X}(t)\mathbf{X}(s)],\: c_{\mathbf{X}\mathbf{Y}}(t) = E[\mathbf{X}(t)\mathbf{Y}], 
    \end{equation}
   Under the assumption that $X$ is independent from $\epsilon$ we obtain
   \begin{equation}
     c_{\mathbf{X}\mathbf{Y}}(t) = E[\mathbf{X}(t)\int \beta(t)\mathbf{X}(s) \,ds \ +\epsilon]
   \end{equation}
    \begin{equation}
     c_{\mathbf{X}\mathbf{Y}}(t) = E[\int \beta(s) \: \mathbf{X}(s)\mathbf{X}(t) \, ds \: | \: X]  + E[\epsilon |\mathbf{X}]
  	\end{equation}
   	\begin{equation}
    	c_{\mathbf{X}\mathbf{Y}}(t) = \int c_{\mathbf{X}}(t,s) \beta(s) \,ds
   \end{equation}
   
   In practice, this results in a large and often difficult to solve system of equations. Even if solved, the result is often a noisy function $\hat{\beta}(s)$ that is not useful for interpretation since it does not use the intuition of smooth functions. Another reason why estimation is not feasible using this approach is colinearity.
   If we approximate the scalar-on-functional regression by assuming a set of discrete observation points for all realizations of the data generating process as
  	\begin{equation}
     Y_i = \sum_{j = 1}^{n} \beta(t_{j})X_i(t_{j}) + \epsilon_{i},
    \end{equation} it becomes clear that if $t_{j}$ is close to $t_{j'}$, $X_{i}(t_{j})$ is close to $X_{i}(t_{j'})$ there will be vectors $X_{i} = (X_i(t_1), \dots, X_i(t_J))'$ that are highly correlated and thus lead to large variances of $\beta$. (cf. ~\cite{kokoszka_introduction_2017}) {\color{red} isn't something missing here? Like "and employ standard multivariate linear regression" // Will be done (Jona)}
   
    
	
   	
	\begin{itemize}
		\item Solution: estimation using truncated basis expansion to approximate data (theoretical description)
	\end{itemize}
	\begin{itemize}
		\item Problem: truncation error $\delta$ and how to deal with it?
	\end{itemize}
	\begin{itemize}

	\begin{itemize}	
		\item Explain how to address truncation error in standard errors
		\item Motivate three estimation procedures
		\begin{enumerate}
			\item truncated b-spline basis expansion without addressing truncation error
			\item truncated b-spline basis expansion WITH addressing truncation error
			\item truncated Eigenbasis expansion (advantages: low number of basis functions get low approximation error)
		\end{enumerate}
	\end{itemize}
	
	\subsection{Draft-Overview}
	\begin{itemize}
		\item Motivate Karhunen-Loeve-Expansion and Eigenbasis from PCA		
		\item Explain Scalar-on-Function Regression
		\item Estimation through basis-expansion (incl. Eigenbasis) [and estimation with roughness penalty]
		\item Address approximation error due to basis-truncation
	\end{itemize}

	\subsection{Literature}
	\begin{itemize}
		\item \cite{kokoszka_introduction_2017}
		\item \cite{hsing_theoretical_2015}
		\item \cite{ramsay_functional_2005}
		\item \cite{horvath_inference_2012}
		\item \cite{cai_prediction_2006}
		\item \cite{levitin_introduction_2007}
	\end{itemize}
	
	\newpage
	\section{Simulation}
	
	\subsection{Draft-Overview}
	\begin{itemize}
		\item Motivate Simulation for some data generating process from application
		\item Describe Simulation Setting from technical standpoint (DGP, set-up for replication, ...)
		
		\item Compare estimation with \begin{enumerate}
			\item b-spline basis without addressing approximation error
			\item ... including proper treatment of approximation error
			\item Eigenbasis constructed from observations
			\end{enumerate}
	
		\item Prediction not Inference (Alternative: Focused on a testing procedure motivated by the application)
		\item Present Results
		\item Explain relevance for application
	\end{itemize}

	\subsection{Literature}
	\begin{itemize}
		\item \cite{shonkwiler_explorations_2009}
		\item R-packages: fda, refund, mgcv
	\end{itemize}
	
	\newpage
	\section{Application}

	\subsection{Draft-Overview}
	\begin{itemize}
		\item Prediction not Inference (Alternative: Focused on a testing procedure motivated by the data set)
		\item IID data set (no dependence between the curves, don't want to do functional time series)
		\item Not necessarily data from economics (like biology, sports, whatever)
		\item Smooth curves or random walk (both fine)
		\item \href{https://functionaldata.wordpress.ncsu.edu/resources/}{https://functionaldata.wordpress.ncsu.edu/resources/}
	\end{itemize}
	
	\subsection{Literature}
	\begin{itemize}
		\item \cite{carey_life_2002}
	\end{itemize}

	\section{Outlook}
	
	\subsection{Literature}
	\begin{itemize}
		\item \cite{James.2009} (shape-restrictions)
	\end{itemize}
	
	\section{Appendix}
	
	\newpage
	
	\section{Bibliography}
	\printbibliography[heading=none]	
	
\end{document}